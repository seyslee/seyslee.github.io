[{"content":"about\n","date":"Jun 30, 2022","permalink":"/","section":"","summary":"about","title":""},{"content":"","date":"Jun 30, 2022","permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs"},{"content":"","date":"Jun 30, 2022","permalink":"/tags/datadog/","section":"Tags","summary":"","title":"datadog"},{"content":"","date":"Jun 30, 2022","permalink":"/tags/monitoring/","section":"Tags","summary":"","title":"monitoring"},{"content":"","date":"Jun 30, 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"개요 # EC2 인스턴스에 설치된 데이터독 에이전트에서 Unable to get disk metrics ... [Error 13] Permission denied 에러가 지속 발생할 때 해결하는 방법을 소개합니다.\n증상 # EC2 인스턴스에서 데이터독 에이전트 v7.37.1 를 사용중입니다.\n도커 컨테이너의 디스크 메트릭 관련 에러 로그가 지속 발생하는 증상입니다.\n데이터독 에이전트 로그\n증상이 발생할 때 출력되는 데이터독 에이전트의 로그 내용은 다음과 같습니다.\n문제되는 로그의 레벨은 Warning이고, 데이터독 대시보드에 보이는 메트릭 수집 자체에는 영향을 주지 않지만, 어드민 관점에서 불필요하게 반복 발생하는 로그는 필수 제거 대상입니다.\n$ tail -10 agent.log 2022-06-30 01:51:38 UTC | CORE | WARN | (pkg/collector/python/datadog_agent.go:124 in LogMessage) | disk:e1dffb2bef34567f | (disk.py:135) | Unable to get disk metrics for /var/lib/docker/containers/471a5aa5d8266ade61d358c1024d704a6b0a3eb7d75f442a01aee362c38cc5dd/mounts/shm: [Errno 13] Permission denied: \u0026#39;/var/lib/docker/containers/471a5aa5d8266ade61d358c1024d704a6b0a3eb7d75f442a01aee362c38cc5dd/mounts/shm\u0026#39;. You can exclude this mountpoint in the settings if it is invalid. 2022-06-30 01:51:38 UTC | CORE | WARN | (pkg/collector/python/datadog_agent.go:124 in LogMessage) | disk:e1dffb2bef34567f | (disk.py:135) | Unable to get disk metrics for /run/docker/netns/0f39da799d9d: [Errno 13] Permission denied: \u0026#39;/run/docker/netns/0f39da799d9d\u0026#39;. You can exclude this mountpoint in the settings if it is invalid. 2022-06-30 01:51:38 UTC | CORE | WARN | (pkg/collector/python/datadog_agent.go:124 in LogMessage) | disk:e1dffb2bef34567f | (disk.py:135) | Unable to get disk metrics for /var/lib/docker/containers/0c3c6d543bfe9dc3bde28599abc0e06e599775a720bc5b8b999a0154b413100d/mounts/shm: [Errno 13] Permission denied: \u0026#39;/var/lib/docker/containers/0c3c6d543bfe9dc3bde28599abc0e06e599775a720bc5b8b999a0154b413100d/mounts/shm\u0026#39;. You can exclude this mountpoint in the settings if it is invalid. 2022-06-30 01:51:38 UTC | CORE | WARN | (pkg/collector/python/datadog_agent.go:124 in LogMessage) | disk:e1dffb2bef34567f | (disk.py:135) | Unable to get disk metrics for /run/docker/netns/de516beeee44: [Errno 13] Permission denied: \u0026#39;/run/docker/netns/de516beeee44\u0026#39;. You can exclude this mountpoint in the settings if it is invalid. 위와 같은 에러 메세지가 끊임없이 반복 발생하는 증상입니다.\n그러나 데이터독 대시보드에 접속해서 해당 인스턴스를 확인해보면 디스크 메트릭을 포함한 모든 메트릭을 정상적으로 수집해오고 있는 상황이었습니다.\n환경 # 문제가 발생한 EC2 Instance의 환경은 다음과 같습니다.\nEC2 인스턴스 내부에서 여러 개의 Docker 컨테이너가 동작하고 있습니다.\nDocker : Docker CE 20.10.7 (linux/amd64) OS : Ubuntu 20.04.3 LTS Datadog Agent : 7.37.1 원인 # EC2 안에서 돌고있는 도커 컨테이너에 디스크 메트릭을 수집하려고 계속 시도하면서 에러 메세지가 발생합니다.\n해결방법 # 데이터독 에이전트 설정에서 도커 컨테이너 관련 파일시스템과 마운트 포인트를 명시적으로 제외하면 해결됩니다.\n상세 해결방법 # 설정파일 확인 # 데이터독 에이전트의 설정파일 경로를 확인합니다.\n$ datadog-agent configcheck 중간에 disk check 부분에서 Config 경로를 확인할 수 있습니다.\n설정파일의 기본경로는 다음과 같습니다.\n... === disk check === Configuration provider: file Configuration source: file:/etc/datadog-agent/conf.d/disk.d/conf.yaml ... ~ === ... 디스크 메트릭 설정파일의 내용을 확인합니다.\n$ cat /etc/datadog-agent/conf.d/disk.d/conf.yaml 기본적으로 아래와 같이 설정되어 있습니다.\n... init_config: ... ... instances: - use_mount: false excluded_filesystems: - autofs - /proc/sys/fs/binfmt_misc - /host/proc/sys/fs/binfmt_misc # mount_point_exclude: # - /proc/sys/fs/binfmt_misc # - /dev/sde # - \u0026#39;[FJ]:\u0026#39; 설정파일 변경 # 디스크 메트릭 설정파일을 변경합니다.\n$ vi /etc/datadog-agent/conf.d/disk.d/conf.yaml 변경내용은 다음과 같습니다.\nexcluded_filesystems에 none, shm, nsfs를 추가합니다. mount_point_exclude 부분을 주석해제합니다. mount_point_exclude에 도커 관련 마운트 포인트를 추가해서 디스크 메트릭 수집에서 제외시킵니다. init_config: ... ... instances: - use_mount: false excluded_filesystems: # Add configs to solve error log # `Unable to get disk metrics` # [Error 13] Permission denied. # == Additional config start == - none - shm - nsfs # == Additional config end == - autofs - /proc/sys/fs/binfmt_misc - /host/proc/sys/fs/binfmt_misc mount_point_exclude: # Add configs to solve error log # `Unable to get disk metrics` # [Error 13] Permission denied. # == Additonal config start == - /var/lib/docker/(containers|overlay2)/ - /run/docker/ - /sys/kernel/debug/ - /run/user/1000/ # == Additional config end == - /proc/sys/fs/binfmt_misc - /dev/sde - \u0026#39;[FJ]:\u0026#39; 작성할 때 # 주석 부분은 안 넣으셔도 됩니다.\n에이전트 재시작 # 변경된 설정을 적용하기 위해 데이터독 에이전트를 재시작합니다.\n$ systemctl restart datadog-agent 재시작 이후 데이터독 에이전트의 상태를 확인합니다.\n$ systemctl status datadog-agent 테스트 # 추가한 디스크 메트릭 설정이 잘 적용되었는지 점검합니다.\n$ datadog-agent configcheck ... === disk check === Configuration provider: file Configuration source: file:/etc/datadog-agent/conf.d/disk.d/conf.yaml ... excluded_filesystems: - none - shm - nsfs - autofs - /proc/sys/fs/binfmt_misc - /host/proc/sys/fs/binfmt_misc mount_point_exclude: - /var/lib/docker/(containers|overlay2)/ - /run/docker/ - /sys/kernel/debug/ - /run/user/1000/ - /proc/sys/fs/binfmt_misc - /dev/sde - \u0026#39;[FJ]:\u0026#39; use_mount: false ~ === ... 정상적으로 적용된 걸 확인할 수 있습니다.\n다시 데이터독 에이전트의 로그를 확인합니다.\n$ tail -f /var/log/datadog/agent.log 2022-06-30 02:16:39 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:38 in CheckStarted) | check:io | Running check... 2022-06-30 02:16:39 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:57 in CheckFinished) | check:io | Done running check, next runs will be logged every 500 runs 2022-06-30 02:16:40 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:38 in CheckStarted) | check:cpu | Running check... 2022-06-30 02:16:40 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:57 in CheckFinished) | check:cpu | Done running check, next runs will be logged every 500 runs 2022-06-30 02:16:45 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:38 in CheckStarted) | check:network | Running check... 2022-06-30 02:16:45 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:57 in CheckFinished) | check:network | Done running check, next runs will be logged every 500 runs 2022-06-30 02:16:46 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:38 in CheckStarted) | check:load | Running check... 2022-06-30 02:16:46 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:57 in CheckFinished) | check:load | Done running check, next runs will be logged every 500 runs 2022-06-30 02:16:47 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:38 in CheckStarted) | check:file_handle | Running check... 2022-06-30 02:16:47 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:57 in CheckFinished) | check:file_handle | Done running check, next runs will be logged every 500 runs 2022-06-30 02:16:47 UTC | CORE | INFO | (pkg/collector/worker/check_logger.go:57 in CheckFinished) | check:file_handle | Done running check, next runs will be logged every 500 runs ... 4 minutes later ... 2022-06-30 02:20:31 UTC | CORE | INFO | (pkg/serializer/serializer.go:388 in sendMetadata) | Sent metadata payload, size (raw/compressed): 1802/496 bytes. 2022-06-30 02:20:33 UTC | CORE | INFO | (pkg/serializer/serializer.go:412 in SendProcessesMetadata) | Sent processes metadata payload, size: 1458 bytes. 디스크 메트릭 제외 설정을 추가로 적용한 이후부터 디스크 메트릭 수집 관련 에러로그가 사라진 걸 확인할 수 있습니다.\n참고자료 # 제 경우는 아래 솔루션을 그대로 적용해서 문제를 해결했습니다.\nDataDog/dd-agent Issue#2932\n","date":"Jun 30, 2022","permalink":"/blog/troubleshoot-datadog-disk-metric/","section":"Blogs","summary":"개요 # EC2 인스턴스에 설치된 데이터독 에이전트에서 Unable to get disk metrics .","title":"데이터독 disk metric error"},{"content":"원문 Automation Best Practices를 번역한 글입니다.\n자동화가 꼭 우리가 구매한 제품을 문앞까지 배달해 줄 자동화된 자동차와 드론에 관한 것이 아닙니다.\n자동화의 목표는 사람들의 삶을 더 편하게 만들고 토요일에 출근하지 않도록 하는 것입니다. 직장에는 늦은 밤과 긴 주말을 잡아먹는 걸 피하기 위해 여전히 자동화할 수 있는 많은 작업이 있습니다. 그렇다고 해서 자동화 자체가 구현하기 쉽다는 의미는 아닙니다.\n자동화된 시스템을 개발하는 것은 하나의 규율이며, 인내심, 견고한 설계 및 프로세스 전반에 걸친 명확한 사고가 필요합니다. 많은 기술 부채를 유발할 수 있는 자동화 시스템을 구현하는 걸 방지하기 위한, 모범 사례가 있습니다. 자동화는 오히려 시스템을 무겁게 만들고 시스템을 유지보수하거나 다시 만들 미래 개발자의 시간을 낭비할 수도 있습니다.\n이를 피하기 위해 자동화된 시스템은 견고한 오류 추적, QA 및 유지 관리가 쉬워야 합니다.\n다음은 자동화 시스템 개발을 위한 몇 가지 유용한 팁입니다.\n잘 만든 자동화 시스템의 조건 # 1. 에러 로깅 # 로깅 및 오류 추적은 흥미로운 작업은 아닙니다. 기본적인 추적 기능을 개발하는 것조차 지루할 수 있습니다.\n그러나 자동화 시스템을 개발하기 위해서는 반드시 필요한 단계입니다.\n로깅 및 오류 추적은 여러 계층에서 발생할 수 있습니다.\n가장 간단한 방법은 발생하는 모든 것을 원시 파일에서 추적하는 것입니다. 특정 데이터 시스템에 저장할 필요가 없기 때문에 오버헤드가 가장 적습니다. 이를 통해 필요한 경우 낮은 수준의 유지 관리 및 향후 데이터 구문 분석이 가능합니다. 표준화된 로그를 생성하면 많은 양의 비정형 데이터를 데이터베이스 외부에 저장할 수 있으며, 최신 기술과 데이터 저장 시스템을 사용하면 이러한 파일을 검색하는 속도가 훨씬 빨라집니다.\n표준 데이터 웨어하우스에 저장된 정보가 적은 상위 수준 로그를 사용하면 문제 및 비정상 시스템을 추적할 수 있습니다. 로깅을 통해 느린 로드 시간, 지속적인 버그 등과 같은 문제를 감지하는 데 사용할 수 있습니다. 비정형 데이터와 달리 데이터 웨어하우스에 높은 수준의 문제를 저장하면 추세를 쉽게 볼 수 있고 향후 분석이 가능합니다.\n특히 더 복잡한 자동화 시스템이 실현됨에 따라 오류와 사용 트렌드를 추적하고 분석할 수 있는 능력이 게임 체인저가 될 것입니다. 이것이 자동화된 자동차, 전자 상거래 및 IOT 장치의 세계인지 여부입니다. 다양한 시스템 간의 엄청난 양의 트랜잭션과 통신 프로토콜을 관리하는 유일한 방법은 자동화뿐입니다.\n자동화 시스템에서 오류 추적 및 로깅은 나중으로 미룰 수 없습니다. 미룬다면 우리가 만드는 시스템의 문제를 절대 관리할 수 없을 것입니다.\n2. QA \u0026amp; 알람 # 오류 로깅은 릴리스 이후에 발생하는 문제에 대한 것입니다. 우리가 버그가 발생하기 전에 최대한 많은 버그를 제거하기를 바랍니다. 문제는 자동화가 시스템을 설정하는 프로그래머와 개발자에게 많은 신뢰를 준다는 것입니다. 자동화된 시스템이 잘못될 수 있는 모든 가능한 방법을 생각하려고 해도 항상 오류가 있으며 발생하는 자동화된 프로세스가 올바르게 발생하는지 확인하기 위해 데이터와 로그를 훑어보는 QA 제품군을 개발해야 합니다 . 시스템이 실행 중인지 확인하는 기본 QA 테스트가 있으며 더 많은 기계 학습 접근 방식을 취할 수 있는 더 복잡한 시스템이 있습니다. 프로세스 문제에 대해 말할 수 있는 패턴 및 데이터 프로필을 찾고 있습니다. 예를 들어 null 값이 정상인 경우도 있지만 특정 문제로 인해 발생하는 경우도 있습니다. 이러한 문제는 특정 리전에서 발생한 다운타임, 특정 OS에서는 제대로 작동하지 않는 업데이트 문제, 표준 통합 및 단위 테스트에서 감지할 수 없는 다양한 기타 문제가 원인일 수 있습니다.\n따라서 시스템이 제대로 작동하는지 확인하고 더 복잡한 문제를 테스트할 수 있도록 기본 테스트를 수행할 수 있는 QA 시스템을 더 깊이 개발하고 개발해야 합니다. 요약하자면, 일반적이지 않은 문제를 탐지하기 위해 데이터를 프로파일링할 수 있는 기계 학습/데이터 과학 시스템의 형태를 구현하는 데 우리가 동의할 부분입니다.\n3. 단순한 코드 # 위의 밈이 나온 이유 중 하나는 잘못 설계된 자동화가 악몽이 되기 때문입니다.\n비효율적이거나 비생산적인 코드 패턴을 의미하는 안티패턴, 과도한 사용자 정의 기능 및 잘못된 설계는 일반적으로 코드를 유지 관리하기 어렵게 만듭니다.\n이렇게 하면 시간을 절약하는 대신 코드를 유지보수하는 데 더 많은 시간이 걸립니다. 자동화로 편리함을 얻는 대신 개발자 시간을 빨아들이는 기술 부채에 시달리는 괴물에게 시달리게 되는 겁니다. ​만약 중학교 고등학교 때 개발한 루브 골드버그Rube Goldberg 장치를 떠올려보면, 간단한 작업을 수행하기 위해 복잡한 시스템을 설계했다는 사실을 기억할 것입니다.\n이것은 일부 자동화 시스템의 좋은 예입니다. 어떤 경우에는 자동화하려고 했던 작업이 더 간단하거나 개발한 시스템이 너무 복잡할 수 있습니다. 과도한 엔지니어링은 미래의 엔지니어가 유지 관리하기 매우 어렵게 만드는 심각한 문제가 될 수 있습니다. 코드를 지나치게 추상화하고, 너무 많은 기능을 개발하고, 그럴듯한 경로가 너무 많으면 코드를 유지 관리할 수 없고 테스트할 수 없습니다.\n코드를 작성한 개발자가 떠나고 다른 프로그래머가 중단했던 부분을 다시 시작해야 할 때까지 이러한 현상은 종종 드러나지 않을 것입니다.\n단순한 코드는 좋은 디자인을 의미하며, 너무 스마트하고 지나치게 설계되지 않도록 합니다. 대부분의 직업이 이 문제로 고통받습니다. 요리사는 한 접시에 너무 많은 것을 담으려 하거나 건축가는 건물에 너무 많은 기능을 담으려 합니다. 결국 가장 중요한 것은 최고의 요리사, 건축가 및 엔지니어들은 단순함이 우아함을 넘어선다는 것을 이해하고 있다는 사실입니다. 이것은 추상화나 객체지향과 같은 개념이 나쁘다는 것을 의미하지는 않습니다. 일부 개발자는 이러한 개념을 유지 관리 가능한 코드보다 더 우선시하는 것 뿐입니다.\n모든 일에는 항상 균형이 있습니다.\n4. 종속성 관리 # 자동화된 시스템과 프로세스에는 움직이는 부분이 많습니다. 즉, 시스템의 안전과 상태를 보장하기 위해 일부 구성 요소는 순서대로 발생해야 하고 일부 구성 요소는 여러 종속성이 완료될 때까지 기다려야 합니다. 점점 더 많은 자동화 시스템이 사람들과 상호 작용하도록 개발됨에 따라, 안전 벽과 노란 테이프 뒤에 있는 로봇 팔을 넘어서, 자동화 시스템은 수행 중인 작업이 정확하고 안전한지 확인하기 위해 여러 종속성을 가져야 합니다.\n시스템이 사람들과 상호 작용하든 컴퓨터의 범위 내에서 작동하든 상관없이, 개발자가 경로를 쉽게 그릴 수 있도록 하는 단순한 종속성 관리를 사용하면 시스템이 실행되지 않아야 할 때 실행되지 않도록 훨씬 쉽게 할 수 있습니다.\n자동화는 심사숙고해서 개발할 때 삶을 더 단순하게 만듭니다. 이는 오류 추적과 시스템이 더 큰 문제를 관리하고 경고하는 방법을 설계하는 것을 의미합니다.\n강력하면서 관리도 편한 자동화 시스템을 만드는 일은 어렵습니다. 그러나 올바르게 구현하면 팀의 생산성을 향상시키고 삶을 단순화할 수 있습니다.\n","date":"Jun 23, 2022","permalink":"/blog/automation-best-practice/","section":"Blogs","summary":"원문 Automation Best Practices를 번역한 글입니다.","title":"Automation best practice"},{"content":"","date":"Jun 23, 2022","permalink":"/tags/dev/","section":"Tags","summary":"","title":"dev"},{"content":"","date":"Jun 22, 2022","permalink":"/tags/container/","section":"Tags","summary":"","title":"container"},{"content":"","date":"Jun 22, 2022","permalink":"/tags/security/","section":"Tags","summary":"","title":"security"},{"content":"개요 # trivy는 사용 편의성이 뛰어나고 성능이 좋은 오픈소스 취약점 스캐너입니다.\n비슷한 소프트웨어로는 clair 가 있습니다.\nAmazon Linux 2의 ALSA 취약점 데이터 소스도 제공합니다.\ntrivy에서 제공하는 모든 OS 관련 데이터 소스는 공식문서에서 확인할 수 있습니다.\ntrivy가 분석한 취약점은 ECR의 취약점 분석 결과와 동일하다고 보면 됩니다.\n차이점은 Amazon ECR의 취약점 스캐닝은 ALAS라는 자체 취약점 코드로 별도 관리합니다. 사실 ALAS 취약점은 여러개(1개 이상)의 CVECommon Vulnerabilities and Exposures 취약점으로 구성됩니다.\ntrivy는 CVE를 분석해서 보여줍니다.\n환경 # OS : macOS Monterey 12.4 (M1 Pro) Shell : zsh + oh-my-zsh 패키지 관리자 : Homebrew 3.5.2 trivy 0.29.1 설치 # macOS용 패키지 관리자인 brew를 사용해서 쉽게 설치 가능합니다.\n$ brew install aquasecurity/trivy/trivy 설치 후 trivy의 버전을 확인합니다.\n$ trivy version Version: 0.29.1 Vulnerability DB: Version: 2 UpdatedAt: 2022-06-16 06:06:28.171512256 +0000 UTC NextUpdate: 2022-06-16 12:06:28.171511856 +0000 UTC DownloadedAt: 2022-06-16 07:07:50.248157 +0000 UTC 사용법 # 컨테이너 취약점 스캔 # Public Registry에 업로드된 컨테이너와 기존에 운영중이던 컨테이너 모두 취약점 분석이 가능합니다.\n취약점 레벨 정하기\n--severity 옵션으로 취약점 레벨을 정할 수 있습니다.\n$ trivy image --severity \u0026#34;LEVELS\u0026#34; PUBLIC_REPO_URL 취약점 결과를 별도 파일로 저장\n--output 옵션을 사용해서 취약점 스캐닝 결과를 별도 파일로 저장할 수 있습니다.\n$ trivy image --severity HIGH,CRITICAL \\ --output ./ebs-csi-driver.txt \\ amazon/aws-ebs-csi-driver:v1.5.1 취약점 스캔 명령어 예시\n인터넷만 연결되어 있다면 모든 컨테이너 이미지를 다운받아와서 trivy가 취약점을 자동 분석할 수 있습니다.\n$ trivy image alpine:latest ... alpine:latest (alpine 3.16.0) Total: 0 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 0) 취약점 스캔 결과 예시\n취약점 스캐닝 결과는 다음과 같이 나옵니다.\n스캔 대상인 amazon/aws-ebs-csi-driver:v1.5.1 이미지는 Public ECR에 업로드된 컨테이너 이미지입니다.\n해당 이미지는 Amazon Web Services에서 공식 제공하고 있습니다.\n$ trivy image --severity \u0026#34;HIGH,CRITICAL\u0026#34; amazon/aws-ebs-csi-driver:v1.5.1 ... amazon/aws-ebs-csi-driver:v1.5.1 (amazon 2 (Karoo)) Total: 8 (HIGH: 7, CRITICAL: 1) ┌────────────────┬────────────────┬──────────┬────────────────────────┬───────────────────────┬────────────────────────────────────────────────────────────┐ │ Library │ Vulnerability │ Severity │ Installed Version │ Fixed Version │ Title │ ├────────────────┼────────────────┼──────────┼────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ cyrus-sasl-lib │ CVE-2022-24407 │ HIGH │ 2.1.26-23.amzn2 │ 2.1.26-24.amzn2 │ cyrus-sasl: failure to properly escape SQL input allows an │ │ │ │ │ │ │ attacker to execute... │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-24407 │ ├────────────────┼────────────────┼──────────┼────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ expat │ CVE-2022-25235 │ CRITICAL │ 2.1.0-12.amzn2 │ 2.1.0-12.amzn2.0.3 │ expat: Malformed 2- and 3-byte UTF-8 sequences can lead to │ │ │ │ │ │ │ arbitrary code... │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-25235 │ │ ├────────────────┼──────────┤ │ ├────────────────────────────────────────────────────────────┤ │ │ CVE-2022-25236 │ HIGH │ │ │ expat: Namespace-separator characters in \u0026#34;xmlns[:prefix]\u0026#34; │ │ │ │ │ │ │ attribute values can lead to arbitrary code... │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-25236 │ ├────────────────┼────────────────┼──────────┼────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ expat │ CVE-2022-25315 │ HIGH │ 2.1.0-12.amzn2 │ 2.1.0-12.amzn2.0.2 │ expat: Integer overflow in storeRawNames() │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-25315 │ ├────────────────┼────────────────┤ ├────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ gzip │ CVE-2022-1271 │ │ 1.5-10.amzn2 │ 1.5-10.amzn2.0.1 │ gzip: arbitrary-file-write vulnerability │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-1271 │ ├────────────────┼────────────────┼──────────┼────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ openssl-libs │ CVE-2022-0778 │ HIGH │ 1:1.0.2k-19.amzn2.0.10 │ 1:1.0.2k-24.amzn2.0.2 │ openssl: Infinite loop in BN_mod_sqrt() reachable when │ │ │ │ │ │ │ parsing certificates │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-0778 │ ├────────────────┼────────────────┼──────────┼────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ xz-libs │ CVE-2022-1271 │ HIGH │ 5.2.2-1.amzn2.0.2 │ 5.2.2-1.amzn2.0.3 │ gzip: arbitrary-file-write vulnerability │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2022-1271 │ ├────────────────┼────────────────┼──────────┼────────────────────────┼───────────────────────┼────────────────────────────────────────────────────────────┤ │ zlib │ CVE-2018-25032 │ HIGH │ 1.2.7-18.amzn2 │ 1.2.7-19.amzn2.0.1 │ zlib: A flaw found in zlib when compressing (not │ │ │ │ │ │ │ decompressing) certain inputs... │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2018-25032 │ └────────────────┴────────────────┴──────────┴────────────────────────┴───────────────────────┴────────────────────────────────────────────────────────────┘ usr/bin/aws-ebs-csi-driver (gobinary) Total: 1 (HIGH: 1, CRITICAL: 0) ┌───────────────────┬────────────────┬──────────┬───────────────────┬──────────────────────────────────┬────────────────────────────────────────────────────────┐ │ Library │ Vulnerability │ Severity │ Installed Version │ Fixed Version │ Title │ ├───────────────────┼────────────────┼──────────┼───────────────────┼──────────────────────────────────┼────────────────────────────────────────────────────────┤ │ k8s.io/kubernetes │ CVE-2021-25741 │ HIGH │ v1.21.0 │ 1.19.15, 1.20.11, 1.21.5, 1.22.2 │ kubernetes: Symlink exchange can allow host filesystem │ │ │ │ │ │ │ access │ │ │ │ │ │ │ https://avd.aquasec.com/nvd/cve-2021-25741 │ └───────────────────┴────────────────┴──────────┴───────────────────┴──────────────────────────────────┴────────────────────────────────────────────────────────┘ 참고자료 # trivy 공식문서\ntrivy는 공식문서도 디테일하게 작성되어 있고, 애초에 사용 난이도도 쉬워서 부담없이 사용할 수 있다는 게 장점인 툴입니다.\n","date":"Jun 22, 2022","permalink":"/blog/how-to-use-trivy/","section":"Blogs","summary":"개요 # trivy는 사용 편의성이 뛰어나고 성능이 좋은 오픈소스 취약점 스캐너입니다.","title":"trivy로 컨테이너 취약점 스캔"},{"content":"","date":"Jun 17, 2022","permalink":"/tags/aws/","section":"Tags","summary":"","title":"aws"},{"content":"개요 # 테라폼을 사용해 AWS Transfer Family와 S3를 배포해서 서비스해보는 과정입니다.\nAWS Transfer Family는 SFTP 서버의 관리형 서비스입니다.\nAWS Transfer Family를 사용하면 SFTP 서버 구축에 필요한 네트워크 구성, EC2 생성, SFTP 서버 패키지 설치 \u0026amp; 설정 등의 인프라 작업이 필요 없어집니다. AWS에서 SFTP 서버를 서버리스로 운영해주기 떄문입니다. SFTP 사용자 관리, 로깅, 엔드포인트 관리도 편합니다.\n단점은 엔지니어가 편해지는 만큼 가격이 비쌉니다.\n테라폼으로 구성할 아키텍처는 다음과 같습니다.\nTransfer Family 사용자가 SFTP 서버에 로그인할 때는 ID, Password 방식이 아닌 SSH 키페어 인증으로 로그인하게 됩니다.\n환경 # Terraform v1.2.3 AWS CLI 2.7.7 Shell : zsh + oh-my-zsh OS : macOS Monterey 12.4 (M1 Pro) 전제조건 # 테라폼이 설치되어 있어야 합니다.\n테라폼으로 AWS 리소스를 배포하기 위해서 AWS CLI에 충분한 권한이 부여되어 있어야 합니다.\nAWS CLI가 설치되어 있어야 합니다.\n테라폼에서 SFTP 서버의 Hostname 설정을 지원하지 않아서, 불가피하게 Hostname 설정만 AWS CLI를 사용합니다. sftp-server-hostname.tf 참조.\n실습하기 # 테라폼 코드 # 다운로드 # 실습에 필요한 테라폼 코드 전체를 다운로드 받습니다.\n다운로드\n코드 구조 # . └── terraform-codes ├── outputs.tf ├── provider.tf ├── route53.tf # Route 53 ├── s3.tf # S3 Bucket ├── sftp-server-hostname.tf # SFTP Server ├── sftp-server-iam.tf # SFTP Server ├── sftp-server.tf # SFTP Server ├── transfer-user-iam.tf # SFTP User ├── transfer-user.tf # SFTP User └── variables.tf # AWS Region 코드 수정 # 실습을 진행하기 전에 각자 환경에 맞게 코드를 수정해주세요.\nS3 버킷 이름\nS3 버킷은 글로벌 환경에서 유니크한 이름을 가져야 합니다.\nvariables.tf에서 S3 버킷 이름을 다른 사람과 겹치지 않도록 변경해주세요.\n변경하지 않으면 버킷 이름이 중복되는 오류가 발생해 S3 버킷이 생성되지 않습니다.\n리전\n디폴트 값으로 SFTP 서버가 서울 리전ap-northeast-2에 배포되도록 설정되어 있습니다.\n자세한 내용은 variables.tf를 참조하세요.\nAWS CLI 권한 확인 # terraform 명령어를 실행하기 전에 AWS CLI에 Administrator 권한이 부여되어 있어야 합니다.\nAWS CLI에서 현재 자신이 어떤 IAM User를 사용하고 있는지 확인합니다.\n$ aws sts get-caller-identity { \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXQOVWG6FYM3OPCLJX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;111111111111\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::111111111111:user/tf-admin\u0026#34; } 제 경우는 Administrator 권한이 부여되어 있는 테라폼 전용 IAM User를 사용하고 있습니다.\n테라폼 배포 # AWS CLI의 권한이 부여된 상태에서 아래 테라폼 명령어를 실행합니다.\n$ terraform init terraform init 명령어를 수행하면 생성되는 .terraform.lock.hcl 파일에서 설정된 Terraform Provider 버전 정보를 확인할 수 있습니다.\n$ cat .terraform.lock.hcl # This file is maintained automatically by \u0026#34;terraform init\u0026#34;. # Manual edits may be lost in future updates. provider \u0026#34;registry.terraform.io/hashicorp/aws\u0026#34; { version = \u0026#34;4.19.0\u0026#34; constraints = \u0026#34;~\u0026gt; 4.0\u0026#34; ... } provider \u0026#34;registry.terraform.io/hashicorp/null\u0026#34; { version = \u0026#34;3.1.1\u0026#34; constraints = \u0026#34;~\u0026gt; 3.1\u0026#34; ... } 이 시나리오에서 terraform provider는 2개를 사용합니다.\nhashicorp/aws v4.19.0 hashicorp/null v3.1.1 aws 프로바이더는 AWS 리소스 배포에 사용합니다.\nnull 프로바이더는 SFTP Server의 Custom Hostname 설정에 사용합니다.\nnull provider는 사용자의 로컬 터미널에서 AWS CLI를 실행해 SFTP Server에 Custom Hostname를 추가합니다.\n자세한 내용은 sftp-server-hostname.tf 파일에서 확인 가능합니다.\nterraform plan 명령어로 배포될 리소스들의 정보를 확인합니다.\n$ terraform plan 이후 리소스 배포를 실시합니다.\n$ terraform apply ... Apply complete! Resources: 13 added, 0 changed, 0 destroyed. Outputs: bucket-name = \u0026#34;rocket-x82q-sftp-bucket\u0026#34; sftp-server-endpoint = \u0026#34;s-993ae220554f4c50a.server.transfer.ap-northeast-2.amazonaws.com\u0026#34; sftp-server-id = \u0026#34;s-993ae220554f4c50a\u0026#34; sftp-username = \u0026#34;alice\u0026#34; 13개의 AWS 리소스를 문제없이 배포했습니다.\nRoute 53에 개인 소유 도메인이 없는 분들은 아쉽게도 sftp-server-endpoint 값으로 SFTP 서버에 로그인해야 합니다.\nSSH 키 생성 # 테라폼으로 리소스 배포가 완료된 이후 SFTP Transfer 유저가 사용할 SSH 키페어를 생성합니다.\n$ ssh-keygen -f sftp-alice Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in sftp-alice Your public key has been saved in sftp-alice.pub The key fingerprint is: ... passphrase는 빈칸으로 입력해서 키를 생성합니다.\n$ ls sftp-alice* sftp-alice sftp-alice.pub 비밀키 sftp-alice와 공개키 sftp-alice.pub이 생성된 걸 확인할 수 있습니다.\nSFTP 유저의 SSH 키 등록 # SFTP 유저가 서버에 접속하려면 SSH 키를 미리 SFTP 서버에 등록해야 합니다.\n공개키의 내용을 확인하고 복사합니다.\n$ cat sftp-alice.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCpmWwof7SgjqepEgFYkHEZEfR84Za7WBKM2b5wvWvPN4u/RcksOXXmn9LMEvLH6ZMx27tBq1Lh/fJet/QKLtntYBjS9WIwsgI2szJRYoiTxpbJOz6vuh13XIO8YUeirb4KLkpMbnj7vkAuU6BGJ5WHgTcVgzM1sPgqNLVxOHy4p2EmKkz1z3EDMolhUm9v0COMw4+8YW78HBizuBdLmIP23o9pNfnKBTtnNHtaIFhJi9f1OK4IvMM+n+sBikrA8mtmMPoTS+agsSFi+LIcqdj00cbm8KAcMtYImQSIjhcUGG+0aPjePOI0Qkj0D0+00GdIxGhJsgbRt+chDHfs1jWQ6ha0SK6Pl3ceQHLtb4mskhQQu78ObTVbt0VNsoRnzhdHVxgisVZGqGsJhY1ahbr0BZn//XrN7zfzRCU203qlbJc76GMWVM+SLNi3zOJYF/rbDJgpIoIwEvqppSrWQUfcPQo0VRxJY1suVd0ms0xeoWjd0sSbIhowBIcASJxQ0X0= xxxxx@xxxxxui-MacBookPro.local AWS Management Console로 돌아가서 아까 생성한 SSH 공개키(sftp-alice.pub)를 Transfer User에 등록합니다.\n공개키 sftp-alice.pub의 내용을 그대로 복사해서 넣어주세요.\n그 후 [Add key] 버튼을 눌러 SSH 공개키를 등록합니다.\nSFTP 서버 세팅이 끝났습니다.\nSFTP 접속 # 이제 SFTP 사용자가 SFTP Client 프로그램을 사용해 서버에 접근하면 됩니다.\nSFTP 접속 명령어는 다음과 같습니다.\n$ sftp -i private_key sftp_username@sftp_server_endpoint 테라폼 코드를 실행하면 Route 53의 Zone과 Record가 생성됩니다.\n그러나 제 경우는 개인 소유의 도메인이 없어서 SFTP에 접속할 때 도메인 대신 Server Endpoint로 접속하게 되었습니다.\nServer Endpoint 주소는 terraform apply로 생성이 완료된 후 마지막 라인 Outputs에 자동 출력됩니다.\n$ sftp -i sftp-alice alice@s-4798bd8d19d646609.server.transfer.ap-northeast-2.amazonaws.com 서버에 정상적으로 로그인될 경우 다음과 같은 메세지가 출력되면서 프롬프트가 sftp\u0026gt;로 변경됩니다.\nConnected to s-4798bd8d19d646609.server.transfer.ap-northeast-2.amazonaws.com. sftp\u0026gt; 사용자가 최초 로그인했을 때 위치하는 경로는 각 유저의 홈 디렉토리인 걸 확인할 수 있습니다.\nsftp\u0026gt; pwd Remote working directory: /rocket-x82q-sftp-bucket/alice help 명령어를 실행해서 전체 SFTP 명령어 리스트를 확인할 수 있습니다.\nsftp\u0026gt; help Available commands: bye Quit sftp cd path Change remote directory to \u0026#39;path\u0026#39; chgrp [-h] grp path Change group of file \u0026#39;path\u0026#39; to \u0026#39;grp\u0026#39; chmod [-h] mode path Change permissions of file \u0026#39;path\u0026#39; to \u0026#39;mode\u0026#39; chown [-h] own path Change owner of file \u0026#39;path\u0026#39; to \u0026#39;own\u0026#39; df [-hi] [path] Display statistics for current directory or filesystem containing \u0026#39;path\u0026#39; exit Quit sftp get [-afpR] remote [local] Download file help Display this help text lcd path Change local directory to \u0026#39;path\u0026#39; lls [ls-options [path]] Display local directory listing lmkdir path Create local directory ln [-s] oldpath newpath Link remote file (-s for symlink) lpwd Print local working directory ls [-1afhlnrSt] [path] Display remote directory listing lumask umask Set local umask to \u0026#39;umask\u0026#39; mkdir path Create remote directory progress Toggle display of progress meter put [-afpR] local [remote] Upload file pwd Display remote working directory quit Quit sftp reget [-fpR] remote [local] Resume download file rename oldpath newpath Rename remote file reput [-fpR] local [remote] Resume upload file rm path Delete remote file rmdir path Remove remote directory symlink oldpath newpath Symlink remote file version Show SFTP version !command Execute \u0026#39;command\u0026#39; in local shell ! Escape to local shell ? Synonym for help 파일 업로드 테스트 # S3에 업로드를 테스트하기 위한 샘플 파일을 생성합니다.\n$ echo \u0026#34;hello world.\u0026#34; \u0026gt;\u0026gt; helloworld.txt SFTP 서버에 다시 접속합니다.\n로컬 파일시스템의 현재 경로에 있는 파일 목록을 조회합니다.\nsftp\u0026gt; lls . helloworld.txt 방금 생성한 helloworld.txt 파일을 SFTP 서버에 업로드하겠습니다.\n실제 파일이 저장되는 장소는 백엔드인 S3 버킷이라는 사실을 명심해야 합니다.\nsftp\u0026gt; put helloworld.txt helloworld.txt Uploading helloworld.txt to /rocket-x82q-sftp-bucket/alice/helloworld.txt helloworld.txt 100% 13 1.0KB/s 00:00 ls 명령어로 파일이 S3에 업로드된 걸 확인할 수 있습니다.\nsftp\u0026gt; ls helloworld.txt SFTP 서버를 빠져나온 다음, AWS CLI에서 S3 버킷 안에 들어있는 오브젝트를 확인합니다.\nsftp\u0026gt; exit $ aws s3 ls s3://rocket-x82q-sftp-bucket/alice/ 2022-06-17 03:10:51 13 helloworld.txt S3 버킷에 파일이 업로드된 걸 확인할 수 있습니다.\n실습환경 정리 # 실습이 종료된 후에는 비용 청구가 발생하는 걸 방지하기 위해 테라폼으로 생성한 리소스를 모두 삭제합니다.\n$ terraform destroy AWS Transfer Family의 SFTP 서버가 저렴한 비용은 아닙니다. 실습이 끝난 후에는 반드시 테라폼으로 생성한 모든 AWS 리소스를 삭제해주세요.\n참고자료 # AWS Transfer Family 요금\nSFTP 엔드포인트 비용이 1시간당 $0.30 입니다. 굉장히 부담스러운 가격입니다.\n","date":"Jun 17, 2022","permalink":"/blog/deploy-sftp-transfer-family-via-terraform/","section":"Blogs","summary":"개요 # 테라폼을 사용해 AWS Transfer Family와 S3를 배포해서 서비스해보는 과정입니다.","title":"AWS Transfer Family 테라폼 구축하기"},{"content":"","date":"Jun 17, 2022","permalink":"/tags/terraform/","section":"Tags","summary":"","title":"terraform"},{"content":"","date":"Jun 14, 2022","permalink":"/tags/vault/","section":"Tags","summary":"","title":"vault"},{"content":"개요 # Vault라는 솔루션이 어떤 건지 감이 안잡히는 분들을 위해 이 글을 작성합니다. 이 튜토리얼로 직접 실습해보면서 볼트에 대해 어렴풋이 이해할 수 있기를 바랍니다.\n이 글에 작성된 실습 예제에서는 다음과 같은 내용을 다룹니다.\n로컬 환경에 vault를 설치하고 개발 서버를 구축합니다. vault에 키를 생성하고 저장합니다. 테라폼으로 EC2 인스턴스를 배포할 때 Vault의 키를 가져오도록 설정하는 방법을 소개합니다. 환경 # OS : macOS Monterey 12.4 Shell : zsh + oh-my-zsh Homebrew 3.5.2 전제조건 # macOS용 패키지 관리자인 brew가 설치되어 있어야 합니다. 테라폼을 실행할 AWS IAM의 Administrator 권한과 Access Key, Secret Key가 있어야 합니다. 배경지식 # Vault는 HashiCorp 사가 개발한 크로스플랫폼 패스워드 및 인증 관리 시스템입니다. 공개되면 안되는 비밀번호, API 키, 토큰 등을 저장하고 관리합니다.\n볼트의 장점은 다양한 Cloud Provider와 Secret 종류들을 중앙에서 제공할 수 있다는 점입니다.\n아래는 볼트의 아키텍처입니다.\n‘Manage Secrets and Protect Sensitive Data, Secure dynamic infrastructure across clouds and environments’ 라는 컨셉으로 다양하게 변화하는 Cloud 환경에서 민감하게 제어되어야 하는 Secrets (Token, 암호, 인증서, SSH Key 등)을 안전하게 보관하고 제어할 수 있도록 해주는 저장소 서비스라고 이해하면 될 듯 합니다.\n데모 # Vault 설치 # brew를 사용해서 최신 버전의 볼트를 로컬에 설치합니다.\n$ brew install vault vault 명령어가 동작하는지 확인합니다.\n$ vault version Vault v1.10.1 (\u0026#39;e452e9b30a9c2c8adfa1611c26eb472090adc767+CHANGES\u0026#39;) 로컬 Vault 서버 시작 # 개발용 Vault 서버를 로컬에 띄우겠습니다.\n$ vault server -dev -dev-root-token-id=\u0026#34;environment\u0026#34; 정상 실행된 결과값은 아래와 같습니다.\n$ vault server -dev -dev-root-token-id=\u0026#34;environment\u0026#34; ==\u0026gt; Vault server configuration: Api Address: http://127.0.0.1:8200 Cgo: disabled Cluster Address: https://127.0.0.1:8201 Go Version: go1.18.1 Listener 1: tcp (addr: \u0026#34;127.0.0.1:8200\u0026#34;, cluster address: \u0026#34;127.0.0.1:8201\u0026#34;, max_request_duration: \u0026#34;1m30s\u0026#34;, max_request_size: \u0026#34;33554432\u0026#34;, tls: \u0026#34;disabled\u0026#34;) Log Level: info Mlock: supported: false, enabled: false Recovery Mode: false Storage: inmem Version: Vault v1.10.1 Version Sha: e452e9b30a9c2c8adfa1611c26eb472090adc767+CHANGES ==\u0026gt; Vault server started! Log data will stream in below: 2022-06-14T21:41:45.810+0900 [INFO] proxy environment: http_proxy=\u0026#34;\u0026#34; https_proxy=\u0026#34;\u0026#34; no_proxy=\u0026#34;\u0026#34; 2022-06-14T21:41:45.810+0900 [WARN] no `api_addr` value specified in config or in VAULT_API_ADDR; falling back to detection if possible, but this value should be manually set 2022-06-14T21:41:45.811+0900 [INFO] core: Initializing versionTimestamps for core 2022-06-14T21:41:45.811+0900 [INFO] core: security barrier not initialized 2022-06-14T21:41:45.811+0900 [INFO] core: security barrier initialized: stored=1 shares=1 threshold=1 2022-06-14T21:41:45.811+0900 [INFO] core: post-unseal setup starting 2022-06-14T21:41:45.813+0900 [INFO] core: loaded wrapping token key 2022-06-14T21:41:45.813+0900 [INFO] core: Recorded vault version: vault version=1.10.1 upgrade time=\u0026#34;2022-06-14 12:41:45.813223 +0000 UTC\u0026#34; 2022-06-14T21:41:45.813+0900 [INFO] core: successfully setup plugin catalog: plugin-directory=\u0026#34;\u0026#34; 2022-06-14T21:41:45.813+0900 [INFO] core: no mounts; adding default mount table 2022-06-14T21:41:45.813+0900 [INFO] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ 2022-06-14T21:41:45.813+0900 [INFO] core: successfully mounted backend: type=system path=sys/ 2022-06-14T21:41:45.813+0900 [INFO] core: successfully mounted backend: type=identity path=identity/ 2022-06-14T21:41:45.814+0900 [INFO] core: successfully enabled credential backend: type=token path=token/ 2022-06-14T21:41:45.814+0900 [INFO] rollback: starting rollback manager 2022-06-14T21:41:45.815+0900 [INFO] core: restoring leases 2022-06-14T21:41:45.815+0900 [INFO] expiration: lease restore complete 2022-06-14T21:41:45.815+0900 [INFO] identity: entities restored 2022-06-14T21:41:45.815+0900 [INFO] identity: groups restored 2022-06-14T21:41:46.020+0900 [INFO] core: post-unseal setup complete 2022-06-14T21:41:46.020+0900 [INFO] core: root token generated 2022-06-14T21:41:46.020+0900 [INFO] core: pre-seal teardown starting 2022-06-14T21:41:46.020+0900 [INFO] rollback: stopping rollback manager 2022-06-14T21:41:46.020+0900 [INFO] core: pre-seal teardown complete 2022-06-14T21:41:46.020+0900 [INFO] core.cluster-listener.tcp: starting listener: listener_address=127.0.0.1:8201 2022-06-14T21:41:46.020+0900 [INFO] core.cluster-listener: serving cluster requests: cluster_listen_address=127.0.0.1:8201 2022-06-14T21:41:46.020+0900 [INFO] core: post-unseal setup starting 2022-06-14T21:41:46.020+0900 [INFO] core: loaded wrapping token key 2022-06-14T21:41:46.020+0900 [INFO] core: successfully setup plugin catalog: plugin-directory=\u0026#34;\u0026#34; 2022-06-14T21:41:46.020+0900 [INFO] core: successfully mounted backend: type=system path=sys/ 2022-06-14T21:41:46.020+0900 [INFO] core: successfully mounted backend: type=identity path=identity/ 2022-06-14T21:41:46.020+0900 [INFO] core: successfully mounted backend: type=cubbyhole path=cubbyhole/ 2022-06-14T21:41:46.021+0900 [INFO] core: successfully enabled credential backend: type=token path=token/ 2022-06-14T21:41:46.021+0900 [INFO] rollback: starting rollback manager 2022-06-14T21:41:46.021+0900 [INFO] core: restoring leases 2022-06-14T21:41:46.021+0900 [INFO] identity: entities restored 2022-06-14T21:41:46.021+0900 [INFO] identity: groups restored 2022-06-14T21:41:46.021+0900 [INFO] core: post-unseal setup complete 2022-06-14T21:41:46.021+0900 [INFO] core: vault is unsealed 2022-06-14T21:41:46.022+0900 [INFO] expiration: lease restore complete 2022-06-14T21:41:46.022+0900 [INFO] expiration: revoked lease: lease_id=auth/token/root/ha0bd2026f65d879aa21234ba2754f71bf0a6a6cb3487e11bf95b579ba5f722b7 2022-06-14T21:41:46.024+0900 [INFO] core: successful mount: namespace=\u0026#34;\u0026#34; path=secret/ type=kv 2022-06-14T21:41:46.035+0900 [INFO] secrets.kv.kv_d2b4bf33: collecting keys to upgrade 2022-06-14T21:41:46.035+0900 [INFO] secrets.kv.kv_d2b4bf33: done collecting keys: num_keys=1 2022-06-14T21:41:46.035+0900 [INFO] secrets.kv.kv_d2b4bf33: upgrading keys finished WARNING! dev mode is enabled! In this mode, Vault runs entirely in-memory and starts unsealed with a single unseal key. The root token is already authenticated to the CLI, so you can immediately begin using Vault. You may need to set the following environment variable: $ export VAULT_ADDR=\u0026#39;http://127.0.0.1:8200\u0026#39; The unseal key and root token are displayed below in case you want to seal/unseal the Vault or re-authenticate. Unseal Key: 7EVjAVuu7sOtz6/9QBs0R652qYB8ErrH0ydQ4Ci+Vng= Root Token: environment Development mode should NOT be used in production installations! 마지막쯤에 출력된 VAULT_ADDR, Unseal Key, Root Token 값은 이후 Vault에 로그인하고 테라폼 배포할 때 사용하는 중요한 정보이기 때문에 따로 기록해둡니다.\n시크릿 구성 # 로컬 Vault 웹서버 주소인 http://127.0.0.1:8200으로 접속합니다.\nMethod : Token Token : environment 토큰 값은 vault 서버 시작시에 출력되었던 Root Token 값을 입력합니다.\n볼트에 로그인 되었습니다. Create secret 을 클릭합니다.\naws_access_key_id aws_secret_access_key region 위 3개의 Secret data를 입력하고 [Save] 버튼을 누릅니다.\nSecret data가 새롭게 생성된 걸 확인할 수 있습니다.\n테라폼 코드 작성 # 테라폼 배포할 코드는 다음과 같은 구조입니다.\n$ tree . ├── instance.tf ├── provider.tf ├── variables.tf └── versions.tf 0 directories, 4 files 테라폼 코드는 4개 파일로 분할 구성되어 있지만, 결국은 t3.mciro 타입의 EC2 인스턴스 1대를 배포한다는 내용입니다.\n각 코드의 내용은 다음과 같습니다.\n아래 내용들을 복사 붙여넣기 해서 한 디렉토리에 모아 담도록 합니다.\n# [1/4] instance.tf resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = var.AMIS[var.AWS_REGION] instance_type = \u0026#34;t3.micro\u0026#34; provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;echo ${aws_instance.example.private_ip} \u0026gt;\u0026gt; private_ips.txt\u0026#34; } tags = { Name = \u0026#34;vault-test-ec2\u0026#34; ManagedBy = \u0026#34;terraform\u0026#34; } } output \u0026#34;ip\u0026#34; { value = aws_instance.example.public_ip } 인스턴스 타입으로는 t3.micro를 사용하겠습니다.\noutput으로 생성한 EC2 인스턴스의 Public IP가 출력되도록 설정했습니다.\n# [2/4] provider.tf terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;= 4.7.0\u0026#34; } } } # ======================================== # Vault # ======================================== data \u0026#34;vault_generic_secret\u0026#34; \u0026#34;aws_creds\u0026#34; { path = \u0026#34;secret/aws\u0026#34; } provider \u0026#34;aws\u0026#34; { region = data.vault_generic_secret.aws_creds.data[\u0026#34;region\u0026#34;] access_key = data.vault_generic_secret.aws_creds.data[\u0026#34;aws_access_key_id\u0026#34;] secret_key = data.vault_generic_secret.aws_creds.data[\u0026#34;aws_secret_access_key\u0026#34;] } 테라폼으로 배포할 때 볼트에 들어있는 Access Key, Secret Key, Region을 사용할 예정입니다.\n# [3/4] variables.tf variable \u0026#34;AWS_REGION\u0026#34; { default = \u0026#34;ap-northeast-2\u0026#34; } variable \u0026#34;AMIS\u0026#34; { type = map(string) # [Tips] # AMI ID `ami-0cbec04a61be382d9` means # Amazon Linux 2 AMI (HVM) - Kernel 5.10 # SSD Volume Type on ap-northeast-2. default = { ap-northeast-2 = \u0026#34;ami-0cbec04a61be382d9\u0026#34; } } 리소스를 배포할 리전은 ap-northeast-2 서울 리전이고, AMI 이미지는 Amazon Linux 2를 사용할 예정입니다.\n# [4/4] versions.tf terraform { required_version = \u0026#34;\u0026gt;= 0.12\u0026#34; } 테라폼 배포 # $ terraform init $ terraform plan $ terraform plan provider.vault.address URL of the root of the target Vault server. Enter a value: plan을 실행하면 Vault 서버의 URL를 입력하라고 나옵니다.\n처음에 볼트 서버를 생성할 때 출력되었던 http://127.0.0.1:8200을 입력합니다.\n$ terraform plan provider.vault.address URL of the root of the target Vault server. Enter a value: http://127.0.0.1:8200 data.vault_generic_secret.aws_creds: Reading... data.vault_generic_secret.aws_creds: Read complete after 0s [id=secret/aws] ... Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + ip = (known after apply) ────────────────────────────────────────────────────────────────────────────────────────────────────────── Note: You didn\u0026#39;t use the -out option to save this plan, so Terraform can\u0026#39;t guarantee to take exactly these actions if you run \u0026#34;terraform apply\u0026#34; now. 1대의 EC2 인스턴스 리소스가 배포될 예정입니다.\nplan을 확인했으니 이제 실제 배포를 실행합니다.\n이번에도 동일하게 Vault 서버 URL http://127.0.0.1:8200을 입력합니다.\n$ terraform apply -auto-approve ... Enter a value: http://127.0.0.1:8200 data.vault_generic_secret.aws_creds: Reading... data.vault_generic_secret.aws_creds: Read complete after 0s [id=secret/aws] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.example will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { + ami = \u0026#34;ami-0cbec04a61be382d9\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) Vault에 들어있는 AWS Access Key, Secret Key를 사용해서 배포가 완료되었습니다.\n... aws_instance.example (local-exec): Executing: [\u0026#34;/bin/sh\u0026#34; \u0026#34;-c\u0026#34; \u0026#34;echo 172.31.53.235 \u0026gt;\u0026gt; private_ips.txt\u0026#34;] aws_instance.example: Creation complete after 13s [id=i-086fc61bd6b09a54b] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: ip = \u0026#34;3.36.46.71\u0026#34; 1개의 리소스가 배포 완료되었습니다.\n마지막에는 출력값으로 생성한 EC2의 Public IP가 나오는 걸 확인할 수 있습니다.\n실습환경 정리 # 테라폼 삭제 # 실습이 끝났습니다.\n테라폼으로 생성한 EC2 인스턴스를 삭제합니다.\n$ terraform destroy provider.vault.address URL of the root of the target Vault server. Enter a value: http://127.0.0.1:8200 data.vault_generic_secret.aws_creds: Reading... data.vault_generic_secret.aws_creds: Read complete after 0s [id=secret/aws] aws_instance.example: Refreshing state... [id=i-074398431097768c9] 중간에 Vault Server의 URL인 http://127.0.0.1:8200을 입력합니다.\nprovider.vault.address URL of the root of the target Vault server. Enter a value: http://127.0.0.1:8200 정말 삭제를 실행할 건지 묻는데 yes를 입력합니다.\n$ terraform destroy ... Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: yes 50초 후 EC2 인스턴스 1대가 삭제되었습니다.\n... aws_instance.example: Destroying... [id=i-074398431097768c9] aws_instance.example: Still destroying... [id=i-074398431097768c9, 10s elapsed] aws_instance.example: Still destroying... [id=i-074398431097768c9, 20s elapsed] aws_instance.example: Still destroying... [id=i-074398431097768c9, 30s elapsed] aws_instance.example: Still destroying... [id=i-074398431097768c9, 40s elapsed] aws_instance.example: Still destroying... [id=i-074398431097768c9, 50s elapsed] aws_instance.example: Destruction complete after 50s Destroy complete! Resources: 1 destroyed. Vault 서버 종료 # 이후 터미널로 돌아와서 개발용 볼트 서버도 Ctrl + C 키를 눌러서 중지하도록 합니다.\nUnseal Key: 7EVjAVuu7sOtz6/9QBs0R652qYB8ErrH0ydQ4Ci+Vng= Root Token: environment Development mode should NOT be used in production installations! b^C==\u0026gt; Vault shutdown triggered 2022-06-14T22:43:57.088+0900 [INFO] core: marked as sealed ... 이상으로 Vault 튜토리얼을 마치겠습니다.\n","date":"Jun 14, 2022","permalink":"/blog/vault-tutorial-for-beginners/","section":"Blogs","summary":"개요 # Vault라는 솔루션이 어떤 건지 감이 안잡히는 분들을 위해 이 글을 작성합니다.","title":"Vault 튜토리얼"},{"content":"","date":"Jun 9, 2022","permalink":"/tags/devops/","section":"Tags","summary":"","title":"devops"},{"content":"","date":"Jun 9, 2022","permalink":"/tags/iac/","section":"Tags","summary":"","title":"iac"},{"content":"문제점 # 테라폼으로 생성한 버킷을 삭제하려고 할 때 내용물이 들어 있으면 테라폼으로 삭제되지 않습니다.\n이 문서에서는 S3 버킷을 수작업으로 비울 필요 없이 테라폼만으로 버킷을 비우고 강제 삭제하는 방법을 소개합니다.\n$ terraform destroy ... aws_iam_role.s3-mybucket-role: Destroying... [id=s3-mybucket-role] aws_iam_role.s3-mybucket-role: Destruction complete after 0s ╷ │ Error: deleting S3 Bucket (mybucket-seyslee-terraform-training): BucketNotEmpty: The bucket you tried to delete is not empty │ status code: 409, request id: NM7NS0KDN2YV6JG4, host id: 0M0e2q8n/aHzPgCqWJWo0/6WVOGEisA7WyA4smqB/9c4wyd8/ArZAbVT60BragCP7x+dWGB9Co8= │ │ 현재 S3에는 helloworld.txt라는 텍스트 파일 오브젝트가 들어있어서 삭제되지 않고 있습니다.\n$ aws s3 ls s3://mybucket-seyslee-terraform-training/ 2022-06-09 00:22:54 211 hello-world.txt 오브젝트들이 들어 있는 버킷을 테라폼으로 강제 삭제하는 방법에 대해 알아보겠습니다.\n중요: 강제 삭제된 버킷은 다시 복구할 수 없다는 점을 명심하세요.\n환경 # Terraform v1.2.2 Provider: terraform-provider-aws_v4.17.1_x5 AWS CLI는 현재 AdministratorAccess 권한이 부여된 상태 해결방법 # 요약 # 테라폼 파일을 수정합니다. S3 버킷의 강제 삭제 설정값을 false에서 true로 변경합니다. 변경사항을 terraform apply로 적용해서 테라폼 상태 파일 .tfstate 에 반영합니다. terraform destroy를 다시 실행합니다. 안에 들어있는 오브젝트들과 함께 버킷이 삭제됩니다. 상세 조치방법 # S3 버킷이 선언된 테라폼 파일의 내용을 확인합니다.\n$ cat s3.tf resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;b\u0026#34; { bucket = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; tags = { Name = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; ManagedBy = \u0026#34;terraform\u0026#34; } } force_destroy 값을 false에서 true로 변경합니다.\n따로 테라폼 파일에 선언되어 있지 않을 경우 기본값 false로 설정됩니다.\n수정한 S3 테라폼 파일입니다.\n$ cat s3.tf resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;b\u0026#34; { bucket = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; force_destroy = true tags = { Name = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; ManagedBy = \u0026#34;terraform\u0026#34; } } force_destroy = true 라인을 새로 작성했습니다.\n참고로 mybucket-seyslee-terraform-training 버킷은 이미 예전에 terraform apply로 생성되어 있던 리소스입니다.\n테라폼 명령어에서 -target 옵션을 사용하면 특정 리소스에만 플랜, 적용, 삭제를 할 수 있습니다.\n위 s3.tf 파일에서 버킷의 리소스 주소는 aws_s3_bucket.b 가 됩니다.\n$ terraform apply -target aws_s3_bucket.b aws_s3_bucket.b: Refreshing state... [id=mybucket-seyslee-terraform-training] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place Terraform will perform the following actions: # aws_s3_bucket.b will be updated in-place ~ resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;b\u0026#34; { ~ force_destroy = false -\u0026gt; true id = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; tags = { \u0026#34;ManagedBy\u0026#34; = \u0026#34;terraform\u0026#34; \u0026#34;Name\u0026#34; = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; } # (9 unchanged attributes hidden) # (2 unchanged blocks hidden) } Plan: 0 to add, 1 to change, 0 to destroy. ╷ │ Warning: Resource targeting is in effect │ │ You are creating a plan with the -target option, which means that the result of │ this plan may not represent all of the changes requested by the current │ configuration. │ │ The -target option is not for routine use, and is provided only for exceptional │ situations such as recovering from errors or mistakes, or when Terraform │ specifically suggests to use it as part of an error message. ╵ Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_s3_bucket.b: Modifying... [id=mybucket-seyslee-terraform-training] aws_s3_bucket.b: Modifications complete after 0s [id=mybucket-seyslee-terraform-training] ╷ │ Warning: Applied changes may be incomplete │ │ The plan was created with the -target option in effect, so some changes requested │ in the configuration may have been ignored and the output values may not be fully │ updated. Run the following command to verify that no other changes are pending: │ terraform plan │ │ Note that the -target option is not suitable for routine use, and is provided only │ for exceptional situations such as recovering from errors or mistakes, or when │ Terraform specifically suggests to use it as part of an error message. ╵ Apply complete! Resources: 0 added, 1 changed, 0 destroyed. 변경사항이 1개 적용되었습니다.\n... ~ resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;b\u0026#34; { ~ force_destroy = false -\u0026gt; true ... force_destory 값이 false에서 true로 변경될 겁니다.\n현재 버킷의 force_destroy 값이 true이기 때문에 이제 삭제 명령어를 재시도합니다.\n테라폼 삭제 명령어를 실행할 때에도 -target 옵션을 사용해서 문제가 발생했던 버킷에 한정해서 삭제를 진행할 수 있습니다.\n$ terraform destroy -target aws_s3_bucket.b aws_s3_bucket.b: Refreshing state... [id=mybucket-seyslee-terraform-training] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # aws_s3_bucket.b will be destroyed - resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;b\u0026#34; { - arn = \u0026#34;arn:aws:s3:::mybucket-seyslee-terraform-training\u0026#34; -\u0026gt; null - bucket = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; -\u0026gt; null - bucket_domain_name = \u0026#34;mybucket-seyslee-terraform-training.s3.amazonaws.com\u0026#34; -\u0026gt; null - bucket_regional_domain_name = \u0026#34;mybucket-seyslee-terraform-training.s3.ap-northeast-2.amazonaws.com\u0026#34; -\u0026gt; null - force_destroy = true -\u0026gt; null - hosted_zone_id = \u0026#34;Z3W03O7B5YMIYP\u0026#34; -\u0026gt; null - id = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; -\u0026gt; null - object_lock_enabled = false -\u0026gt; null - region = \u0026#34;ap-northeast-2\u0026#34; -\u0026gt; null - request_payer = \u0026#34;BucketOwner\u0026#34; -\u0026gt; null - tags = { - \u0026#34;ManagedBy\u0026#34; = \u0026#34;terraform\u0026#34; - \u0026#34;Name\u0026#34; = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; } -\u0026gt; null - tags_all = { - \u0026#34;ManagedBy\u0026#34; = \u0026#34;terraform\u0026#34; - \u0026#34;Name\u0026#34; = \u0026#34;mybucket-seyslee-terraform-training\u0026#34; } -\u0026gt; null - grant { - id = \u0026#34;84a9142d6f4328db8f659834b9b87f22be5cdee2c61f0cd4da3a0f0dadefe959\u0026#34; -\u0026gt; null - permissions = [ - \u0026#34;FULL_CONTROL\u0026#34;, ] -\u0026gt; null - type = \u0026#34;CanonicalUser\u0026#34; -\u0026gt; null } - versioning { - enabled = false -\u0026gt; null - mfa_delete = false -\u0026gt; null } } Plan: 0 to add, 0 to change, 1 to destroy. ╷ │ Warning: Resource targeting is in effect │ │ You are creating a plan with the -target option, which means that the result of │ this plan may not represent all of the changes requested by the current │ configuration. │ │ The -target option is not for routine use, and is provided only for exceptional │ situations such as recovering from errors or mistakes, or when Terraform │ specifically suggests to use it as part of an error message. ╵ Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: yes aws_s3_bucket.b: Destroying... [id=mybucket-seyslee-terraform-training] aws_s3_bucket.b: Destruction complete after 1s ╷ │ Warning: Applied changes may be incomplete │ │ The plan was created with the -target option in effect, so some changes requested │ in the configuration may have been ignored and the output values may not be fully │ updated. Run the following command to verify that no other changes are pending: │ terraform plan │ │ Note that the -target option is not suitable for routine use, and is provided only │ for exceptional situations such as recovering from errors or mistakes, or when │ Terraform specifically suggests to use it as part of an error message. ╵ Destroy complete! Resources: 1 destroyed. 삭제가 안되던 버킷이 삭제 완료되었습니다.\n$ cat terraform.tfstate { \u0026#34;version\u0026#34;: 4, \u0026#34;terraform_version\u0026#34;: \u0026#34;1.2.2\u0026#34;, \u0026#34;serial\u0026#34;: 126, \u0026#34;lineage\u0026#34;: \u0026#34;1e2c7869-b23e-3c00-6b89-e8096e5343ab\u0026#34;, \u0026#34;outputs\u0026#34;: {}, \u0026#34;resources\u0026#34;: [] } 테라폼 상태 파일 terraform.tfstate에서도 S3 리소스가 삭제되어 깔끔해졌습니다.\nAWS Management Console에서 버킷 목록을 조회합니다.\ndestroy로 삭제되지 않던 버킷이 삭제되어 없어진 걸 확인할 수 있습니다.\nAWS CLI에서도 S3 버킷이 삭제된 걸 확인할 수 있습니다.\n$ aws s3 ls s3://mybucket-seyslee-terraform-training/ An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist 존재하지 않는 버킷이라고 나옵니다.\n이상으로 조치 완료되었습니다.\n참고자료 # Terraform force_destroy\nTerraform: Handling the deletion of a non-empty AWS S3 Bucket\n","date":"Jun 9, 2022","permalink":"/blog/terraform-delete-non-empty-s3-bucket/","section":"Blogs","summary":"문제점 # 테라폼으로 생성한 버킷을 삭제하려고 할 때 내용물이 들어 있으면 테라폼으로 삭제되지 않습니다.","title":"테라폼 S3 버킷 강제삭제"},{"content":"개요 # 지정한 리전의 gp2 볼륨을 gp3로 모두 마이그레이션 하는 방법을 소개합니다.\nAWS Management Console에서도 수작업으로 가능하지만 이 글에서는 AWS CLI를 활용한 쉘 스크립트로 변경 작업을 진행합니다.\ngp3로 변경해야할 EBS 볼륨이 200개인데 AWS Management Console로 하나씩 작업하는 일은 지옥입니다.\n전제조건 # jq 설치\n스크립트 안에서 json 응답을 파싱해서 Volume ID와 Volume 상태를 추출하도록 동작하기 때문에, 스크립트를 실행하는 클라이언트에 jq 설치가 필요합니다.\nmacOS의 경우 패키지 관리자인 brew를 통해 쉽게 설치가 가능합니다.\n$ brew install jq 설치 후 jq 명령어의 동작을 확인합니다.\n$ jq --version jq-1.6 AWS CLI\nAWS CLI 설치와 인증이 미리 구성된 상태여야 합니다.\n$ aws --version aws-cli/2.7.5 Python/3.9.13 Darwin/21.5.0 source/arm64 prompt/off 사용법 # jq 설치 확인 # 스크립트 실행 전에 jq 명령어가 설치되어 있는지 확인\n$ which jq /opt/homebrew/bin/jq 스크립트 준비 # gp3-migration.sh\n그대로 스크립트를 실행하면 안되고, region 변수는 자신의 환경에 맞게 변경합니다.\nregion=\u0026#39;ap-northeast-2\u0026#39; 스크립트 실행 # $ sh gp3-migration.sh 주의사항 # 스크립트를 실행하게 되면 지정한 AWS 리전에 위치한 모든 gp2 볼륨이 gp3로 변경 시작됩니다.\n볼륨 타입 변경 과정은 기본적으로 무중단으로 진행되기 때문에 실제 프로덕션 환경에서 실행하더라도 서비스에는 영향을 주지 않고 안전하게 완료됩니다.\n상황에 따라 EBS 볼륨 타입 변경이 최대 24시간 소요될 수 있습니다.\n한 번 스펙을 변경한 EBS 볼륨은 6시간이 지난 후에 다시 변경 가능합니다.\n타입 변경이 진행 중인 EBS 볼륨의 상태는 콘솔에서 확인할 때 In-use - optimizing (n%)으로 표시됩니다.\nAmazon EKSElastic Kubernetes Service 노드에서 gp3, io2 타입의 EBS 볼륨을 사용하려면 EBS CSI Driver 구성이 필요합니다.1 2\n데모 # 터미널 # gp3-migration.sh 스크립트를 실행하면 터미널에 결과값이 나타납니다.\n이 예제에서는 7개의 gp2 타입이 서울 리전에 존재하고, 이를 일괄 변경하는 시나리오입니다.\n$ sh gp3-migration.sh [i] Start finding all gp2 volumes in ap-northeast-2 [i] List up all gp2 volume in ap-northeast-2 vol-11111111111111111 vol-22222222222222222 vol-33333333333333333 vol-44444444444444444 vol-55555555555555555 vol-66666666666666666 vol-77777777777777777 [i] ============================= [i] Migrating all gp2 volumes to gp3 [i] OK: volume vol-11111111111111111 changed to state \u0026#39;modifying\u0026#39; [i] OK: volume vol-22222222222222222 changed to state \u0026#39;modifying\u0026#39; [i] OK: volume vol-33333333333333333 changed to state \u0026#39;modifying\u0026#39; [i] OK: volume vol-44444444444444444 changed to state \u0026#39;modifying\u0026#39; [i] OK: volume vol-55555555555555555 changed to state \u0026#39;modifying\u0026#39; [i] OK: volume vol-66666666666666666 changed to state \u0026#39;modifying\u0026#39; [i] OK: volume vol-77777777777777777 changed to state \u0026#39;modifying\u0026#39; AWS Management Console # gp3-migration.sh 스크립트를 실행한 후 AWS Management Console → EC2 → Volume 페이지를 확인합니다.\n타입 변경이 진행중인 EBS 볼륨의 상태는 in-use - optimizing으로 표시됩니다.\ngp2 → gp3 타입 변경은 EBS의 사용중인 용량, 인프라 환경에 따라 최대 24시간 소요될 수 있습니다.\n잠시 기다리면 볼륨 상태가 in-use로 바뀌며 볼륨 타입이 변경 완료됩니다.\n작업 완료!\nAWS 블로그 게시글 gp2 to gp3 migration\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAWS 공식문서 EBS CSI Driver\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"Jun 3, 2022","permalink":"/blog/script-gp2-volumes-to-gp3-migration/","section":"Blogs","summary":"개요 # 지정한 리전의 gp2 볼륨을 gp3로 모두 마이그레이션 하는 방법을 소개합니다.","title":"gp2 to gp3 마이그레이션 스크립트"},{"content":"개요 # EC2 인스턴스의 표준시를 변경하는 방법을 소개합니다.\n종종 EC2 인스턴스의 시간을 한국 표준시로 변경해야하는 경우가 있습니다.\n제 최근 사례로는 EC2 인스턴스 로컬에서 돌리는 테스트 코드로 인해 생성되는 로그 시간이 협정 세계시UTC, Universal Time Coordinated가 아닌 한국 표준시KST, Korea Standard Time로 찍히도록 해야하는 상황 등이 있었습니다.\n환경 # OS : Amazon Linux 2 Shell : bash 설정방법 # Amazon Linux 2 인스턴스는 기본적으로 UTC협정 세계시 표준 시간대로 설정됩니다.\nOS 버전 확인 # 현재 접속한 인스턴스의 운영체제가 Amazon Linux 2 임을 확인합니다.\n$ id uid=1000(ec2-user) gid=1000(ec2-user) groups=1000(ec2-user),4(adm),10(wheel),190(systemd-journal) $ cat /etc/os-release NAME=\u0026#34;Amazon Linux\u0026#34; VERSION=\u0026#34;2\u0026#34; ID=\u0026#34;amzn\u0026#34; ID_LIKE=\u0026#34;centos rhel fedora\u0026#34; VERSION_ID=\u0026#34;2\u0026#34; PRETTY_NAME=\u0026#34;Amazon Linux 2\u0026#34; ... 인스턴스의 OS 버전이 Amazon Linux 2입니다.\n현재 타임존 확인 # $ timedatectl Local time: Tue 2022-05-31 08:58:34 UTC Universal time: Tue 2022-05-31 08:58:34 UTC RTC time: Tue 2022-05-31 08:58:35 Time zone: n/a (UTC, +0000) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a 현재 설정된 Time zone 값이 n/a이면 협정 세계시UTC를 의미합니다.\n타임존 변경 # 이 예제에서는 타임존을 서울 Asia/Seoul로 설정하겠습니다.\n타임존 목록 확인\n전체 타임존 목록 중 Seoul로 검색합니다.\n$ timedatectl list-timezones | grep -i seoul Asia/Seoul 타임존 변경\n타임존을 Asia/Seoul로 변경합니다.\n$ sudo timedatectl set-timezone Asia/Seoul 타임존 설정 재확인 # Time zone 값이 n/a (UTC, +0000)에서 Asia/Seoul (KST, +0900)으로 변경되었습니다.\n$ timedatectl Local time: Tue 2022-05-31 17:58:59 KST Universal time: Tue 2022-05-31 08:58:59 UTC RTC time: Tue 2022-05-31 08:59:00 Time zone: Asia/Seoul (KST, +0900) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a date 명령어로 현재 시간을 확인합니다.\n$ date Tue May 31 17:59:01 KST 2022 date 결과도 한국 표준시KST로 출력되는 걸 확인할 수 있습니다.\n타임존 설정의 유지 # 위 과정에서 조치한 EC2 타임존 설정은 영구적용이기 때문에 EC2 인스턴스가 리부팅된 후에도 계속 유지됩니다.\n$ uptime 19:49:56 up 0 min, 0 users, load average: 0.13, 0.03, 0.01 현재 EC2 인스턴스의 업타임을 보면 방금 전 리부팅된 상태입니다.\n리부팅이 완료된 후 타임존을 확인합니다.\n$ timedatectl Local time: Tue 2022-05-31 19:49:58 KST Universal time: Tue 2022-05-31 10:49:58 UTC RTC time: Tue 2022-05-31 10:49:59 Time zone: Asia/Seoul (KST, +0900) NTP enabled: yes NTP synchronized: yes RTC in local TZ: no DST active: n/a EC2 인스턴스를 리부팅한 뒤에도 여전히 Time zone 값이 Asia/Seoul (KST, +0900) 로 설정된 걸 확인할 수 있습니다.\n참고자료 # AWS 공식문서\nAmazon Linux의 표준 시간대 변경\n","date":"May 31, 2022","permalink":"/blog/change-ec2-timezone/","section":"Blogs","summary":"개요 # EC2 인스턴스의 표준시를 변경하는 방법을 소개합니다.","title":"EC2 타임존 변경"},{"content":"","date":"May 31, 2022","permalink":"/tags/linux/","section":"Tags","summary":"","title":"linux"},{"content":"개요 # eksctl을 사용해서 자신만의 EKS 클러스터를 생성해보도록 하겠습니다.\n주의사항\n이 실습 과정으로 인해 AWS 비용이 발생할 수 있습니다.\n실습이 끝난 후에는 실습환경 전체 정리를 참고해서 반드시 생성한 AWS 전체 리소스를 삭제해주세요.\n계속 EKS Cluster를 켜두면 거액의 AWS 비용이 발생합니다.\n전제조건 # 이 가이드에 포함된 실습을 진행하기 전에 아래의 몇 가지 준비사항이 필요합니다.\nAWS CLI 툴이 설치된 상태 kubectl 툴이 설치된 상태 eksctl 툴이 설치된 상태 이 글에서는 aws-cli, kubectl, eksctl의 설치방법까지는 다루지 않습니다.\n환경 # EKS Cluster # EKS Cluster를 아래와 같이 구성할 예정입니다.\n리전: ap-northeast-2 (Seoul) 클러스터 이름: eks-course-cluster 노드그룹 이름: ng-1 1 nodegroup 3 worker node (t3.small) 실습비용 절약을 위해 인스턴스 타입은 t3.small로 정합니다.\nClient # OS: macOS Monterey 12.4 (M1 Pro) Shell: zsh + oh-my-zsh kubectl: v1.24.1 aws-cli: 2.7.4 eksctl: 0.100.0 실습 # EKS 생성용 IAM User 생성 # toy-admin이라는 이름의 AWS IAM User를 생성합니다.\n권한은 AWS 관리형 정책인 AdministratorAccess를 부여합니다.\ntoy-admin 유저의 액세스 키를 발급 받은 다음, AWS CLI에서 Credential을 구성합니다.\n$ aws configure --profile toy-admin EC2 키페어 생성 # AWS Management Console에서 EC2 키 페어를 새로 생성합니다.\n키 페어 이름은 eks-course로 정합니다.\n여기서 만든 SSH 키페어는 EKS Cluster의 EC2 노드를 생성할 때 사용되며, 전체 EC2 노드에 해당 키가 주입됩니다.\nAWS CLI 인증 # toy-admin 프로필의 Credential 설정을 확인합니다.\n$ aws configure list --profile toy-admin Name Value Type Location ---- ----- ---- -------- profile toy-admin manual --profile access_key ****************X4XX shared-credentials-file secret_key ****************8xxx shared-credentials-file region ap-northeast-2 config-file ~/.aws/config AWS_PROFILE 환경변수는 AWS CLI에서 현재 사용중인 프로필 이름을 지정합니다.\n프로필을 default에서 toy-admin으로 전환하기 위해 환경변수를 설정합니다.\n$ export AWS_PROFILE=toy-admin 프로필을 변경한 후에는 AWS CLI에서 현재 사용중인 IAM 권한을 확인합니다.\n$ aws sts get-caller-identity { \u0026#34;UserId\u0026#34;: \u0026#34;XXXXXQOVWG0FYM0OXXXXX\u0026#34;, \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/toy-admin\u0026#34; } 결과값을 보면 toy-admin IAM User로 인증되어 있는 상태입니다.\n매니페스트 작성 # EKS 클러스터 배포에 사용할 클러스터 매니페스트를 작성합니다.\n파일명은 eks-cluster.yaml로 정하겠습니다.\n아래 클러스터 매니페스트는 eksctl 공식문서 샘플을 기반으로 작성되었습니다.\n$ cat eks-cluster.yaml apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: eks-course-cluster region: ap-northeast-2 nodeGroups: - name: ng-1 instanceType: t3.small desiredCapacity: 3 ssh: # use existing EC2 key pair publicKeyName: eks-course 실습비용을 절약하기 위해 인스턴스 타입은 t3.small로 설정합니다. publicKeyName 값은 이전 과정에서 AWS 콘솔에서 생성한 EC2 Key Pair의 이름과 동일해야 합니다. 클러스터 생성 # $ ls eks-cluster.yaml 작성한 eks-cluster.yaml 파일을 사용해서 EKS 클러스터를 생성합니다.\ntime 명령어는 명령어 완료 시간 측정을 위해 조합했습니다.\n$ time eksctl create cluster -f eks-cluster.yaml EKS 클러스터 생성은 AWS CloudFormation을 통해 모든 구성이 자동 진행됩니다.\nAWS Management Console → CloudFormation → Stack에 들어가서 직접 진행사항을 확인할 수 있습니다.\nVPC, Subnet, EC2, EBS, 보안그룹Security Group, ASGAuto Scaling Group, NAT Gateway 등 클러스터 운영에 필요한 모든 AWS 리소스를 CloudFormation으로 생성해주는 걸 실시간으로 확인할 수 있습니다.\n$ time eksctl create cluster -f eks-cluster.yaml ... 2022-05-29 19:25:42 [ℹ] using EC2 key pair \u0026#34;eks-course\u0026#34; ... 중간에 출력된 로그를 보면 eks-course라는 이름의 SSH key pair도 잘 받아오는 걸 확인할 수 있습니다.\n... 2022-05-29 19:43:41 [✔] EKS cluster \u0026#34;eks-course-cluster\u0026#34; in \u0026#34;ap-northeast-2\u0026#34; region is ready eksctl create cluster -f eks-cluster.yaml 0.41s user 0.25s system 0% cpu 17:58.99 total EKS 클러스터 생성 완료까지 17분 58초가 걸린 걸 확인할 수 있습니다.\nEKS 클러스터에 필요한 리소스가 굉장히 많기 때문에 생성 완료까지 긴 시간이 소요됩니다.\nEKS 클러스터 생성이 완료되면 kubectl context가 자동으로 새 클러스터로 변경됩니다.\n현재 kubectl context를 확인합니다.\n$ kubectl config current-context toy-admin@eks-course-cluster.ap-northeast-2.eksctl.io 클러스터 확인 # $ eksctl get cluster NAME REGION EKSCTL CREATED eks-course-cluster\tap-northeast-2\tTrue 서울 리전에 eks-course-cluster라는 이름의 EKS 클러스터가 생성되었습니다.\nEKS 클러스터의 노드그룹 ng-1도 확인해봅니다.\n$ eksctl get nodegroup --cluster eks-course-cluster CLUSTER NODEGROUP\tSTATUS CREATED\tMIN SIZE\tMAX SIZE\tDESIRED CAPACITY\tINSTANCE TYPE\tIMAGE ID ASG NAME TYPE eks-course-cluster\tng-1\tCREATE_COMPLETE\t2022-05-29T10:38:46Z\t3 3 3 t3.small ami-06512ccb913a9d11d\teksctl-eks-course-cluster-nodegroup-ng-1-NodeGroup-UQ0GUDZPSXUD\tunmanaged 저희가 구성한대로 t3.small 3대로 구성되어 운영중인 걸 확인할 수 있습니다.\nkubectl명령어로 노드 리스트를 확인해도 결과는 동일합니다.\n$ kubectl get node NAME STATUS ROLES AGE VERSION ip-192-168-12-158.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 7m58s v1.22.6-eks-7d68063 ip-192-168-38-166.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 7m54s v1.22.6-eks-7d68063 ip-192-168-80-123.ap-northeast-2.compute.internal Ready \u0026lt;none\u0026gt; 7m58s v1.22.6-eks-7d68063 AWS Management Console → EC2 에서도 인스턴스 3대가 생성된 걸 확인할 수 있습니다.\n이제 EKS 클러스터 구축이 끝났습니다.\nEKS 비용 # EKS 컨트롤 플레인의 비용은 1시간당 0.1USD 입니다. 노드그룹은 온디맨드 EC2 비용으로 산정됩니다. 위 예제의 경우는 t3.small on-demand x 3대 입니다. 그 밖에 NAT Gateway, EBS 비용도 추가로 나갑니다. 더 자세한 사항은 AWS 공식문서를 참고하세요. 실습환경 전체 정리 # EKS 전체 비용이 부담스럽다면 필요할 때만 클러스터를 생성해서 실습하도록 합니다.\nEKS 클러스터 삭제는 eks delete cluster 명령어를 사용하면 됩니다.\n이전 과정에서 생성한 EKS 클러스터를 삭제하는 방법은 다음과 같습니다.\n$ eksctl delete cluster \\ -f eks-cluster.yaml \\ --approve eksctl로 EKS 클러스터를 생성하면 CloudFormation 스택으로 구성됩니다.\n클러스터 삭제도 마찬가지로 스택만 제거하면 포함된 전체 AWS 리소스가 사라지므로 깔끔하게 EKS 관련 리소스 전체를 정리할 수 있습니다.\n마치며 # 비용이 좀 나가지만 그래도 개인 소유의 EKS Cluster를 얻었습니다.\n이제 자신이 원하는 대로 서비스를 배포하고 운영할 수 있습니다.\n다들 쿠버네티스를 더 쉽고 재밌게 즐기길 바라면서 이만 글 마칩니다.\nBon voyage!\n","date":"May 30, 2022","permalink":"/blog/k8s/create-eks-cluster-using-eksctl/","section":"Blogs","summary":"개요 # eksctl을 사용해서 자신만의 EKS 클러스터를 생성해보도록 하겠습니다.","title":"EKS Cluster 생성"},{"content":"","date":"May 30, 2022","permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes"},{"content":"개요 # ConsoleMe에서 Spoke Account에 있는 IAM 리소스들을 수집하도록 설정하는 방법을 소개한다.\n설정방법 # 1. IAM 구성 # ConsoleMe 아키텍쳐에서 Central Account와 Spoke Account를 이해하는게 중요하다.\nCentral Account : ConsoleMe 인스턴스가 위치한 중앙 계정 Spoke Accounts : ConsoleMe로 관리해야하는 AWS 계정들 위와 같이 IAM Role을 구성한다.\nCentral Account와 Spoke Accoun 양쪽에 IAM Role, Policy 설정이 필요하다.\nCentral Account와 Spoke Account에서 필요한 IAM 작업을 각각 나열하면 다음과 같다.\nCentral Account # Central Role 생성 Central Policy 생성 생성한 Central Policy를 Central Role에 부여Attach Spoke Accounts # Spoke Account가 1개 이상일 경우, 각 AWS 계정마다 아래 IAM 작업 과정 전체를 반복한다.\nSpoke Worker Role 생성 Spoke Policy 생성 생성한 Spoke Policy를 Spoke Worker Role에 부여Attach Spoke Worker Role에 신뢰관계 설정 Spoke Role의 Policy 설정 # Spoke Role에서 사용할 Policy는 ConsoleMe 공식문서의 설정을 그대로 복사해서 생성했다.\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;autoscaling:Describe*\u0026#34;, \u0026#34;cloudwatch:Get*\u0026#34;, \u0026#34;cloudwatch:List*\u0026#34;, \u0026#34;config:BatchGet*\u0026#34;, \u0026#34;config:List*\u0026#34;, \u0026#34;config:Select*\u0026#34;, \u0026#34;ec2:describeregions\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:describevpcendpoints\u0026#34;, \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;iam:*\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketTagging\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketTagging\u0026#34;, \u0026#34;sns:GetTopicAttributes\u0026#34;, \u0026#34;sns:ListTagsForResource\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;sns:SetTopicAttributes\u0026#34;, \u0026#34;sns:TagResource\u0026#34;, \u0026#34;sns:UnTagResource\u0026#34;, \u0026#34;sqs:GetQueueAttributes\u0026#34;, \u0026#34;sqs:GetQueueUrl\u0026#34;, \u0026#34;sqs:ListQueues\u0026#34;, \u0026#34;sqs:ListQueueTags\u0026#34;, \u0026#34;sqs:SetQueueAttributes\u0026#34;, \u0026#34;sqs:TagQueue\u0026#34;, \u0026#34;sqs:UntagQueue\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;Sid\u0026#34;: \u0026#34;iam\u0026#34; } ], \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34; } Spoke Role에 Policy를 생성해서 부여헀으면 다음은 Spoke Role에 신뢰관계를 아래와 같이 설정해준다.\nSpoke Role의 신뢰관계 설정 # Spoke Role에 신뢰관계Trust Relationship을 설정해준다.\nCentral Role이 Spoke Role을 AssumeRole 할 수 있도록 신뢰하게 만들어주는 작업이다.\n{ \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: [ \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;sts:TagSession\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::1243456789012:role/YOUR_CONSOLEME_CENTRAL_ROLE_NAME_HERE\u0026#34; } } ], \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34; } 신뢰관계에서 123456789012는 본인의 Central Account ID로 변경하고 (Spoke Account의 ID가 아니라는 점을 명심하자), YOUR_CONSOLEME_CENTRAL_ROLE_NAME_HERE은 자신의 Central Role의 이름으로 알맞게 변경한다.\n2. ConsoleMe 설정 변경 # 변경 전 설정파일 # role_name 값의 의미: AWS Config 쿼리 또는 Spoke Account의 리소스에 대한 정책 업데이트와 같은 특정 작업을 수행하기 전에 각 Spoke Accoun에 있는 어떤 Role을 Assume Role 해야하는지 ConsoleMe에게 알려준다.\nbase.yaml 파일에는 기본적으로 policies 값이 주석처리되어 있다.\n주석을 제거해서 Spoke Account의 Role을 AssumeRole 할 수 있게 활성화한다.\n$ cat base.yaml ... #policies: # role_name: ConsoleMeRole ... policies 값의 role_name은 이전 과정에서 구성한 AssumeRole할 Spoek Account의 Role 이름으로 변경해주면 된다.\n변경 후 설정파일 # $ cat base.yaml ... policies: role_name: consoleme-spoke-account-worker-role ... 설정 시 참고하기 # role_name은 한 개만 설정 가능하다.\n즉, 각각의 Spoke Account마다 있는 Worker Role 이름을 모두 똑같이 생성해줘야 한다.\nSpoke Account의 Role 이름을 여러개 설정할 경우, ConsoleMe 서비스 시작 시 에러가 발생하지 않지만 마지막 라인의 Role 이름만 인식해서 IAM 리소스를 가져온다.\n$ cat base.yaml ... policies: role_name: consoleme-spoke-account-worker-role-A role_name: consoleme-spoke-account-worker-role-B ... 위 예제의 경우 마지막 라인인 consoleme-spoke-account-worker-role-B라는 이름의 IAM Role만 AssumeRole해서 리소스들을 가져온다.\n3. ConsoleMe 재시작 # 변경된 설정을 적용하기 위해 ConsoleMe 서비스를 재시작한다.\n4. IAM 수집결과 확인 # ConsoleMe 서비스 재시작 후에는 ConsoleMe 컨테이너에 로그 모니터링을 걸어놓는다.\nConsoleMe 프로세스가 올라오면서 설정된 Spoke Role을 AssumeRole 해서 IAM 리소스들을 잘 가져오는지 모니터링하는 목적이다.\n$ docker logs -f consoleme ConsoleMe 웹페이지 → Policies 메뉴로 들어간다.\nPolicies 화면에서 Spoke Account에 있는 IAM Role, Policy가 잘 수집되었는지 확인한다.\n작업 끝!\n결론 # Central Account와 Spoke Account의 이해와 AssumeRole이 정상 동작하도록 IAM Role, Policy 구성을 잘 체크하는게 핵심 포인트라고 생각된다.\nConsoleMe 자체가 AssumeRole에 Cross Account 환경까지 접목되어 있어서 AWS 초심자에게는 구성하는 과정이 헷갈리고 어려워보일 수도 있다.\n하지만 겁먹지 말자. 겉으로 보기만 어렵지 생각보다 쉽게 구성할 수 있는 작업이다.\n참고자료 # 아래 3개의 ConsoleMe 공식문서를 참고해서 구성했다.\nSpoke Accounts\nAccount Syncing\nResource Syncing\n","date":"May 25, 2022","permalink":"/blog/consoleme-resource-syncing/","section":"Blogs","summary":"개요 # ConsoleMe에서 Spoke Account에 있는 IAM 리소스들을 수집하도록 설정하는 방법을 소개한다.","title":"ConsoleMe 리소스 동기화"},{"content":"개요 # ConsoleMe는 기본적으로 AWS SES를 연동을 통해 메일 알림Email Notification 기능을 지원한다.\n메일 알림을 받고 싶을 경우, ConsoleMe와 AWS SES간에 연동 설정하는 방법을 소개한다.\nConsoleMe가 AWS SES를 통해 메일을 보내는 아키텍쳐는 다음과 같다.\n설정방법 # SES 생성 # 먼저 도메인 주소의 SES Identity를 생성한다.\n이미 생성해서 사용하고 있는 SES Identity가 있을 경우, AWS SES 생성 과정은 건너 뛰면 된다.\n위 사진처럼 반드시 Identity type을 Domain으로 설정해서 생성해야한다.\n생성한 도메인 기반의 SES Identity는 인증을 받은 상태여야 한다.\nIAM 설정 # ConsoleME EC2 인스턴스가 IAM Role을 Instance Profile로 사용하는 구성이다.\n아래는 ConsoleMe EC2가 사용하는 IAM Role의 전체 권한 중 SES와 관련된 일부 권한만 뽑아낸 내용이다.\n{ \u0026#34;Statement\u0026#34;: [ { ... TRUNCATED ... }, { \u0026#34;Sid\u0026#34;: \u0026#34;SendEmailNotificationFromConsoleMe\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ses:SendEmail\u0026#34;, \u0026#34;ses:SendRawEmail\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ses:us-east-1:123456789012:identity/company-name.com\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;ses:FromAddress\u0026#34;: [ \u0026#34;*@company-name.com\u0026#34; ] } } }, { ... TRUNCATED ... } ], \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34; } ConsoleMe EC2는 메일 발송을 위해 ses:SendEmail과 ses:SendRawEmail 권한이 필요하다.\nResource 값에는 이전 과정에서 생성한 도메인 기반의 SES Identity의 아마존 리소스 주소(ARN)를 입력한다.\nConfig 수정 # SES 설정 # # SES configuration is necessary for ConsoleMe to send e-mails to your users. ConsoleMe sends e-mails to notify # administrators and requesters about policy requests applicable to them. ses: support_reference: \u0026#34;Please contact us at consoleme@example.com if you have any questions or concerns.\u0026#34; arn: arn:aws:ses:us-east-1:123456789012:identity/company-name.com region: us-east-1 consoleme: name: ConsoleMe sender: bob@company-name.com SES 설정 파라미터 # support_reference : 메일 내용 맨 아래에 표시되는 추가 안내 멘트. arn : SES Identity의 아마존 리소스 주소. region : AWS SES Identity의 리전 이름. region 설정값을 생략할 경우, ConsoleMe에서 기본값인 us-east-1로 자동지정한다. consoleme.name : 보내는 사람의 이름. 이메일 제목의 맨 앞에 표시된다. consoleme.sender : ConsoleMe가 이메일을 보낼 때 찍히는 발신자의 메일 주소. sender 값에 입력한 메일 주소는 AWS SES의 인증을 받은 상태여야 한다. 관리자 메일링 리스트 설정 # ConsoleMe 설정파일에서 fallback_policy_request_reviewers 그룹에 포함된 메일 목록은 ConsoleMe 메일 알람을 받을 관리자 메일링 리스트를 의미한다.\ngroups: ... fallback_policy_request_reviewers: - alice@company-name.com - bob@company-name.com - carol@company-name.com 설정파일의 fallback_policy_request_reviewers 값은 policies.py 코드에서 참조한다.\n# lib/policies.py async def send_communications_new_comment( extended_request: ExtendedRequestModel, user: str, to_addresses=None ): \u0026#34;\u0026#34;\u0026#34; Send an email for a new comment. Note: until ABAC work is completed, if to_addresses is empty, we will send an email to fallback reviewers :param extended_request: ExtendedRequestModel :param user: user making the comment :param to_addresses: List of addresses to send the email to :return: \u0026#34;\u0026#34;\u0026#34; if not to_addresses: to_addresses = config.get(\u0026#34;groups.fallback_policy_request_reviewers\u0026#34;, []) request_uri = await get_policy_request_uri_v2(extended_request) await send_new_comment_notification( extended_request, to_addresses, user, request_uri ) policies.py 코드에서 send_communications_new_comment 함수는 새 코멘트 등록에 대한 알림 메일을 발송하는 함수다.\n메일 테스트 # ConsoleMe는 다음과 같은 상황이 발생할 때 메일을 발송한다.\n권한 신청request 페이지에서 새 댓글comment이 등록된 경우 권한 신청request의 상태가 취소cancel, 거부reject, 승인approve 중 하나로 변경된 경우 권한 요청 페이지All Policy Requests에서 테스트용 Comment를 남기거나 테스트용 권한 리퀘스트의 상태를 변경해보며 알림 메일이 잘 보내지는지 테스트 해본다.\nConsoleMe가 AWS SES를 통해 메일을 발송할 때 ConsoleMe 컨테이너가 관련 로그를 찍는다.\n테스트 메일 발송하기 전에 ConsoleMe 도커 컨테이너에 로그 모니터링을 걸어놓고 발송 테스트를 해보자.\n$ docker logs -f consoleme | grep ses 내 경우 ConsoleMe EC2에서 사용하는 Instance Profile(IAM Role)에 SES 메일 발송ses:SendEmail 권한이 잘못 설정되어 있어서 에러를 경험했다.\nses:SendEmail 권한 에러 발생시 ConsoleMe 컨테이너가 출력하는 에러 로그는 다음과 같다.\n{ \u0026#34;asctime\u0026#34;: \u0026#34;2022-05-19T09:14:43Z+0000\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;consoleme\u0026#34;, \u0026#34;processName\u0026#34;: \u0026#34;MainProcess\u0026#34;, \u0026#34;filename\u0026#34;: \u0026#34;ses.py\u0026#34;, \u0026#34;funcName\u0026#34;: \u0026#34;send_email\u0026#34;, \u0026#34;levelname\u0026#34;: \u0026#34;ERROR\u0026#34;, \u0026#34;lineno\u0026#34;: 83, \u0026#34;module\u0026#34;: \u0026#34;ses\u0026#34;, \u0026#34;threadName\u0026#34;: \u0026#34;MainThread\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Exception sending email\u0026#34;, \u0026#34;to_user\u0026#34;: [\u0026#34;bob@company-name.com\u0026#34;], \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;function\u0026#34;: \u0026#34;consoleme.lib.ses.send_email\u0026#34;, \u0026#34;sender\u0026#34;: \u0026#34;alice@company-name.com\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;ConsoleMe: Policy change request for arn:aws:s3:::example-bucket has been updated to approved and committed\u0026#34;, \u0026#34;exc_info\u0026#34;: \u0026#34;Traceback (most recent call last):\\n File \\\u0026#34;/apps/consoleme/consoleme/lib/ses.py\\\u0026#34;, line 67, in send_email\\n response = await sync_to_async(client.send_email)(\\n File \\\u0026#34;/usr/local/lib/python3.8/site-packages/asgiref/sync.py\\\u0026#34;, line 444, in __call__\\n ret = await asyncio.wait_for(future, timeout=None)\\n File \\\u0026#34;/usr/local/lib/python3.8/asyncio/tasks.py\\\u0026#34;, line 455, in wait_for\\n return await fut\\n File \\\u0026#34;/usr/local/lib/python3.8/concurrent/futures/thread.py\\\u0026#34;, line 57, in run\\n result = self.fn(*self.args, **self.kwargs)\\n File \\\u0026#34;/usr/local/lib/python3.8/site-packages/asgiref/sync.py\\\u0026#34;, line 486, in thread_handler\\n return func(*args, **kwargs)\\n File \\\u0026#34;/usr/local/lib/python3.8/site-packages/botocore/client.py\\\u0026#34;, line 386, in _api_call\\n return self._make_api_call(operation_name, kwargs)\\n File \\\u0026#34;/usr/local/lib/python3.8/site-packages/botocore/client.py\\\u0026#34;, line 705, in _make_api_call\\n raise error_class(parsed_response, operation_name)\\nbotocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the SendEmail operation: User `arn:aws:sts::123456789012:assumed-role/consoleme-instance-profile/i-0a123bcd4e5678901\u0026#39; is not authorized to perform `ses:SendEmail\u0026#39; on resource `arn:aws:ses:us-east-1:123456789012:identity/company-name.com\u0026#39;\u0026#34;, \u0026#34;eventTime\u0026#34;: \u0026#34;2022-05-19T02:12:29.940004-07:00\u0026#34;, \u0026#34;hostname\u0026#34;: \u0026#34;12a34567bc89\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2022-05-19T09:14:43Z+0000\u0026#34; } 전체 에러 로그 내용 중 중요한 부분은 ConsoleMe EC2에서 사용하는 IAM Role에 ses:SendEmail 권한이 부여되어 있지 않아서 메일 발송이 안되었다는 내용이다.\nUser `arn:aws:sts::123456789012:assumed-role/consoleme-instance-profile/i-0a123bcd4e5678901\u0026#39; is not authorized to perform `ses:SendEmail\u0026#39; on resource `arn:aws:ses:us-east-1:123456789012:identity/company-name.com\u0026#39; 메일 샘플\nConsoleMe가 보내는 기본 메일 템플릿은 다음과 같다.\nRequest 상태 변경 시 알림 메일 샘플\n#====[메일 제목]====# ConsoleMe: Policy change request for arn:aws:s3:::example-bucket has been updated to approved and committed #====[메일 내용]====# A policy change request for arn:aws:s3:::example-bucket has been updated to approved and committed See the request here: https://www.company.com/policies/request/1234567b-a10f-1234-a12a-1e2ece345678. Please contact us at consoleme@example.com if you have any questions or concerns. #=================# 새 코멘트 등록 시 알림 메일 샘플\n#====[메일 제목]====# ConsoleMe: A new comment has been added to Policy Change request for arn:aws:s3:::example-bucket #====[메일 내용]====# A new comment has been added to the policy change request for arn:aws:s3:::example-bucket by bob@company-name.com See the request here: https://www.company.com/policies/request/1234567b-a10f-1234-a12a-1e2ece345678. Please contact us at consoleme@example.com if you have any questions or concerns. #=================# 참고자료 # [공식문서] ConsoleMe SES 연동 설정\n[공식문서] ConsoleMe EC2에서 사용하는 IAM 권한 설정\n관련 코드 # ConsoleMe 알림 메일 발송과 관련된 코드들.\nlib/v2/requests.py\n권한신청 상태 변경 시 메일발송 코드 새 코멘트 등록 시 메일발송 코드 lib/policies.py\n권한신청 상태 변경 시 메일발송 코드 새 코멘트 등록 시 메일발송 코드 ","date":"May 20, 2022","permalink":"/blog/consoleme-ses-integration/","section":"Blogs","summary":"개요 # ConsoleMe는 기본적으로 AWS SES를 연동을 통해 메일 알림Email Notification 기능을 지원한다.","title":"ConsoleMe SES 연동"},{"content":"개요 # IAM Role이 다른 IAM Role을 AssumeRole 하도록 허용하려면 Assume될 IAM Role의 신뢰 관계Trust Relationship 설정을 수정해야 한다.\n중요: 2개의 IAM Role이 동일한 AWS 계정 내에 있는지, 아니면 서로 다른 AWS 계정에 있는지에 따라 IAM 설정 방법은 다르다.\n해결방법 # IAM Role이 같은 AWS 계정에 있을 때 # Role_A와 Role_B라는 2개의 IAM Role이 있다고 가정해 보자.\nRole_A가 Role_B를 AssumeRole 할 수 있도록 허용하려면 먼저 Role_B의 신뢰 관계Trust Relationship 설정을 다음과 같이 수정해야 한다.\nRole_B에 설정된 신뢰 관계 # { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::111111111111:role/Role_A\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } 동일한 AWS 계정 내에서 IAM Role이 또다른 IAM Role을 AssumeRole 할 수 있게 허용하려면 신뢰 관계Trust Relationship 설정만 추가해주면 끝난다.\nPrincipal # 권한을 부여할 IAM Role을 지정하는 보안 주체Principal 요소에 유의한다.\n일반적으로 보안 주체Principal 요소는 정책에서 IAM User, IAM Role, AWS Service에 다른 AWS 리소스에 대한 액세스 권한을 부여하는 데 사용된다.\nAWS IAM 공식문서에서 보안주체Principal 요소에 대해 더 자세한 정보를 얻을 수 있다.\nIAM Role이 서로 다른 AWS 계정에 있을 때 # Role_A와 Role_B가 서로 다른 AWS 계정에 있다고 가정해본다.\n이 경우, 신뢰 관계Trust Relationship 설정은 두 IAM Role이 동일한 AWS 계정에 있을 때처럼 똑같이 설정해주면 된다.\nRole_B의 신뢰 관계Trust Relationship 설정에 Role_A가 AssumeRole 할수 있도록 허용해준다.\n여기서 중요한 차이점은 Role_A에 sts:AssumeRole 권한이 있는 추가 정책Policy이 필요하다는 점이다.\n따라서 최종 IAM 설정 결과는 다음과 같다.\nRole_B에 설정된 신뢰 관계 # { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::111111111111:role/Role_A\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } 2개의 IAM Role이 같은 AWS 계정에 속한 경우와 신뢰 관계Trust Relationship 설정 내용은 동일하다.\nRole_A에 설정된 Policy # 그리고 Role_A에는 다음과 같은 정책Policy가 부여되어야 한다.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::222222222222:role/Role_B\u0026#34; } } 이제 Role_A는 다른 계정에 있는 Role_B를 AssumeRole 할 수 있다.\n결론 # IAM Role이 다른 AWS 계정의 IAM Role을 AssumeRole 하는 경우가 그렇게 흔한 케이스는 아니다.\n그래서 나도 처음에 AssuemRole이 거부되는 에러를 마주했을 때 쉽게 해결책을 찾을 수 없었다.\n중요: 결과적으로 다른 AWS 계정에 있는 IAM Role을 AssumeRole 하기 위해서는 수행하는 Role 쪽에 Policy 추가가 반드시 필요하다.\n참고자료 # [영문] AWS IAM: Allowing a Role to Assume Another Role\n","date":"May 17, 2022","permalink":"/blog/aws-iam-allowing-a-role-to-assume-another-role/","section":"Blogs","summary":"개요 # IAM Role이 다른 IAM Role을 AssumeRole 하도록 허용하려면 Assume될 IAM Role의 신뢰 관계Trust Relationship 설정을 수정해야 한다.","title":"Role to Role AssumeRole 설정"},{"content":"개요 # AWS CLI 명령어를 사용해서 특정 EC2 인스턴스 타입이 어디 가용영역AZ, Availiability Zone에서 지원하는지 확인하는 방법을 소개합니다.\n전제조건 # AWS CLI가 미리 설치되어 있어야 합니다.\nAWS CLI 설치\nAWS CLI가 설치되어 있지 않을 경우, macOS 패키지 관리자인 brew를 통해 쉽게 설치할 수 있습니다.\n$ brew install awscli 설치 후 AWS CLI 명령어가 잘 동작하는지 확인합니다.\n$ aws --version aws-cli/2.7.0 Python/3.9.12 Darwin/21.4.0 source/arm64 prompt/off 확인방법 # 명령어 형식 # $ aws ec2 describe-instance-type-offerings \\ --filters Name=instance-type,Values=\u0026lt;INSTANCE-TYPE\u0026gt; \\ --location-type availability-zone \\ --region \u0026lt;REGION\u0026gt; INSTANCE-TYPE과 REGION 값은 각자 상황에 맞게 변경해서 실행합니다.\n명령어 예시 # 예제 1\ng4dn.xlarge 인스턴스 타입(GPU)이 도쿄 리전ap-northeast-1의 어떤 가용영역AZ, Availiability Zone에서 지원하는지 확인하는 명령어\n$ aws ec2 describe-instance-type-offerings \\ --filters Name=instance-type,Values=g4dn.xlarge \\ --location-type availability-zone \\ --region ap-northeast-1 { \u0026#34;InstanceTypeOfferings\u0026#34;: [ { \u0026#34;InstanceType\u0026#34;: \u0026#34;g4dn.xlarge\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-1b\u0026#34; }, { \u0026#34;InstanceType\u0026#34;: \u0026#34;g4dn.xlarge\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-1d\u0026#34; }, { \u0026#34;InstanceType\u0026#34;: \u0026#34;g4dn.xlarge\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-1c\u0026#34; } ] } g4dn.xlarge 인스턴스 타입은 도쿄 리전ap-northeast-1의 가용영역 b, c, d에서만 지원하고 있습니다.\n예제 2\nt3.medium 인스턴스 타입이 서울 리전ap-northeast-2의 어떤 가용영역AZ, Availiability Zone에서 지원하는지 확인하는 명령어\n$ aws ec2 describe-instance-type-offerings \\ --filters Name=instance-type,Values=t3.medium \\ --location-type availability-zone \\ --region ap-northeast-2 { \u0026#34;InstanceTypeOfferings\u0026#34;: [ { \u0026#34;InstanceType\u0026#34;: \u0026#34;t3.medium\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-2b\u0026#34; }, { \u0026#34;InstanceType\u0026#34;: \u0026#34;t3.medium\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-2d\u0026#34; }, { \u0026#34;InstanceType\u0026#34;: \u0026#34;t3.medium\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-2c\u0026#34; }, { \u0026#34;InstanceType\u0026#34;: \u0026#34;t3.medium\u0026#34;, \u0026#34;LocationType\u0026#34;: \u0026#34;availability-zone\u0026#34;, \u0026#34;Location\u0026#34;: \u0026#34;ap-northeast-2a\u0026#34; } ] } t3.medium 인스턴스 타입은 서울 리전ap-northeast-2의 가용영역 전체(a, b, c, d)에서 지원하고 있다.\nt2나 t3가 속한 범용 인스턴스 패밀리는 일반적으로 모든 가용영역에서 지원하고 있습니다.\n참고자료 # Example 3: To check whether an instance type is supported 섹션을 참고\nAWS CLI Command Reference - describe-instance-type-offerings\n","date":"May 9, 2022","permalink":"/blog/checking-ec2-type-support-in-az/","section":"Blogs","summary":"개요 # AWS CLI 명령어를 사용해서 특정 EC2 인스턴스 타입이 어디 가용영역AZ, Availiability Zone에서 지원하는지 확인하는 방법을 소개합니다.","title":"EC2 인스턴스 타입이 지원하는 AZ 확인"},{"content":"","date":"May 5, 2022","permalink":"/tags/docker/","section":"Tags","summary":"","title":"docker"},{"content":"개요 # EC2에서 ECR Private 레포지터리로 도커 이미지를 푸시하는 방법을 소개한다.\n구성도로 표현하면 다음과 같다.\n환경 # OS : Ubuntu 20.04.3 LTS Shell : bash AWS CLI : 1.18.69 Docker : 20.10.7 전제조건 # EC2에 연결된 IAM RoleInstance Profile에 ECR 업로드 권한이 부여되어 있어야 한다.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:Describe*\u0026#34;, \u0026#34;ecr:Get*\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34;, \u0026#34;ecr:BatchDeleteImage\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:ecr:REGION:AWS_ACCOUNT_ID:repository/REPOSITORY_NAME\u0026#34; ] } ] } REGION, AWS_ACCOUNT_ID, REPOSITORY_NAME 값은 개인 환경마다 다르다. 꼭 변경해서 사용한다.\n본문 # ECR 로그인 # 현재 시스템에 AWS CLI가 설치된 걸 확인한다.\nAmazon Linux 2 또는 Ubuntu 20.04.3 LTS 기준으로, OS에 AWS CLI가 기본적으로 설치되어 있다.\n$ aws --version aws-cli/1.18.69 Python/3.8.10 Linux/5.13.0-1017-aws botocore/1.16.19 AWS CLI 1.18.69 버전이 설치된 걸 확인할 수 있다.\n이미지를 푸시하려는 Amazon ECRElastic Container Registry 레포지터리에 대해 Docker 클라이언트를 인증한다.\n이미지를 업로드할 ECR 레포지터리가 여러개일 경우, 인증 토큰을 각각 받아야 한다. 인증 토큰은 12시간 동안만 유효하다. $ aws ecr get-login-password --region REGION | docker login --username AWS --password-stdin AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com ... Login Succeeded Login Succeeded 메세지가 출력되면 로그인이 정상적으로 완료된 것이다.\nECR 레포지터리 생성 # AWS Management Console에 로그인한다.\n이미지를 업로드하기 위해 ECR Private 레포지터리를 미리 생성해놓는다.\n현재는 이미지가 없이 비어있는 ECR 레포지터리이다.\n이미지 Push # ECR 레포지터리에 업로드할 도커 이미지를 확인한다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE test-user/nginx 0.0.7 00ffc0fd000a 9 days ago 1.35GB test-user/nginx 0.0.6 b0a00d000b00 9 days ago 1.35GB 내 경우 Image ID가 00ffc0fd000a인 이미지를 ECR 레지스트리에 업로드하려고 한다.\nECR 레지스트리에 업로드 할 이미지에 태그를 붙인다.\ntag 명령어 형식 # $ docker tag IMAGE_ID AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/ECR_REPOSITORY_NAME:TAG tag 명령어 예시 # $ docker tag 00ffc0fd000a 123456789012.dkr.ecr.ap-northeast-2.amazonaws.com/repository:latest 태그를 붙인 이미지가 새로 추가된 걸 확인한다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE 123456789012.dkr.ecr.ap-northeast-2.amazonaws.com/repository latest 00ffc0fd000a 9 days ago 1.35GB test-user/nginx 0.0.7 00ffc0fd000a 9 days ago 1.35GB test-user/nginx 0.0.6 b0a00d000b00 9 days ago 1.35GB 똑같은 이미지를 태그만 바꿔서 등록헀기 떄문에 IMAGE ID가 34ffc1fd958a로 동일하다는 점을 확인할 수 있다.\n이미지를 ECR 레포지터리에 푸시한다.\npush 명령어 형식 # $ docker push AWS_ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/ECR_REPISOTRY_NAME:TAG AWS_ACCOUNT_ID, REGION, ECR_REPOSITORY_NAME, TAG 값은 각자의 환경에 맞게 바꿔준다.\npush 명령어 예시 # $ docker push 123456789012.dkr.ecr.ap-northeast-2.amazonaws.com/repository:latest ... latest: digest: sha256:18fxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx size: 5584 컨테이너 이미지를 ECR 레포지터리에 업로드 완료했다.\n결과 확인 # AWS Console 로그인 후 ECR Registry에서 이미지가 생성된 것을 확인할 수 있다.\n이걸로 작업완료.\n참고자료 # ECR 업로드 작업 같은 경우는 AWS 공식문서에도 친절히 설명되어 있다.\nAWS 공식문서\nDocker 이미지 푸시\n","date":"May 5, 2022","permalink":"/blog/docker-push-ecr-image/","section":"Blogs","summary":"개요 # EC2에서 ECR Private 레포지터리로 도커 이미지를 푸시하는 방법을 소개한다.","title":"EC2에서 ECR로 도커 이미지 푸시"},{"content":"","date":"May 2, 2022","permalink":"/tags/tekton/","section":"Tags","summary":"","title":"tekton"},{"content":"개요 # minikube를 통해 Tekton Pipeline과 Tekton Dashboard를 설치하고 데모를 구성해본다.\n환경 # OS: macOS Monterey 12.3.1 Shell: zsh minikube v1.25.2 Homebrew 3.4.9 전제조건 # minikube가 미리 설치되어 있어야 한다. 시작하기 # 1. Task 데모 # 미니큐브 클러스터를 생성한다.\n$ minikube start 😄 Darwin 12.3.1 (arm64) 의 minikube v1.25.2 ✨ 자동적으로 docker 드라이버가 선택되었습니다 👍 minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중 🚜 베이스 이미지를 다운받는 중 ... 💾 쿠버네티스 v1.23.3 을 다운로드 중 ... \u0026gt; preloaded-images-k8s-v17-v1...: 419.07 MiB / 419.07 MiB 100.00% 2.37 MiB \u0026gt; gcr.io/k8s-minikube/kicbase: 343.12 MiB / 343.12 MiB 100.00% 1.87 MiB p/ 🔥 Creating docker container (CPUs=2, Memory=1988MB) ... 🐳 쿠버네티스 v1.23.3 을 Docker 20.10.12 런타임으로 설치하는 중 ▪ kubelet.housekeeping-interval=5m ▪ 인증서 및 키를 생성하는 중 ... ▪ 컨트롤 플레인이 부팅... ▪ RBAC 규칙을 구성하는 중 ... 🔎 Kubernetes 구성 요소를 확인... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 🌟 애드온 활성화 : storage-provisioner, default-storageclass 🏄 끝났습니다! kubectl이 \u0026#34;minikube\u0026#34; 클러스터와 \u0026#34;default\u0026#34; 네임스페이스를 기본적으로 사용하도록 구성되었습니다. kubectl을 사용하여 클러스터가 성공적으로 생성되었는지 확인할 수 있다.\n$ kubectl cluster-info Kubernetes control plane is running at https://127.0.0.1:57074 CoreDNS is running at https://127.0.0.1:57074/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. tekton pipeline을 설치한다.\n$ kubectl apply --filename \\ https://storage.googleapis.com/tekton-releases/pipeline/latest/release.yaml $ kubectl get pod -n tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-pipelines-controller-55487dcfb8-vww2f 1/1 Running 0 31s tekton-pipelines-webhook-794864555f-g9fnm 1/1 Running 0 31s hello-world.yaml 파일을 아래와 같이 작성한다.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: hello spec: steps: - name: echo image: alpine script: | #!/bin/sh echo \u0026#34;Hello World\u0026#34; 작성한 hello-world.yaml 파일을 배포한다.\n$ kubectl apply -f hello-world.yaml task.tekton.dev/hello created True, Succeeded는 Task가 정상적으로 완료되었다는 의미이다.\n$ kubectl get taskrun hello-task-run NAME SUCCEEDED REASON STARTTIME COMPLETIONTIME hello-task-run True Succeeded 29s 5s 실행된 Task의 결과(로그)를 확인해본다.\n$ kubectl logs --selector=tekton.dev/taskRun=hello-task-run Hello World 실행한 Task 로그에 Hello World가 출력됐다.\nTask가 정상 실행된 걸 확인할 수 있다.\n2. Pipeline 데모 # 두 번째 테스크 생성하고 실행하기\nHello World Task가 이미 있다. 이제 두 번째 Task인 Goodbye World를 만들어본다.\n파일명은 goodbye-world.yaml 이다.\napiVersion: tekton.dev/v1beta1 kind: Task metadata: name: goodbye spec: steps: - name: goodbye image: ubuntu script: | #!/bin/bash echo \u0026#34;Goodbye World!\u0026#34; 작성한 goodbye-world.yaml 파일을 배포한다.\n$ kubectl apply --filename goodbye-world.yaml task.tekton.dev/goodbye created Pipeline 생성\nhello-goodbye-pipeline.yaml 파일을 작성한다.\napiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: hello-goodbye spec: tasks: - name: hello taskRef: name: hello - name: goodbye runAfter: - hello taskRef: name: goodbye 작성한 파일을 배포한다.\n$ kubectl apply --filename hello-goodbye-pipeline.yaml PipelineRun 개체로 파이프라인을 인스턴스화한다.\nhello-goodbye-pipeline-run.yaml이라는 새 파일을 아래와 같이 작성한다.\napiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: name: hello-goodbye-run spec: pipelineRef: name: hello-goodbye 작성한 파일을 배포한다.\n$ kubectl apply --filename hello-goodbye-pipeline-run.yaml pipelinerun.tekton.dev/hello-goodbye-run created Tekton CLI를 사용해서 hello-goodbye-run Pipelien Run의 로그를 확인한다.\n$ tkn pipelinerun logs hello-goodbye-run -f -n default tektoncd-cli 설치 # 참고로 tkn 명령어를 사용하려면 tektoncd-cli 설치가 필요하다.\nbrew 명령어로 tektoncd-cli를 설치할 수 있다.\n$ brew install tektoncd-cli 실행된 파이프라인 로그에서 hello task와 goodbye task가 모두 정상 실행된 결과를 확인할 수 있다.\n$ tkn pipelinerun logs hello-goodbye-run -f -n default [hello : echo] Hello World [goodbye : goodbye] Goodbye World! 3. Tekton Dashboard 설치 # Tekton Dashboard는 Tekton을 관리하기 편하게 해주는 Web UI이다.\n최신 버전의 Tekton Dashboard를 설치하려면 아래 명령어를 실행한다.\n$ kubectl apply --filename https://github.com/tektoncd/dashboard/releases/latest/download/tekton-dashboard-release.yaml tekton-dashboard-xxx Pod가 새로 생성된 걸 확인할 수 있다.\n$ kubectl get pods --namespace tekton-pipelines NAME READY STATUS RESTARTS AGE tekton-dashboard-6c66f85968-f42h5 1/1 Running 0 34s tekton-pipelines-controller-55487dcfb8-vww2f 1/1 Running 0 8m21s tekton-pipelines-webhook-794864555f-g9fnm 1/1 Running 0 8m21s Tekton Dashboard에 접속하기 위해 포트를 설정한다.\n$ kubectl proxy --port=8080 그 다음 웹 브라우저에서 아래 주소를 입력해 Tekton Dashboard로 접속 가능하다.\nhttp://localhost:8080/api/v1/namespaces/tekton-pipelines/services/tekton-dashboard:http/proxy/#/clustertasks 또는 이 방법도 가능하다.\n$ kubectl --namespace tekton-pipelines port-forward svc/tekton-dashboard 9097:9097 위 명령어가 실행된 후 웹 브라우저를 열고 http://localhost:9097로 접속하면 Tekton Dashboard 화면이 나타난다.\n끝!\n참고자료 # Tekton 공식문서\n","date":"May 2, 2022","permalink":"/blog/installing-tekton-on-minikube/","section":"Blogs","summary":"개요 # minikube를 통해 Tekton Pipeline과 Tekton Dashboard를 설치하고 데모를 구성해본다.","title":"Tekton 설치"},{"content":"증상 # Lambda function을 생성할 때 ECR 권한 에러 메세지가 출력되며 생성이 불가능한 증상.\n에러 메세지\n람다가 ECR Image에 접근할 권한이 없으니, 먼저 ECRElastic Container Registry의 권한을 확인해보라고 알려주고 있다.\nLambda does not have permission to access the ECR image. Check the ECR permissions. 원인 # 에러 메세지에 이미 정답이 있었다. 이래서 로그의 중요성을 절대 간과해서도 의심해서도 안된다.\nECRElastic Container Registry에 Lambda function 접근을 허용해주는 Permission이 등록되어 있지 않았다.\nLambda function이 ECR에 접근해 컨테이너 이미지를 받아올 때, 권한이 거부되어 실패하는 것이다.\n해결방법 # 람다 전용 IAM Role 생성 # Lambda function에 부여할 IAM Role을 생성한다.\n이 IAM Role에는 Lambda function이 ECR에 인증을 받고, Repository에 접근해서 컨테이너 이미지를 받아오고 이미지 목록을 확인할 수 있는 권한이 부여되어 있다.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaToECR1\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:SetRepositoryPolicy\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34;, \u0026#34;ecr:Get*\u0026#34;, \u0026#34;ecr:Describe*\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34;, \u0026#34;ecr:ListImages\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:PutImage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:ecr:ap-northeast-2:ACCOUNT_ID:repository/REPOSITORY_NAME\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaToECR2\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Resource 값의 리전 ap-northeast-2, ACCOUNT_ID, REPOSITORY_NAME 값은 자신의 환경에 맞게 바꾸자.\nECR 권한 설정 # AWS Management Console에 로그인 한 다음, ECRElastic Container Registry 서비스로 들어간다.\n그후 Lambda function를 연결할 ECR 레포지터리를 선택한다.\nAmaozn ECR의 컨테이너 이미지와 동일한 계정에 있는 Lambda Function의 경우 Amazon ECR 레포지토리에 권한을 추가해줘야 한다. 다음 예는 ECR 레포지터리의 최소 권한 정책을 보여준다.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2008-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaECRImageRetrievalPolicy\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:DeleteRepositoryPolicy\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:GetRepositoryPolicy\u0026#34;, \u0026#34;ecr:SetRepositoryPolicy\u0026#34; ], \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;aws:sourceArn\u0026#34;: \u0026#34;arn:aws:lambda:ap-northeast-2:ACCOUNT_ID:function:*\u0026#34; } } } ] } 위 정책 값을 ECR의 Permissions에 넣어주면, Lambda function이 해당 ECR 레포지터리에 접근할 수 있게 된다.\n그러면 ECR 레포지터리의 Permissions 메뉴에는 이렇게 변환되어 보인다.\nECR에 권한을 부여한 다음, Lambda function에 롤을 부여해서 다시 생성해보면 문제없이 생성되는 걸 확인할 수 있다.\n이것으로 권한 문제는 조치 완료.\n참고자료 # AWS 공식문서\nDeploying Lambda functions as container images\n","date":"Apr 26, 2022","permalink":"/blog/lambda-to-ecr-error/","section":"Blogs","summary":"증상 # Lambda function을 생성할 때 ECR 권한 에러 메세지가 출력되며 생성이 불가능한 증상.","title":"Check the ECR permission"},{"content":"","date":"Apr 21, 2022","permalink":"/tags/celery/","section":"Tags","summary":"","title":"celery"},{"content":"개요 # Celery worker의 Queue를 설정하는 방법을 소개합니다.\n환경 # Celery : 5.2.3 (dawn-chorus) Python : 3.8.13 Shell : bash 본문 # 테스크 이름으로 큐 분류 # Task 파일 수정\napp.conf.task_queues 설정을 통해 테스크 이름에 따라 큐를 분류할 수 있다.\n# celery_tasks.py from celery import Celery from kombu import Queue import time app = Celery(\u0026#39;tasks\u0026#39;, broker=\u0026#39;redis://localhost:6379\u0026#39;, backend=\u0026#39;redis://localhost:6379\u0026#39;) app.conf.task_default_queue = \u0026#39;default\u0026#39; app.conf.task_queues = ( Queue(\u0026#39;normal_tasks\u0026#39;, routing_key=\u0026#39;normal.#\u0026#39;), Queue(\u0026#39;urgent_tasks\u0026#39;, routing_key=\u0026#39;urgent.#\u0026#39;), ) @app.task def normal_task(x): time.sleep(x) return x @app.task def urgent_task(x): return x Celery worker 띄우기\ncelery_tasks.py 파일을 작성했다. 이제 쉘에서 워커를 띄우면 된다.\n위 시나리오에서는 Queue가 총 2개(slow_tasks, quick_tasks)이기 때문에, Celery Worker도 각각 큐마다 띄워주어야 한다.\n$ celery -A celery_tasks worker -Q normal_tasks -l INFO $ celery -A celery_tasks worker -Q urgent_tasks -l INFO 명령어 옵션 설명\n-A: Celery Worker가 수행할 App 이름(파일명)을 지정하는 옵션\n-Q: Queue 이름을 지정하는 옵션\n-l(--loglevel): 출력할 로그레벨 옵션. Celery worker의 로그 레벨에는 DEBUG(디버그), INFO(정보성), WARNING(경고), ERROR(오류), CRITICAL(치명), FATAL(심각)이 있다.\n특정 테스크에만 큐 지정하기 # @app.task 데코레이터에 queue='QUEUE_NAME' 파라미터를 지정하는 방법도 있다.\n@app.task(queue=\u0026#39;normal_tasks\u0026#39;) def normal_task(x): time.sleep(x) return x Celery worker 인스턴스를 동작시키는 방법은 위에 내용을 참고하면 된다.\n참고자료 # How to route tasks to different queues with Celery and Django\n특정 테스크에만 큐 지정하는 방법을 소개하는 글.\n","date":"Apr 21, 2022","permalink":"/blog/routing-celery-tasks/","section":"Blogs","summary":"개요 # Celery worker의 Queue를 설정하는 방법을 소개합니다.","title":"Celery task의 queue 라우팅"},{"content":"","date":"Apr 21, 2022","permalink":"/tags/python/","section":"Tags","summary":"","title":"python"},{"content":"개요 # 특정 레포지터리의 전체 커밋로그를 삭제(초기화)하는 방법을 설명한다.\n내 Public Repository에 크리티컬한 보안 이슈를 일으킬만한 이미지 파일이나 인증 정보 등이 노출되었을 경우 이 방법을 쓰자.\n원리 # 전체 커밋로그 삭제(초기화) 원리는 간단하다.\nmain 브랜치를 복제해서 latest_branch 브랜치를 만든다. main 브렌치를 삭제한다. main 브랜치가 삭제될 때 그 안에 포함된 Commit log 전체도 같이 삭제된다. latest_branch의 이름을 main으로 변경한다. 끝. 주의사항\n이 방법을 쓸 경우, 지금까지의 main 브랜치의 전체 커밋 로그(히스토리)가 사라진다.\n삭제된 커밋로그는 복구할 방법이 없다는 사실을 명심한다.\n해결법 # Checkout # # [\u0026gt;] main $ git checkout --orphan latest_branch checkout은 새로운 브랜치를 만드는 명령어이다.\nlatest_branch라는 이름의 브랜치를 생성한다.\n전체 파일 Add # # [ ] main # [\u0026gt;] latest_branch $ git add -A -A는 전체 파일을 추가하는 옵션이다.\n전체 파일을 새로 만든 `latest_branch에 추가한다.\n# [ ] main # [\u0026gt;] latest_branch $ git commit -am \u0026#34;Initial commit\u0026#34; 전체 파일을 그대로 새로 만든 브랜치 latest_branch에 Commit 한다.\n브랜치 삭제 # # [X] main # [\u0026gt;] latest_branch $ git branch -D main 기존 main 브랜치를 삭제한다. main 브랜치를 없애는 이유는 핵심 목적인 Commit Log 전체를 삭제하기 위해서이다.\n현재 브랜치 이름을 main으로 변경 # # [\u0026gt;] latest_branch --\u0026gt; main $ git branch -m main -m은 브랜치의 이름을 변경하는 옵션이다.\n강제 업데이트 # # [\u0026gt;] main $ git push -f origin main 마지막으로 커밋한 전체 파일을 강제로 main 브랜치에 올린다.\n결과확인 # $ git log --graph * commit c2af2fd2490bd4bdaeea9daaa5193deb2927a7d1 (HEAD -\u0026gt; main, origin/main, origin/HEAD) | Author: seyslee \u0026lt;username@xxxxx.com\u0026gt; | Date: Fri Apr 15 22:09:51 2022 +0900 | | rebuilding site 2022년 4월 15일 금요일 22시 09분 51초 KST | * commit 265451476ff6acb4e9998940f8c90acbf03225cf | Author: seyslee \u0026lt;username@xxxxx.com\u0026gt; | Date: Fri Apr 15 22:08:28 2022 +0900 | | rebuilding site 2022년 4월 15일 금요일 22시 08분 28초 KST | * commit 9dc438a22eddd7a6074170a0201bba0fa632c107 Author: seyslee \u0026lt;username@xxxxx.com\u0026gt; Date: Fri Apr 15 22:03:35 2022 +0900 Initial commit --graph 옵션은 커밋 로그 전체를 트리 형태로 그려준다.\n지금까지의 전체 커밋로그가 사라졌다. 이제 중요 정보가 노출된 커밋로그는 이 세상에서 사라졌다.\n조치 끝!\n결론 # rm -rf .git 명령어로 .git 디렉토리를 지우는 건 권장하지 않는다.\n서브모듈 설정 등도 .git 안에 모두 포함되어 있는데, 이것들 다시 설정 잡는 것도 번거롭고 전체 레포지터리 설정을 날리는게 위험하기도 하니까.\n애초에 작업 목적을 잘 생각해보면 우리는 레포지터리의 커밋 로그 전체를 날리고 싶었던 것이다.\nmain 브랜치를 그대로 다른 브랜치에 복제 떠서 백업한 후 main 브랜치 자체를 날리는게 훨씬 간단하고 안전하다. 브랜치가 삭제될 때 그 안의 모든 커밋 로그도 같이 삭제되니까.\n","date":"Apr 15, 2022","permalink":"/blog/initialize-entire-commit-log/","section":"Blogs","summary":"개요 # 특정 레포지터리의 전체 커밋로그를 삭제(초기화)하는 방법을 설명한다.","title":"git 커밋로그 전체 삭제"},{"content":"","date":"Apr 15, 2022","permalink":"/tags/github/","section":"Tags","summary":"","title":"github"},{"content":"개요 # EC2 인스턴스의 디스크 공간이 부족한 상황에서는 EBS 볼륨 용량을 늘리거나 파일시스템에 불필요한 파일들을 정리해서 여유공간을 다시 확보하는 조치가 필요하다.\n이 문서에서는 Linux EC2 인스턴스의 EBSElastic Block Storage Volume의 용량을 늘리는 방법을 소개한다.\n환경 # OS : Amazon Linux 2 Type : t2.micro EBS Volume Type: gp3 Size: 10GiB → gp3 30GiB (xfs 타입) Shell : bash ID : ec2-user gp3\n배경지식으로 설명하자면 EBS 볼륨은 되도록이면 gp2 타입의 다음 세대인 gp3 타입을 사용하도록 하자. 비용이나 성능 측면에서 모두 월등하다.\n참고로 gp2 타입은 2014년에 처음 출시되었다.\nAWS 블로그 포스팅에 따르면 gp3의 최고 성능은 gp2 볼륨의 최대 처리량보다 4배 빠르다고 한다. 가격도 기존 gp2 볼륨 타입보다 20% 저렴하다.\nAWS 블로그 포스팅\n전제조건 # AWS Management Console에 미리 로그인되어 있어야 함 SSH나 Session Manager 등으로 EC2 Instance에 원격접속 가능한 네트워크 환경 해결방법 # AWS Management Console # EBSElastic Block Storage Volume의 용량을 늘리는 작업은 AWS Management Console에서 진행해야한다.\n충분한 권한을 가진 AWS 계정으로 로그인했다는 전제하에 다음 과정을 진행한다.\nEC2 서비스 메뉴 → 왼쪽 사이드바 메뉴 → Volumes 클릭\n이후 작업대상 EC2에 부착된 EBS Volume을 체크해준다.\n우측 상단에 Actions → Modify volume 클릭.\nEBS 볼륨의 용량 값(Size (GiB))을 원하는 값으로 늘린다.\n현재 시나리오에서는 10GB를 30GB로 변경헀다.\n위 안내문을 요약하자면 다음과 같다.\nAWS Console에서 늘리면 작업이 끝나는 게 아니라, EC2 인스턴스에 접속해서 일련의 추가 명령어 실행을 해야 완벽히 적용된다. AWS Console 영역에서 EBS Volume 용량 변경이 완료되는 데에는 몇 분 정도 소요될 수 있다. EBS Volume 용량 변경이 시작될 때부터 EBS Volume의 비용은 바뀐 용량 기준으로 청구된다. 안내문 내용을 이해했다면 [Modify] 버튼을 클릭한다.\nEC2 # 1. EC2 접속 # EC2에 SSH 또는 Session Manager를 통해 접속한다.\n각자 환경에서 접속 가능한 방법으로 EC2에 연결하면 된다.\n2. 디스크 구성 및 타입 확인 # root 파일시스템의 타입이 xfs인 걸 확인할 수 있다.\n[ec2-user@test-ec2 ~]$ df -hT Filesystem Type Size Used Avail Use% Mounted on devtmpfs devtmpfs 474M 0 474M 0% /dev tmpfs tmpfs 483M 0 483M 0% /dev/shm tmpfs tmpfs 483M 404K 483M 1% /run tmpfs tmpfs 483M 0 483M 0% /sys/fs/cgroup /dev/xvda1 xfs 10G 1.6G 8.5G 16% / tmpfs tmpfs 97M 0 97M 0% /run/user/0 3. lsblk 확인 # [ec2-user@test-ec2 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 30G 0 disk └─xvda1 202:1 0 10G 0 part / 루트 볼륨 /dev/xvda는 /dev/xvda1 파티션 하나로만 구성되어 있다.\nxvda 볼륨 전체용량은 30G이지만 xvda1 파티션의 용량은 10G만 할당되어 있으므로 확장이 필요하다.\n4. 볼륨 확장 # xvda1 파티션의 용량을 확장시킨다.\n[ec2-user@test-ec2 ~]$ sudo growpart /dev/xvda 1 CHANGED: partition=1 start=4096 old: size=20967391 end=20971487 new: size=62910431 end=62914527 xvda1 파티션의 용량이 정상적으로 변경(CHANGED)된 걸 확인할 수 있다.\n5. lsblk 재확인 # lsblk 명령어를 다시 실행한다.\n[ec2-user@test-ec2 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 30G 0 disk └─xvda1 202:1 0 30G 0 part / xvda1 파티션이 10G에서 30G로 확장된 걸 확인할 수 있다.\n루트 파일시스템(/)의 디스크 사용률을 확인했을 때는 16%로 그대로이다.\n아직 마지막 명령어가 남아있다.\n[ec2-user@test-ec2 ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 474M 0 474M 0% /dev tmpfs 483M 0 483M 0% /dev/shm tmpfs 483M 436K 483M 1% /run tmpfs 483M 0 483M 0% /sys/fs/cgroup /dev/xvda1 10G 1.6G 8.5G 16% / tmpfs 97M 0 97M 0% /run/user/0 6. 파일시스템 확장 (xfs) # 각 볼륨에서 파일 시스템을 확장하려면 xfs_growfs 명령을 사용하면 된다.\n아래 명령어는 루트 파일시스템(/)을 확장시킨다.\n[ec2-user@test-ec2 ~]$ sudo xfs_growfs -d / meta-data=/dev/xvda1 isize=512 agcount=6, agsize=524159 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1 spinodes=0 data = bsize=4096 blocks=2620923, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=0 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 data blocks changed from 2620923 to 7863803 만약 EC2에 XFS 툴이 설치되지 않은 경우, 다음과 같이 xfsprogs 패키지를 설치하면 된다.\nxfsprogs 패키지에는 XFS 파일 시스템을 위한 관리 도구와 디버깅 도구가 포함되어 있다.\n[ec2-user@test-ec2 ~]$ sudo yum install xfsprogs df -h 명령어로 파일시스템 사용률을 확인한다.\n루트 파일시스템(/)의 용량이 10G에서 30G로 증가했다.\n루트 파일시스템(/)의 사용률도 16%에서 6%로 감소되었다.\n[ec2-user@test-ec2 ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 474M 0 474M 0% /dev tmpfs 483M 0 483M 0% /dev/shm tmpfs 483M 436K 483M 1% /run tmpfs 483M 0 483M 0% /sys/fs/cgroup /dev/xvda1 30G 1.7G 29G 6% / tmpfs 97M 0 97M 0% /run/user/0 이제 EC2 인스턴스의 디스크EBS Volume 증설 작업은 정상 완료됐다.\n결론 # EC2 디스크 증설 작업은 종종 발생하기 때문에 익혀두면 좋다.\nEBS 볼륨의 파일 시스템 타입이 xfs인지 ext4에 따라 설정 방법이 살짝 달라진다는 점을 명심하자.\n참고자료 # AWS 공식문서\nExtend a Linux file system after resizing a volume\n","date":"Apr 15, 2022","permalink":"/blog/extending-linux-fs-after-resizing-ebs-volume/","section":"Blogs","summary":"개요 # EC2 인스턴스의 디스크 공간이 부족한 상황에서는 EBS 볼륨 용량을 늘리거나 파일시스템에 불필요한 파일들을 정리해서 여유공간을 다시 확보하는 조치가 필요하다.","title":"EC2 디스크 증설"},{"content":"","date":"Apr 15, 2022","permalink":"/tags/os/","section":"Tags","summary":"","title":"os"},{"content":"","date":"Mar 22, 2022","permalink":"/tags/hugo/","section":"Tags","summary":"","title":"hugo"},{"content":"개요 # Hugo 블로그를 운영하면서 발생할 수 있는 found no layout file for ... 에러를 해결한다.\n환경 # Hardware : MacBook Pro(14\u0026quot;, 2021, M1 Pro) OS : macOS Monterey 12.3 Shell : zsh Hugo : v0.95.0 (Brew로 설치함) Browser : Chrome 99.0.4844.83 증상 # 기존에 사용중이던 맥북에서 신규 맥북으로 옮겨서 블로그 레포지터리를 클론했다.\n이제 신규 맥북에서 Github 블로그를 관리하려는 목적이었다.\n$ git clone https://github.com/seyslee/blog.git 현재 내가 위치한 경로를 확인한다.\n$ pwd /Users/ive/github/personal/blog Hugo 테스트 웹서버를 실행했다.\n여기서 -D 옵션은 숨김 처리(draft: true)되어 있는 게시글도 모두 표시하겠다는 설정이다.\n$ hugo server -D ... WARN 2022/03/22 18:48:28 found no layout file for \u0026#34;HTML\u0026#34; for kind \u0026#34;term\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. WARN 2022/03/22 18:48:28 found no layout file for \u0026#34;HTML\u0026#34; for kind \u0026#34;term\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. WARN 2022/03/22 18:48:28 found no layout file for \u0026#34;HTML\u0026#34; for kind \u0026#34;term\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. WARN 2022/03/22 18:48:28 found no layout file for \u0026#34;HTML\u0026#34; for kind \u0026#34;term\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. WARN 2022/03/22 18:48:28 found no layout file for \u0026#34;HTML\u0026#34; for kind \u0026#34;term\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. | EN -------------------+------ Pages | 29 Paginator pages | 0 Non-page files | 114 Static files | 3 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 103 ms Watching for changes in /Users/xxxxx/github/personal/blog/{archetypes,content,layouts,static} Watching for config changes in /Users/xxxxx/github/personal/blog/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 잘 실행되는 것처럼 보이지만 아래 에러 메세지를 주목하자.\nfile for \u0026#34;HTML\u0026#34; for kind \u0026#34;term\u0026#34;: You should create a template file which matches Hugo Layouts Lookup Rules for this combination. template file을 인식하지 못하는 문제가 있다.\n테스트용 웹페이지 주소인 http://localhost:1313/에 접속해도 빈 페이지가 뜬다.\n원인 # 복제해온 레포지터리 안에 포함된 hugo-theme-codex Submodule이 비어 있었다.\nBlog repository는 제대로 복사Clone해서 가져왔지만, 안에 포함된 Submodule을 가져오지 못한 문제였다.\nblog/ [Blog repository] └── themes/ └── hugo-theme-codex/ [Submodule] └── . 위 hugo-theme-codex 디렉토리는 내가 사용하는 테마명이다. 이 글을 보는 다른 사람들은 다른 테마명으로 보일 수 있으니 참고하자.\n실제로 Submodule 안에 파일들을 확인해본 결과이다.\n$ cd themes $ ls hugo-theme-codex $ cd hugo-theme-codex $ ls -al total 0 drwxr-xr-x 2 xxxxx staff 64 3 22 18:53 . drwxr-xr-x 4 xxxxx staff 128 3 22 18:53 .. hugo-theme-codex 디렉토리 안에 파일이 존재하지 않는다.\n해결방법 # git clone # 레포지터리를 복제할 때 해당 레포지터리에 안에 포함된 Submodule도 함께 복제해서 가져오도록 --recursive 옵션을 주면 해결된다.\n기존에 로컬로 복제해온 레포지터리를 rm -rf blog으로 삭제한 다음, 아래 명령어를 실행해서 다시 복제해서 가져오도록 한다.\n$ git clone https://github.com/\u0026lt;USERNAME\u0026gt;/\u0026lt;REPONAME\u0026gt;.git --recursive 실제 명령어 실행결과는 다음과 같다.\n$ git clone https://github.com/seyslee/blog.git --recursive Cloning into \u0026#39;blog\u0026#39;... remote: Enumerating objects: 1909, done. remote: Counting objects: 100% (1909/1909), done. remote: Compressing objects: 100% (1334/1334), done. remote: Total 1909 (delta 1116), reused 1316 (delta 523), pack-reused 0 Receiving objects: 100% (1909/1909), 81.07 MiB | 3.20 MiB/s, done. Resolving deltas: 100% (1116/1116), done. Submodule \u0026#39;public\u0026#39; (https://github.com/seyslee/seyslee.github.io.git) registered for path \u0026#39;public\u0026#39; Submodule \u0026#39;themes/hugo-theme-codex\u0026#39; (https://github.com/jakewies/hugo-theme-codex.git) registered for path \u0026#39;themes/hugo-theme-codex\u0026#39; Cloning into \u0026#39;/Users/xxxxx/github/personal/blog/public\u0026#39;... remote: Enumerating objects: 1054, done. remote: Counting objects: 100% (1054/1054), done. remote: Compressing objects: 100% (609/609), done. remote: Total 1054 (delta 576), reused 814 (delta 339), pack-reused 0 Receiving objects: 100% (1054/1054), 79.73 MiB | 3.15 MiB/s, done. Resolving deltas: 100% (576/576), done. Cloning into \u0026#39;/Users/xxxxx/github/personal/blog/themes/hugo-theme-codex\u0026#39;... remote: Enumerating objects: 1391, done. remote: Counting objects: 100% (49/49), done. remote: Compressing objects: 100% (48/48), done. remote: Total 1391 (delta 20), reused 9 (delta 1), pack-reused 1342 Receiving objects: 100% (1391/1391), 604.03 KiB | 839.00 KiB/s, done. Resolving deltas: 100% (701/701), done. Submodule path \u0026#39;public\u0026#39;: checked out \u0026#39;af91d944400180ec2afdb85205dfef19f3a8556c\u0026#39; Submodule path \u0026#39;themes/hugo-theme-codex\u0026#39;: checked out \u0026#39;9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da\u0026#39; --recursive 옵션으로 인해 themes/hugo-theme-codex 와 public 서브모듈도 함께 받아오는 걸cloning 확인할 수 있다.\n테스트 # 블로그 테스트 웹페이지를 다시 띄워보자.\n$ hugo server -D Start building sites … hugo v0.95.0+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 114 Paginator pages | 0 Non-page files | 114 Static files | 14 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 112 ms Watching for changes in /Users/xxxxx/github/personal/blog/{archetypes,content,layouts,static,themes} Watching for config changes in /Users/xxxxx/github/personal/blog/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 아까와 같은 에러 메세지는 출력되지 않는다.\nChrome 브라우저를 열어서 테스트 페이지 http://localhost:1313에 접속해본다.\nHugo 블로그가 문제없이 잘 보인다.\n핵심 원인은 최초에 클론해서 가져올 때, 블로그 안에 포함된 테마 서브모듈을 제대로 가져오지 못한 문제였다.\ngit clone 할 때 submodule이 포함되어 있는 레포지터리라면, --recursive 옵션을 주도록 하자.\n참고자료 # https://nochoco-lee.tistory.com/87\nhttps://discourse.gohugo.io/t/found-no-layout-file-for/36512\n","date":"Mar 22, 2022","permalink":"/blog/found-no-layout-file-in-hugo/","section":"Blogs","summary":"개요 # Hugo 블로그를 운영하면서 발생할 수 있는 found no layout file for .","title":"hugo found no layout file 에러"},{"content":"개요 # 멀티 클러스터 기반의 쿠버네티스를 관리할 때 생산성Productivity을 높여주는 플러그인들을 설치하고 사용하는 방법을 안내하는 문서이다.\n환경 # OS : macOS Monterey 12.3 (M1 Pro) Shell : zsh + oh-my-zsh Terminal : iTerm 2 mac용 패키지 관리자 : Homebrew 3.4.3 전제조건 # brew\nmacOS 패키지 관리자인 brew가 이미 설치된 환경이어야 합니다.\n이 글에서는 brew 설치방법을 다루지 않습니다.\n플러그인 목록 # 1. k9s # 설명 # 쿠버네티스 전체 관리 툴. 알록달록한 컬러 표시, 표시된 정보가 실시간으로 바뀌는 Interactive 기능, TUITerminal User Interface 기반이라 kubectl 명령어 입력 없이 방향키와 단축키만으로 클러스터와 관련된 모든 작업이 가능해진다.\n설치방법 # homebrew로 설치한다.\n$ brew install k9s 사용법 예시 # 쿠버네티스 클러스터에 접근 가능한 환경에서 아래 명령어를 실행하면 k9s 관리 창이 뜬다.\n$ k9s 위 스크린샷은 k9s에서 전체 파드를 보고 있는 화면이다.\n자세한 k9s 조작법은 k9s 깃허브 레포지터리를 참고한다.\n2. kubecolor # 설명 # 가독성 향상 플러그인. kubectl 명령어 결과의 각 컬럼에 색깔을 표시해서 출력해준다. kubectl 명령어 결과값에 가독성을 높여줌.\n2022년 3월 기준으로, 만든 사람이 플러그인 업데이트를 잘 안하고 있는 것으로 확인됨.\n설치방법 # homebrew로 설치한다.\n$ brew install kubecolor 사용법 예시 # $ kubecolor get pod kubectl 명령어에 컬러 표시를 항상 적용하고 싶다면, 쉘 설정파일 안에 alias 설정을 추가하면 더 편하게 사용 가능하다. 아래는 zsh 기준의 설정방법.\n$ vi ~/.zshrc ... # kubecolor alias kubectl=kubecolor ... 3. krew # 설명 # 패키지 관리자. krew는 kubectl 플러그인 패키지 매니저이다. 쿠버네티스 전용 homebrew라고 이해하면 된다.\n설치방법 # homebrew로 설치한다.\n$ brew install krew 사용법 예시 # $ kubectl krew list PLUGIN VERSION ctx v0.9.4 krew v0.4.3 ns v0.9.4 tree v0.4.1 krew로 설치한 플러그인 목록을 출력하는 명령어이다.\n$ kubectl krew update krew로 설치한 플러그인 전체의 최신 버전을 확인하는 명령어.\n4. stern # 설명 # 여러 대의 파드, 컨테이너 동시 모니터링 가능. stern과 비슷한 기능을 하는 플러그인으로는 kail이 있다.\n설치방법 # homebrew로 설치한다.\n$ brew install stern 사용법 # $ stern -n prometheus sample-prom-pod -n : 네임스페이스 지정\nsample-prom-pod : (예시) 파드 이름에 sample-prom-pod가 포함된 파드들의 로그만 실시간 모니터링tail한다.\n5. tree # 설명 # kubernetes 리소스 간의 관계를 트리형태로 쉽게 표현해준다.\n설치방법 # krew로 설치한다.\n$ kubectl krew install tree 사용법 예시 # $ kubectl tree deploy sample-redis NAMESPACE NAME READY REASON AGE sample Deployment/sample-redis - 138d sample ├─ReplicaSet/sample-redis-000df0b0 - 138d sample ├─ReplicaSet/sample-redis-x00x0c000 - 125d sample └─ReplicaSet/sample-redis-xx0b00xxc - 67d sample └─Pod/sample-redis-xx5x00xxc-zqbrk True 2d12h 특정 Deployment에 속한 ReplicaSet과 Pod 정보를 트리 형태로 표현해준다.\n6. kubectx \u0026amp; kubens # 쿠버네티스의 컨텍스트를 여러개 사용하고 있거나, 네임스페이스를 여러개 사용하고 있을 때 필요한 플러그인들이다. 몇 글자 안되는 짧은 명령어로 클러스터 간의 이동, 네임스페이스 간의 이동이 가능하므로 멀티 클러스터 환경을 관리하는 엔지니어라면 반드시 사용하는 걸 추천한다.\n설명 # kubectx는 컨텍스트를 쉽게 변경할 수 있도록 도움을 준다. kubectl config use-context dev-cluster 같은 긴 명령어를 사용하지 않아도 된다.\nkubens는 기본 네임스페이스를 변경할 수 있도록 도와준다. 이 두 도구 모드 [Tab] 완성기능을 지원한다. 그 뿐만 아니라, fzffuzzy finder를 설치하면 대화식 메뉴를 제공하기 때문에, 더 편하게 사용할 수 있다.\n설치방법 # krew로 설치한다.\n$ kubectl krew install kubectx $ kubectl krew install kubens 설치 후 툴은 kubectl ctx 및 kubectl ns 명령어로 사용할 수 있다.\nkubectl 명령어가 k로 alias 설정되어 있다면 k ctx, k ns로 더 축약해서 사용 가능하다.\n사용법 예시 # fzf 플러그인이 같이 설치되어 있는 상태에서 실행하면, 아래처럼 방향키를 통해 이동해서 선택 가능한 대화식 메뉴로 동작한다.\n$ kubectl ctx \u0026gt; docker-desktop dev-cluster qa-cluster prod-cluster 4/4 $ kubectl ns default redis \u0026gt; prometheus grafana nginx 5/5 7. kubectl 자동완성 # 설명 # kubectl 자동완성은 플러그인은 아니고, kubectl에서 기본 지원하는 기능으로 추가 설치는 필요없다.\n설정방법 # 아래는 kubectl 자동완성 기능 설정 방법이다. (zsh 기준)\n$ vi ~/.zshrc ... # kubectl autocompletion source \u0026lt;(kubectl completion zsh) ... zsh 시작 시 kubectl의 자동완성 기능을 기본 활성화한다.\n$ source ~/.zshrc 변경한 설정내용을 바로 적용한다.\n이제 kubectl 명령어 입력 후 [tab] 키를 눌러본다.\n$ kubectl [tab] [tab] 키를 누르면 아래와 같이 kubectl 명령어 다음에 올 수 있는 하위 명령어 리스트가 출력된다. 이 상태에서 [tab] 키를 한 번 더 누르면 대화형 메뉴처럼 방향키와 엔터로 선택 가능하다.\n$ kubectl alpha -- Commands for features in alpha annotate -- 자원에 대한 주석을 업데이트합니다 api-resources -- Print the supported API resources on the server api-versions -- Print the supported API versions on the server, in the form of \u0026#34;group/version\u0026#34; apply -- Apply a configuration to a resource by file name or stdin attach -- Attach to a running container ... 참고사항 # 내 경우는 kubecolor랑 kubectl 자동완성autocomplete을 동시에 사용하면 버그가 발생했다. 따라서 두 기능을 같이 사용하는 것은 권장하지 않는다.\n현재 이 이슈는 3월 28일 기준으로 아직 해결되지 않은 상태이다. 해당 깃허브 이슈는 autocompletion bug #78에 등록되어 있다.\n마치며 # 유용한 쿠버네티스 플러그인을 추가로 발견할 때마다 글을 업데이트하고 있습니다.\n이 외에 공유하고 싶은 쿠버네티스 플러그인이 있다면 언제든 댓글로 남겨주세요.\n","date":"Mar 21, 2022","permalink":"/blog/k8s/setup-k8s-tools/","section":"Blogs","summary":"개요 # 멀티 클러스터 기반의 쿠버네티스를 관리할 때 생산성Productivity을 높여주는 플러그인들을 설치하고 사용하는 방법을 안내하는 문서이다.","title":"필수 쿠버네티스 관리 툴"},{"content":"개요 # 쿠버네티스 워커 노드가 파일시스템 사용률이 높을 때, docker system prune 명령어를 실행해서 Docker 오브젝트를 정리하여 파일시스템 공간을 확보할 수 있다.\n환경 # OS : Amazon Linux 2 Shell : bash ID : ec2-user 해결방법 # $ id uid=1000(ec2-user) gid=1000(ec2-user) groups=1000(ec2-user),4(adm),10(wheel),190(systemd-journal),1950(docker) EC2 Instance에 ec2-user로 로그인한다.\n파일시스템 사용률을 확인한다.\n$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 7.7G 0 7.7G 0% /dev tmpfs 7.7G 0 7.7G 0% /dev/shm tmpfs 7.7G 1.4M 7.7G 1% /run tmpfs 7.7G 0 7.7G 0% /sys/fs/cgroup /dev/nvme0n1p1 100G 81G 20G 81% / tmpfs 1.6G 0 1.6G 0% /run/user/0 100GB EBSElastic Block Storage가 붙어있는 노드이다.\nroot 파일시스템(/)의 사용률이 81%로 높다.\n도커에서 사용중인 파일 시스템 현황을 확인한다.\n정리하여 확보할 수 있는 용량(RECLAIMABLE)이 이미지만 59.56GB이다.\n$ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 107 20 67.56GB 59.56GB (88%) Containers 46 21 2.952GB 2.763GB (93%) Local Volumes 16 10 4.433MB 230.9kB (5%) Build Cache 0 0 0B 0B 사용하지 않는 모든 Docker object들을 정리한다.\n$ docker system prune -af 명령어 옵션 설명\n-a : Dangling 이미지 말고도 사용하지 않는 컨테이너 이미지들을 모두 삭제\n-f : 실행할 것인지에 대한 여부를 묻지 않고 바로 실행한다.\n$ docker system prune -af Deleted Containers: b2da4de9da8e92a0000068846aec0000e7f4b82df4dde011408cac40499a4769 a1d989685169fb1eb0000000ab00000f041c77e0cbea0155e5da24a494bb86f4 ... Total reclaimed space: 63.92GB 사용하지 않는 Docker 오브젝트 일괄 삭제한 결과, 63.92GB 공간을 확보했다.\n정리 후 파일시스템 사용률을 다시 확인해보니, 81% → 11% 로 크게 떨어졌다.\n$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 7.7G 0 7.7G 0% /dev tmpfs 7.7G 0 7.7G 0% /dev/shm tmpfs 7.7G 1.4M 7.7G 1% /run tmpfs 7.7G 0 7.7G 0% /sys/fs/cgroup /dev/nvme0n1p1 100G 11G 90G 11% / tmpfs 1.6G 0 1.6G 0% /run/user/0 $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 16 16 6.589GB 590.3MB (8%) Containers 30 21 273.7MB 2B (0%) Local Volumes 19 7 4.435MB 622.6kB (14%) Build Cache 0 0 0B 0B 결론 # 서버의 주기적인 파일시스템 정리는 인프라 관리의 핵심 업무 중 하나이니 잘 알아두고 실무에 써먹자.\n파일시스템 정리와 같은 업무는 반복적이고 단순하기 때문에 자동화하기 좋은 사례이다. OS에서 cron 설정을 걸거나, AWS 서비스를 이용해 이제 자동화를 고민해보자.\n만약 OS 영역에서 cron을 활용해 자동화를 하고 싶다면, 이 글을 참고하면 좋다.\n참고자료 # Docker의 prune 사용법: 사용하지 않는 Docker 오브젝트 일괄 삭제\n","date":"Mar 16, 2022","permalink":"/blog/cleaning-docker-filesystem/","section":"Blogs","summary":"개요 # 쿠버네티스 워커 노드가 파일시스템 사용률이 높을 때, docker system prune 명령어를 실행해서 Docker 오브젝트를 정리하여 파일시스템 공간을 확보할 수 있다.","title":"도커 파일시스템 정리"},{"content":"개요 # 테라폼을 통해 인프라 자동화, 코드로서 인프라IaC, Infrastructure as Code를 맛보기로 체험해보겠습니다.\n테라폼 파일을 적용해서 1대의 EC2 Instance를 자동 생성하고 삭제해보겠습니다.\n배경지식 # 테라폼Terraform\n하시코프Hashicorp에서 오픈소스로 개발중인 클라우드 인프라스트럭처 자동화를 지향하는 코드로서의 인프라스트럭처Infrastructure as Code, IaC 도구입니다.\nIaC는 코드로 인프라스트럭처를 관리한다는 개념으로 테라폼에서는 하시코프 설정 언어HCL, Hashicorp Configuration Language을 사용해 클라우드 리소스를 선언합니다.\n전제조건 # macOS용 패키지 관리자인 Homebrew가 미리 설치되어 있어야 합니다.\nHomebrew 설치방법\n환경 # Hardware : MacBook Pro 13\u0026quot;, M1, 2020 OS : macOS Monterey 12.2.1 Shell : zsh Terraform : v1.1.6 Homebrew : 3.3.16 방법 # 1. 테라폼 설치 # macOS 패키지 관리자인 brew를 이용해 terraform을 설치합니다.\n$ brew install terraform ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; Updated Formulae Updated 2 formulae. ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/terraform/manifests/1.1.6 Already downloaded: /Users/ive/Library/Caches/Homebrew/downloads/27878bff7eadcac8864a00be6ce343a4540476ed6fc8d7dfc576944971ebdf51--terraform-1.1.6.bottle_manifest.json ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/terraform/blobs/sha256:f6 Already downloaded: /Users/ive/Library/Caches/Homebrew/downloads/097e36e15ba08ef7554c666a971b9989a99e6a9ac84bcaf96b51ad43bed62804--terraform--1.1.6.arm64_monterey.bottle.tar.gz ==\u0026gt; Pouring terraform--1.1.6.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/terraform/1.1.6: 6 files, 69.5MB ==\u0026gt; Running `brew cleanup terraform`... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`). terraform 명령어 동작상태를 확인합니다.\n$ terraform version Terraform v1.1.6 on darwin_arm64 arm64용 Terraform v1.1.6이 설치되었습니다.\n2. IAM User 생성 # AWS Management Console에 로그인합니다.\nIAM으로 들어가서 테라폼 전용 IAM User를 생성합니다.\nUser에 Policy로 AdministratorAccess를 부여합니다.\nTerraform을 이용해서 인스턴스를 생성하려면 Access Key와 Secrey Key가 필요하므로, 2개의 키 값을 반드시 메모해둡니다.\n3. 테라폼 파일 작성 # terraform 코드를 2개 작성합니다.\n테라폼 파일의 확장자는 .tf 입니다.\nterraform-study ├── instance.tf └── version.tf instance.tf : 서울 리전ap-northeast-2에 1개의 EC2 Instance를 생성하는 파일. instance.tf에는 Provider의 리전 정보, Access Key, Secret Key가 선언됩니다. version.tf : 테라폼 버전의 제한을 거는 파일입니다. instance.tf # $ cat instance.tf provider \u0026#34;aws\u0026#34; { access_key = \u0026#34;ACCESS_KEY_HERE\u0026#34; secret_key = \u0026#34;SECRET_KEY_HERE\u0026#34; region = \u0026#34;ap-northeast-2\u0026#34; } resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { ami = \u0026#34;ami-0dd97ebb907cf9366\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } IAM 유저를 새로 만들면서 발급된 access_key, secret_key 값을 넣어줍니다.\n주의사항\naccess_key와 secret_key 값은 AWS 계정, 패스워드와 동일한 역할을 하기 때문에 절대 깃허브 레포지터리에 올려서도 안되고, 다른 사람들에게 공유해서도 안됩니다.\n만약 다른 사람이 해당 키 값들을 획득하면 AWS의 모든 권한을 탈취해서 전체 리전에 모든 리소스를 배포하는 사고가 발생해서 몇만 달러라는 거액을 청구받을 수 있습니다. 구글링을 해보면 알겠지만 이런 사고가 생각보다 많이 발생합니다.\naccess_key와 secret_key 키 값은 본인만 갖고 있어야 한다는 점을 꼭 명심하세요!\n위 코드에서 ami는 AMIAmazon Machine Image의 ID를 의미하는 변수입니다.\nUbuntu OS가 설치된 AMIAmazon Machine Image ID는 Amazon EC2 AMI Locator 사이트에서 검색이 가능합니다.\nversion.tf # version.tf 파일은 테라폼 버전 0.12 이상에서만 해당 테라폼 코드가 실행되도록 제한하는 파일입니다.\n테라폼 버전과 프로바이더 버전에 따라서 테라폼 코드 실행결과가 다를 수가 있어서 일관된 결과를 얻기 위해 버전 제한을 걸었습니다.\n$ cat versions.tf terraform { required_version = \u0026#34;\u0026gt;= 0.12\u0026#34; } 4. 테라폼 초기화 # $ terraform init terraform init 명령어를 실행하면 클라우드 제공자 플러그인Provier plugin을 탐색한 후 설치합니다.\n$ terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/aws... - Installing hashicorp/aws v4.2.0... - Installed hashicorp/aws v4.2.0 (signed by HashiCorp) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \u0026#34;terraform init\u0026#34; in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other terraform 초기화가 시작되면서 자동으로 최신 버전의 AWS 플러그인 v4.2.0이 설치되었습니다.\n... - Installing hashicorp/aws v4.2.0... - Installed hashicorp/aws v4.2.0 (signed by HashiCorp) ... 5. 테라폼 적용 # 미리 생성될 리소스 정보를 확인합니다.\n작성한 코드에 문제가 있으면 terraform plan이 에러가 발생하고 실행되지 않습니다.\n$ terraform plan 변경될 내용을 미리 확인한 후에는 실제 적용합니다.\n$ terraform apply $ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.example will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { + ami = \u0026#34;ami-0dd97ebb907cf9366\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;t2.micro\u0026#34; + ipv6_address_count = (known after apply) + ipv6_addresses = (known after apply) + key_name = (known after apply) + monitoring = (known after apply) + outpost_arn = (known after apply) + password_data = (known after apply) + placement_group = (known after apply) + placement_partition_number = (known after apply) + primary_network_interface_id = (known after apply) + private_dns = (known after apply) + private_ip = (known after apply) + public_dns = (known after apply) + public_ip = (known after apply) + secondary_private_ips = (known after apply) + security_groups = (known after apply) + source_dest_check = true + subnet_id = (known after apply) + tags_all = (known after apply) + tenancy = (known after apply) + user_data = (known after apply) + user_data_base64 = (known after apply) + vpc_security_group_ids = (known after apply) + capacity_reservation_specification { + capacity_reservation_preference = (known after apply) + capacity_reservation_target { + capacity_reservation_id = (known after apply) } } + ebs_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + snapshot_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } + enclave_options { + enabled = (known after apply) } + ephemeral_block_device { + device_name = (known after apply) + no_device = (known after apply) + virtual_name = (known after apply) } + metadata_options { + http_endpoint = (known after apply) + http_put_response_hop_limit = (known after apply) + http_tokens = (known after apply) + instance_metadata_tags = (known after apply) } + network_interface { + delete_on_termination = (known after apply) + device_index = (known after apply) + network_interface_id = (known after apply) } + root_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: 엄청난 양의 생성계획(Plan) 정보가 출력됩니다.\n여기서 (known after apply)는 말그대로 실제 적용 후에 확인 가능한 값임을 의미합니다.\n그래서 실무에서는 plan에서 이상이 발견되지 않아도 apply 후에는 문제가 발생하는 일이 간혹 있습니다.\n실행 여부를 묻는 질문에 yes 를 입력하고 [Enter]를 칩니다.\nEnter a value: yes aws_instance.example: Creating... aws_instance.example: Still creating... [10s elapsed] aws_instance.example: Still creating... [20s elapsed] aws_instance.example: Creation complete after 21s [id=i-082bf1d8b3de239c8] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. 마지막 라인에 1개의 리소스가 정상 생성되었다(1 added)고 결과가 나옵니다.\nAWS Management Console에서도 EC2 Instance 1대가 새로 생성되었는지 확인해보겠습니다.\n서울 리전에 EC2 Instance 1대가 생성된 후 정상 동작중(Running)입니다.\n6. 삭제 # EC2 Instance를 계속 켜놓으면 비용이 발생합니다.\n이제 Terraform으로 생성한 EC2 Instance를 다시 삭제해보겠습니다.\n$ terraform destroy $ terraform destroy aws_instance.example: Refreshing state... [id=i-082bf1d8b3de239c8] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # aws_instance.example will be destroyed - resource \u0026#34;aws_instance\u0026#34; \u0026#34;example\u0026#34; { - ami = \u0026#34;ami-0dd97ebb907cf9366\u0026#34; -\u0026gt; null - arn = \u0026#34;arn:aws:ec2:ap-northeast-2:111111111111:instance/i-082bf1d8b3de239c8\u0026#34; -\u0026gt; null - associate_public_ip_address = true -\u0026gt; null - availability_zone = \u0026#34;ap-northeast-2a\u0026#34; -\u0026gt; null - cpu_core_count = 1 -\u0026gt; null - cpu_threads_per_core = 1 -\u0026gt; null - disable_api_termination = false -\u0026gt; null - ebs_optimized = false -\u0026gt; null - get_password_data = false -\u0026gt; null - hibernation = false -\u0026gt; null - id = \u0026#34;i-082bf1d8b3de239c8\u0026#34; -\u0026gt; null - instance_initiated_shutdown_behavior = \u0026#34;stop\u0026#34; -\u0026gt; null - instance_state = \u0026#34;running\u0026#34; -\u0026gt; null - instance_type = \u0026#34;t2.micro\u0026#34; -\u0026gt; null - ipv6_address_count = 0 -\u0026gt; null - ipv6_addresses = [] -\u0026gt; null - monitoring = false -\u0026gt; null - primary_network_interface_id = \u0026#34;eni-0ca186f601dfde6d4\u0026#34; -\u0026gt; null - private_dns = \u0026#34;ip-172-31-0-60.ap-northeast-2.compute.internal\u0026#34; -\u0026gt; null - private_ip = \u0026#34;172.31.0.60\u0026#34; -\u0026gt; null - public_dns = \u0026#34;ec2-3-35-219-159.ap-northeast-2.compute.amazonaws.com\u0026#34; -\u0026gt; null - public_ip = \u0026#34;3.35.219.159\u0026#34; -\u0026gt; null - secondary_private_ips = [] -\u0026gt; null - security_groups = [ - \u0026#34;default\u0026#34;, ] -\u0026gt; null - source_dest_check = true -\u0026gt; null - subnet_id = \u0026#34;subnet-8d7ec4e6\u0026#34; -\u0026gt; null - tags = {} -\u0026gt; null - tags_all = {} -\u0026gt; null - tenancy = \u0026#34;default\u0026#34; -\u0026gt; null - vpc_security_group_ids = [ - \u0026#34;sg-58799024\u0026#34;, ] -\u0026gt; null - capacity_reservation_specification { - capacity_reservation_preference = \u0026#34;open\u0026#34; -\u0026gt; null } - credit_specification { - cpu_credits = \u0026#34;standard\u0026#34; -\u0026gt; null } - enclave_options { - enabled = false -\u0026gt; null } - metadata_options { - http_endpoint = \u0026#34;enabled\u0026#34; -\u0026gt; null - http_put_response_hop_limit = 1 -\u0026gt; null - http_tokens = \u0026#34;optional\u0026#34; -\u0026gt; null - instance_metadata_tags = \u0026#34;disabled\u0026#34; -\u0026gt; null } - root_block_device { - delete_on_termination = true -\u0026gt; null - device_name = \u0026#34;/dev/sda1\u0026#34; -\u0026gt; null - encrypted = false -\u0026gt; null - iops = 100 -\u0026gt; null - tags = {} -\u0026gt; null - throughput = 0 -\u0026gt; null - volume_id = \u0026#34;vol-01302622a80fb4b39\u0026#34; -\u0026gt; null - volume_size = 8 -\u0026gt; null - volume_type = \u0026#34;gp2\u0026#34; -\u0026gt; null } } Plan: 0 to add, 0 to change, 1 to destroy. Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: 삭제를 실행하면 대량의 상태 정보가 바뀔 것을 미리 알려주면서, 정말로 실행할 것인지 묻습니다.\nyes 입력 후 [Enter] 키를 입력합니다.\nEC2 Instance 1대를 제거하는 데 약 40초 걸렸습니다.\nEC2 Instance를 만들면서 추가적으로 생성된 다른 리소스들이 많기 때문입니다. (EBS, Security Group, ENI 등)\nEnter a value: yes aws_instance.example: Destroying... [id=i-082bf1d8b3de239c8] aws_instance.example: Still destroying... [id=i-082bf1d8b3de239c8, 10s elapsed] aws_instance.example: Still destroying... [id=i-082bf1d8b3de239c8, 20s elapsed] aws_instance.example: Still destroying... [id=i-082bf1d8b3de239c8, 30s elapsed] aws_instance.example: Destruction complete after 40s Destroy complete! Resources: 1 destroyed. 마지막 줄에 1개의 리소스가 정상 삭제되었다고 나옵니다. (1 destroyed.)\nAWS Management Console에서 확인해봐도 EC2 Instance가 삭제(Terminated)된 것을 볼 수 있습니다.\n결론 # 코드로 인프라를 관리하면 일관성 있는 상태를 유지하고, 이력 관리도 편하며, 인프라 배포/관리를 자동화할 수 있습니다.\n편하게 일하고 싶은 Cloud Engineer라면 인프라 구축 자동화 툴인 Terraform을 꼭 배워놓도록 합시다.\n","date":"Feb 22, 2022","permalink":"/blog/terraform-demo/","section":"Blogs","summary":"개요 # 테라폼을 통해 인프라 자동화, 코드로서 인프라IaC, Infrastructure as Code를 맛보기로 체험해보겠습니다.","title":"테라폼으로 EC2 Instance 생성하기 데모"},{"content":"개요 # EC2 Instance에 CloudWatch Agent를 설치하고 모니터링, 로그 수집, System Manager의 parameter store에 CloudAgent 설정파일을 저장하는 방법을 소개합니다.\n환경 # EC2 Instance\nType : t2.micro OS : Amazon Linux release 2 (Karoo) Shell : bash ID : ec2-user 전제조건 # EC2 Instance가 생성된 상태 설정방법 # IAM 설정 # 생성한 EC2 Instance에 CloudWatch Agent를 사용하기 위한 IAM 권한을 부여하자.\nEC2 Instance에 붙힐 Role을 새로 만든다.\nRole 이름 : AWSCloudWatchRoleForEC2 모니터링할 대상(EC2 Instance)에 CloudWatch Agent 관련 권한 2개가 부여되어 있어야 모니터링이 가능하다.\nRole에 추가가 필요한 Policy 2개 (Parameter store에 CloudWatch agent 설정상태를 저장하기 위해 IAM 권한이 필요합니다)\nCloudWatchAgentServerPolicy CloudWatchAgentAdminPolicy : ssm:PutParameter Action 때문에 필요하다. 이 모든 구성을 끝내고 가장 중요한 작업은 EC2 인스턴스에 우리가 구성한 IAM Role을 붙이는(권한을 부여하는) 것이다.\n테스트용 Apache(httpd) 설치 # EC2 Instance에 SSH로 로그인하자.\nCloudWatch Log 수집을 테스트하기 위해 EC2 Instance에 Apachehttpd를 설치한다.\nLast login: Thu Jan 13 13:11:39 2022 from 117.111.22.231 __| __|_ ) _| ( / Amazon Linux 2 AMI ___|\\___|___| https://aws.amazon.com/amazon-linux-2/ [ec2-user@ip-xxx-xx-xx-xxx ~]$ $ sudo su $ id uid=0(root) gid=0(root) groups=0(root) ec2-user에서 root 계정으로 변경한다.\nApache 설치 작업 과정에서는 root 권한을 얻는게 중요하다.\nhttpd 패키지를 설치한다.\n$ yum install -y httpd ... Installed: httpd.x86_64 0:2.4.51-1.amzn2 Dependency Installed: apr.x86_64 0:1.7.0-9.amzn2 apr-util.x86_64 0:1.6.1-5.amzn2.0.2 apr-util-bdb.x86_64 0:1.6.1-5.amzn2.0.2 generic-logos-httpd.noarch 0:18.0.0-4.amzn2 httpd-filesystem.noarch 0:2.4.51-1.amzn2 httpd-tools.x86_64 0:2.4.51-1.amzn2 mailcap.noarch 0:2.1.41-2.amzn2 mod_http2.x86_64 0:1.15.19-1.amzn2.0.1 Complete! 정상적으로 httpd를 설치 완료했다.\n$ yum list installed httpd Loaded plugins: extras_suggestions, langpacks, priorities, update-motd Installed Packages httpd.x86_64 2.4.51-1.amzn2 @amzn2-core httpd 2.4.51 버전이 설치되었다.\n기본 메인페이지를 생성하고, httpd 데몬을 시작하자.\n$ echo \u0026#34;Hello world from $(hostname -f)\u0026#34; \u0026gt; /var/www/html/index.html $ systemctl start httpd $ exit $ whoami ec2-user httpd 구성 작업이 끝난 후에는 exit 명령어로 ec2-user 계정으로 다시 돌아온다.\nAWS에서 제공하는 CloudWatch Agent의 설치파일을 받아온다.\n$ wget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm --2022-01-13 13:22:09-- https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.104.13 Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.104.13|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 46891760 (45M) [application/octet-stream] Saving to: ‘amazon-cloudwatch-agent.rpm’ 100%[======================================\u0026gt;] 46,891,760 3.79MB/s in 14s 2022-01-13 13:22:24 (3.23 MB/s) - ‘amazon-cloudwatch-agent.rpm’ saved [46891760/46891760] 45MB 크기의 CloudWatch 패키지 파일이 생성되었다.\n$ ls -lh total 45M -rw-rw-r-- 1 ec2-user ec2-user 45M Aug 5 01:07 amazon-cloudwatch-agent.rpm 패키지 파일을 설치한다.\n$ sudo rpm -U ./amazon-cloudwatch-agent.rpm create group cwagent, result: 0 create user cwagent, result: 0 Cloud Watch 설치 마법사를 이용하면 간단하게 CloudWatch Agent를 구성할 수 있다.\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard ============================================================= = Welcome to the AWS CloudWatch Agent Configuration Manager = ============================================================= On which OS are you planning to use the agent? 1. linux 2. windows 3. darwin default choice: [1]: 1 ... 보통 기본값일 경우, Enter 키를 입력해서 다음 단계로 계속 넘어간다.\n설정 단계를 넘기다보면 로그 모니터링 쪽 설정 단계가 나온다.\n로그 모니터링 설정 # CloudWatch Agent가 /var/log/httpd/access_log 와 /var/log/httpd/error_log를 수집하도록 설정한다.\naccess_log 수집 설정\naccess_log의 절대경로는 /var/log/httpd/access_log 이다.\n로그 파일의 절대경로Log file path를 입력할 때 오타에 주의한다.\n... Do you want to monitor any log files? 1. yes 2. no default choice: [1]: 1 Log file path: /var/log/httpd/access_log Log group name: default choice: [access_log] Log stream name: default choice: [{instance_id}] error_log 수집 설정\nerror_log의 절대경로는 /var/log/httpd/error_log 이다.\n... Do you want to specify any additional log files to monitor? 1. yes 2. no default choice: [1]: 1 Log file path: /var/log/httpd/error_log Log group name: default choice: [error_log] Log stream name: default choice: [{instance_id}] Do you want to specify any additional log files to monitor? 1. yes 2. no default choice: [1]: 2 Saved config file to /opt/aws/amazon-cloudwatch-agent/bin/config.json successfully. 마지막에는 CloudWatch Agent의 설정파일 경로를 알려준다.\nPlease check the above content of the config. The config file is also located at /opt/aws/amazon-cloudwatch-agent/bin/config.json. Edit it manually if needed. CloudAgent 설정파일의 경로인 /opt/aws/amazon-cloudwatch-agent/bin/config.json 를 확인한다.\n실제로 해당 경로의 CloudWatch Agent 설정파일을 확인해보자.\n$ cat /opt/aws/amazon-cloudwatch-agent/bin/config.json { \u0026#34;agent\u0026#34;: { \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;run_as_user\u0026#34;: \u0026#34;root\u0026#34; }, \u0026#34;logs\u0026#34;: { \u0026#34;logs_collected\u0026#34;: { \u0026#34;files\u0026#34;: { \u0026#34;collect_list\u0026#34;: [ { \u0026#34;file_path\u0026#34;: \u0026#34;/var/log/httpd/access_log\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;access_log\u0026#34;, \u0026#34;log_stream_name\u0026#34;: \u0026#34;{instance_id}\u0026#34; }, { \u0026#34;file_path\u0026#34;: \u0026#34;/var/log/httpd/error_log\u0026#34;, \u0026#34;log_group_name\u0026#34;: \u0026#34;error_log\u0026#34;, \u0026#34;log_stream_name\u0026#34;: \u0026#34;{instance_id}\u0026#34; } ] } } }, \u0026#34;metrics\u0026#34;: { \u0026#34;append_dimensions\u0026#34;: { \u0026#34;AutoScalingGroupName\u0026#34;: \u0026#34;${aws:AutoScalingGroupName}\u0026#34;, \u0026#34;ImageId\u0026#34;: \u0026#34;${aws:ImageId}\u0026#34;, \u0026#34;InstanceId\u0026#34;: \u0026#34;${aws:InstanceId}\u0026#34;, \u0026#34;InstanceType\u0026#34;: \u0026#34;${aws:InstanceType}\u0026#34; }, \u0026#34;metrics_collected\u0026#34;: { \u0026#34;collectd\u0026#34;: { \u0026#34;metrics_aggregation_interval\u0026#34;: 60 }, \u0026#34;disk\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60, \u0026#34;resources\u0026#34;: [ \u0026#34;*\u0026#34; ] }, \u0026#34;mem\u0026#34;: { \u0026#34;measurement\u0026#34;: [ \u0026#34;mem_used_percent\u0026#34; ], \u0026#34;metrics_collection_interval\u0026#34;: 60 }, \u0026#34;statsd\u0026#34;: { \u0026#34;metrics_aggregation_interval\u0026#34;: 60, \u0026#34;metrics_collection_interval\u0026#34;: 10, \u0026#34;service_address\u0026#34;: \u0026#34;:8125\u0026#34; } } } } CloudWatch Agent의 설치 목적\n핵심 목적은 EC2 인스턴스에 CloudWatch Agent를 설치하면, 더 많은 지표를 수집할 수 있다.\nCloudWatch 웹에서는 메모리 모니터링은 불가능하지만, CloudWatch Agent에서는 메모리를 모니터링해서 메모리 사용량을 볼 수 있다.\nCloudWatch Agent 시작 # ssm:AmazonCloudWatch-linux 는 Parameter Store의 기본default 이름이다.\nssm은 AWS System Manager를 의미한다.\n명령어 설명\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:AmazonCloudWatch-linux -s -a fetch-config : 에이전트가 최신 버전의 CloudWatch 에이전트 구성 파일을 로드한다.\n-c ssm:AmazonCloudWatch-linux : CloudAgent 구성 파일은 AWS System Managerssm의 AmazonCloudWatch-linux 라는 이름의 파라미터 스토어에서 가져온다.\n-s : 에이전트 설정이 끝난 후 CloudWatch 에이전트를 재시작하는 옵션\n명령어 실행결과\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:AmazonCloudWatch-linux -s ****** processing amazon-cloudwatch-agent ****** /opt/aws/amazon-cloudwatch-agent/bin/config-downloader --output-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --download-source ssm:AmazonCloudWatch-linux --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default Region: ap-northeast-2 credsConfig: map[] Successfully fetched the config and saved in /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp Start configuration validation... /opt/aws/amazon-cloudwatch-agent/bin/config-translator --input /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json --input-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --output /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default 2022/01/13 03:16:55 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp ... Valid Json input schema. I! Detecting run_as_user... No csm configuration found. Configuration validation first phase succeeded /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent -schematest -config /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml Configuration validation second phase failed ======== Error Log ======== 2022-01-13T03:16:55Z E! [telegraf] Error running agent: Error parsing /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml, open /usr/share/collectd/types.db: no such file or directory 마지막 라인에서 /usr/share/collectd/types.db: no such file or directory 에러가 발생한다.\ntypes.db 파일이 해당 경로에 정말로 없는지 체크해보자.\n$ ls -l /usr/share/collectd/types.db ls: cannot access /usr/share/collectd/types.db: No such file or directory 확인해보니 해당 경로에 types.db 파일이 존재하지 않아서 발생하는 에러이다.\n직접 해당 경로에 types.db 파일을 만들어주자.\n$ sudo mkdir -p /usr/share/collectd/ -p : 중간에 디렉토리가 없으면 해당 상위 디렉토리도 같이 생성하는 옵션\n$ sudo touch /usr/share/collectd/types.db types.db 파일도 생성한다.\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:AmazonCloudWatch-linux -s ****** processing amazon-cloudwatch-agent ****** /opt/aws/amazon-cloudwatch-agent/bin/config-downloader --output-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --download-source ssm:AmazonCloudWatch-linux --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default Region: ap-northeast-2 credsConfig: map[] Successfully fetched the config and saved in /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp Start configuration validation... /opt/aws/amazon-cloudwatch-agent/bin/config-translator --input /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json --input-dir /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d --output /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml --mode ec2 --config /opt/aws/amazon-cloudwatch-agent/etc/common-config.toml --multi-config default 2022/01/13 03:19:19 Reading json config file path: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.d/ssm_AmazonCloudWatch-linux.tmp ... Valid Json input schema. I! Detecting run_as_user... No csm configuration found. Configuration validation first phase succeeded /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent -schematest -config /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.toml Configuration validation second phase succeeded Configuration validation succeeded amazon-cloudwatch-agent has already been stopped Created symlink from /etc/systemd/system/multi-user.target.wants/amazon-cloudwatch-agent.service to /etc/systemd/system/amazon-cloudwatch-agent.service. Redirecting to /bin/systemctl restart amazon-cloudwatch-agent.service types.db 파일을 생성한 뒤부터 정상적으로 CloudWatch Agent가 실행된다.\n$ sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a status -m ec2 { \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;starttime\u0026#34;: \u0026#34;2022-01-13T13:34:11+0000\u0026#34;, \u0026#34;configstatus\u0026#34;: \u0026#34;configured\u0026#34;, \u0026#34;cwoc_status\u0026#34;: \u0026#34;stopped\u0026#34;, \u0026#34;cwoc_starttime\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;cwoc_configstatus\u0026#34;: \u0026#34;not configured\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.247349.0b251399\u0026#34; } AWS CloudWatch Agent가 정상적으로 실행중(running)이며, 에이전트가 설정된 상태(configured)이다.\n로그 수집 결과 확인 # 이제 AWS Console로 이동한 후 CloudWatch 서비스 화면으로 접속한다.\nEC2 Instance의 access_log와 error_log를 수집된 게 보인다.\nOS상의 로그와 동일한 내용으로 로그가 잘 수집되었다.\n결론 # 이 포스팅은 CloudWatch Agent의 수동 설치를 다룬다.\n수동 설치는 클라우드 네이티브 인프라스트럭처의 핵심 가치인 자동화Automation와는 거리가 멀다.\n만약 CloudWatch Agent를 설치해야할 EC2 인스턴스가 200대라면, 200대 각자 들어가서 CloudWatch Agent를 설치하는건 비효율적이고(사람을 갈아넣으면 뭐든 가능하겠지만) 인적 실수Human fault를 유발할 확률이 높다.\n조만간 나도 자동설치 관련해서 직접 글을 쓸 예정이다.\nCloudWatch Agent 자동 설치 방법이 궁금하다면 OpsNow Techblog의 CloudWatch 에이전트를 손쉽게 설치하는 방법 – OpsNow Tech Blog 포스팅을 참고하면 좋다. 설명을 잘해놓은 좋은 글이다.\n","date":"Jan 13, 2022","permalink":"/blog/installing-unified-cloud-watch-agent/","section":"Blogs","summary":"개요 # EC2 Instance에 CloudWatch Agent를 설치하고 모니터링, 로그 수집, System Manager의 parameter store에 CloudAgent 설정파일을 저장하는 방법을 소개합니다.","title":"CloudWatch Agent 설치 및 구성"},{"content":" # 증상 # Jenkins 파이프라인 스크립트를 짜고 파이프라인을 새로 만들었다.\n// Testing jenkins slave node node(label: \u0026#39;builder\u0026#39;) { stage(\u0026#39;Preparation\u0026#39;) { git branch: \u0026#39;main\u0026#39;, url: \u0026#39;\u0026lt;https://github.com/seyslee/nodejs-with-docker-toyproj.git\u0026gt;\u0026#39; } stage(\u0026#39;Build\u0026#39;) { def myTestContainer = docker.image(\u0026#39;node:4.6\u0026#39;) myTestContainer.pull() myTestContainer.inside() { sh \u0026#39;npm install\u0026#39; } } } Pipeline script 설명 (테스트용의 엄청 단순한 코드다) Git 원본\nbuilder 라벨이 붙은 Jenkins 노드에서만 실행한다. 참고로 이 코드는 Jenkins slave이 정상 동작하는지 테스트하기 위한 코드다. [Preparation 단계] 내 Docker 데모 레포지터리의 main branch를 Pull 한다. [Build 단계] docker 이미지 node:4.6를 빌드한다. [Build 단계] docker 컨테이너 안에서 npm install 명령어를 실행한다. Jenkins 웹페이지에서 만든 파이프라인을 빌드를 돌려보니 docker를 사용하는 단계Stage에서 아래와 같은 에러 메세지가 출력된다.\ndial unix /var/run/docker.sock: connect: permission denied\n자세한 Stages Logs\n+ docker pull node:4.6 Warning: failed to get default registry endpoint from daemon (Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.26/info: dial unix /var/run/docker.sock: connect: permission denied). Using system default: \u0026lt;https://index.docker.io/v1/\u0026gt; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.26/images/create?fromImage=node\u0026amp;tag=4.6: dial unix /var/run/docker.sock: connect: permission denied Jenkins 에러 메세지 화면\ndocker 에러로 인해 내가 만든 Pipeline을 제대로 빌드할 수 없는 상황이다.\n환경 # 호스트 환경정보\nOS : Ubuntu 20.04.3 LTS Shell : bash ID : root Docker : Docker version 20.10.12, build e91ed57 Jenkins Server : Jenkins 2.319.1 (도커 컨테이너 형태로 배포됨) 호스트의 Docker 구성\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 170a33da3a09 wardviaene/jenkins-slave \u0026#34;/entrypoint.sh\u0026#34; 2 hours ago Up 2 hours 0.0.0.0:2222-\u0026gt;22/tcp, :::2222-\u0026gt;22/tcp suspicious_goldstine 현재 내 경우는 jenkins를 docker 컨테이너 형태로 띄워서 사용중이다.\n$ docker run -p 2222:22 -v /var/jenkins_home:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock --restart always -d wardviaene/jenkins-slave 호스트 서버의 /var/run/docker.sock 파일을 컨테이너에 공유해서 사용 중이다.\n$ ls -lh /var/run/docker.sock srw-rw---- 1 root systemd-coredump 0 Jan 11 10:35 /var/run/docker.sock 호스트 서버에서 확인한 /var/run/docker.sock 파일. 현재 내 구성은 이 docker.sock 파일을 젠킨스 컨테이너와 공유하도록 볼륨 매핑되어 있다.\n원인 # jenkins 컨테이너가 사용하는 docker.sock 파일의 권한 설정 문제\n해결방법 # 컨테이너 내부의 /var/run/docker.sock 파일에 알맞은 그룹 권한 docker을 부여한다.\n상세 해결방법 # 호스트 서버 # Jenkins 컨테이너가 구동되고 있는 호스트 서버에 접속한다.\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 170a33da3a09 wardviaene/jenkins-slave \u0026#34;/entrypoint.sh\u0026#34; 2 hours ago Up 2 hours 0.0.0.0:2222-\u0026gt;22/tcp, :::2222-\u0026gt;22/tcp suspicious_goldstine 현재 구성은 호스트 서버 위에서 jenkins가 컨테이너 형태로 운영중인 구성이다.\njenkins 컨테이너에 bash 쉘로 접속한다.\n$ docker exec -it 170a33da3a09 bash Jenkins 컨테이너 내부 # $ id uid=0(root) gid=0(root) groups=0(root) bash 명령어로 Jenkins 컨테이너에 root ID로 들어왔다.\n컨테이너 내부에서 docker.sock 파일의 존재여부와 권한을 확인한다.\n$ ls -al /var/run/ total 20 drwxr-xr-x 1 root root 4096 Jan 11 10:36 . drwxr-xr-x 1 root root 4096 Jan 11 10:36 .. srw-rw---- 1 root 998 0 Jan 11 10:35 docker.sock drwxrwxrwt 2 root root 4096 May 8 2017 lock drwxr-xr-x 2 root root 4096 Jun 9 2017 sshd -rw-r--r-- 1 root root 4 Jan 11 10:36 sshd.pid -rw-rw-r-- 1 root utmp 0 May 8 2017 utmp docker.sock 파일의 그룹 권한이 이름이 없고 GID 998만 적혀있다.\n$ cat /etc/group | grep 998 $ ## 결과가 출력되지 않음 = 998번 그룹이 없다 컨테이너 내부에서 GID 998에 해당하는 Group이 없기 때문에 그룹 이름이 아닌 998로만 표시되는 것이다.\n$ cat /etc/group | grep docker docker❌999:jenkins 컨테이너의 기준에서 jenkins 유저가 속해있는 docker 그룹의 GID는 999이다.\n컨테이너 내부에 위치한 docker.sock 파일의 그룹 권한이 이상하기 때문에 GID 999를 가진 docker 그룹에 다시 연결해주도록 하자.\n$ sudo chown root:docker /var/run/docker.sock bash: sudo: command not found sudo 명령어가 존재하지 않는 컨테이너도 있으니 당황하지 말자.\ndocker.sock 파일의 소유자 권한을 root, 소유 그룹을 docker로 변경한다.\n$ chown root:docker /var/run/docker.sock $ ls -lh /var/run/ total 12K srw-rw---- 1 root docker 0 Jan 11 10:35 docker.sock drwxrwxrwt 2 root root 4.0K May 8 2017 lock drwxr-xr-x 2 root root 4.0K Jun 9 2017 sshd -rw-r--r-- 1 root root 4 Jan 11 10:36 sshd.pid -rw-rw-r-- 1 root utmp 0 May 8 2017 utmp docker.sock 파일의 그룹 권한이 docker로 변경되었다.\n도커 소켓파일의 권한에 대한 조치는 완료됐다.\nJenkins 웹페이지로 돌아간다.\n[Build Now] 버튼을 클릭해서 처음에 build 실패한 Pipeline을 다시 빌드해보자.\n컨테이너의 docker.sock 파일의 소유 그룹을 docker 로 맞춰준 후부터 빌드가 잘 완료된다.\n..... [Pipeline] // stage [Pipeline] stage [Pipeline] { (Build) [Pipeline] isUnix [Pipeline] sh + docker pull node:4.6 4.6: Pulling from library/node Digest: sha256:a1cc6d576734c331643f9c4e0e7f572430e8baf9756dc24dab11d87b34bd202e Status: Image is up to date for node:4.6 [Pipeline] isUnix [Pipeline] sh ..... [Pipeline] } [Pipeline] // node [Pipeline] End of Pipeline Finished: SUCCESS Jenkins 빌드 로그에서 볼 때도 문제가 발생했던 docker pull node:4.6 부분에서도 정상적으로 이미지를 가져온다.\n결론 # 이 이슈는 조치는 간단한데 생각보다 귀찮고 당황스럽다.\nPermission denied 에러를 처음 마주했을 때는 낯설고 무서웠지만 이제는 감흥도 없고 조치가 익숙하다.\n","date":"Jan 11, 2022","permalink":"/blog/var-run-docker-sock-permission-denied/","section":"Blogs","summary":"# 증상 # Jenkins 파이프라인 스크립트를 짜고 파이프라인을 새로 만들었다.","title":"docker 권한 문제 해결: /var/run/docker.sock permission denied"},{"content":"","date":"Jan 11, 2022","permalink":"/tags/jenkins/","section":"Tags","summary":"","title":"jenkins"},{"content":"","date":"Jan 8, 2022","permalink":"/tags/backup/","section":"Tags","summary":"","title":"backup"},{"content":"개요 # DD4500 백업장치의 폴트 디스크 교체 작업 시, 인적실수를 방지하고 안전하게 교체하는 정식 절차를 소개합니다.\n환경 # Hardware : EMC DD4500\n작업형태 : 데이터센터 현장의 하드웨어 작업과 CLI 작업이 혼합된 시나리오\n전제조건 # 폴트 발생 디스크를 제거하고 신규로 투입할 교체용 디스크 파트\n작업절차 # failed disk 교체 # 1. Login # 작업 대상인 Data Domain 4500에 SSH로 로그인한다.\nData Domain 2500, Data Domain 4500 모델 기준으로 SSH 기본 포트는 TCP 22번이다. 참고로 Data Domain의 관리자가 명령어를 사용해서 SSH 포트를 변경할 수도 있다.\n2. Failed Disk 확인 # $ disk show state Enclosure Disk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --------- ---------------------------------------------- [...] 14 . . . f . . . . . . . . . . . f 는 Failed Disks 를 의미한다. 14번 Enclosure의 4번 디스크가 고장난 상황이다.\n3. 교체 대상 디스크 LED 표시 # 명령어 형식\n$ disk beacon \u0026lt;enclosure-id\u0026gt;.\u0026lt;disk-id\u0026gt; 정상 디스크를 잘못 분리하는 인적 실수(Human Fault)를 방지하기 위해 고장난 Disk의 전면부 LED를 깜빡거리도록 표시하는 명령어이다.\n제조사인 EMC가 작성한 문서에서도 disk beacon 명령어를 실행한 상태에서 디스크를 교체하기를 권장하고 있다.\n실제 수행 명령어\n$ disk beacon 14.4 14번 인클로저의 4번 디스크의 LED를 점멸(Blinking)시킨다.\nCtrl + C를 누르면 LED 깜빡임을 멈출 수 있다.\n참고사항\n특정 디스크의 LED가 켜지지 않을 경우, enclosure beacon 명령어를 사용해서 모든 디스크에서 상태 표시등이 깜박이는지 확인할 수 있다.\n4. 디스크 교체 # Enclosure 전면부 앞으로 이동한다.\n고장난 디스크를 제거(eject)한 후 신규 디스크를 장착(insert)한다. 약 5분 정도 기다린다.\n새로 장착된 디스크 슬롯이 failed 상태에서 정상 상태인 spare 또는 In-use 로 변화하는 데 까지 약간의 시간이 소요될 수 있기 때문이다.\n디스크 교체 시 Data Domain이 동작하는 과정\nExamine the newly inserted disk for any partitions that exist.\nIf there are no partitions on the disk, then a number of partitions will be created. In one partition, specifically partition 3, will we create a DataDomain superblock. This disk will then become a spare disk as indicated by the \u0026rsquo;s\u0026rsquo; state above.\nIf partition 3 exists, it is checked for the presence of a superblock. If a superblock exists and the superblock shows the information from another system, the disk will become a foreign or unknown.\nIf partition 3 exists but there is no superblock, the disk will become a \u0026lsquo;v\u0026rsquo; available disk.\n5. Disk 상태 확인 # $ disk show state Enclosure Disk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --------- ---------------------------------------------- [...] 14 . . . v . . . . . . . . . . . 디스크 상태 표시 정보\n. : In-use Disks. 서비스 중인 정상 디스크. v : Available Disks. 사용 가능하나 투입되지 않은 디스크. f : Failed Disks. 고장으로 사용 불가능한 상태의 디스크 s(spare) 상태로 변경되어야 정상적으로 인식한 것이다. 교체 후 Disk 상태가 정상 표기되지 않을 경우 # 교체한 디스크의 상태가 . (In-use) 이거나 s (Spare)일 경우 아래 과정은 수행하지 않아도 된다.\n6. Disk 상태 변경 명령어를 실행 # 명령어 형식\nDisk 상태를 Unknown에서 Online 상태로 변경해주는 명령어.\n$ disk unfail \u0026lt;enclosure-id\u0026gt;.\u0026lt;disk-id\u0026gt; 실제 수행 명령어\n$ disk unfail 14.4 The \u0026#39;disk unfail\u0026#39; command will add the disk to the active stroage tier and mari it as a space. Any existing data on this disk will be lost. Are you sure? (yes|no|?) [no] yes ok, proceeding yes 를 입력해서 Disk 상태를 Unknown에서 Online 상태로 변경해준다.\nok, proceeding 메세지가 출력되면 새로 장착한 디스크가 활성화 스토리지 티어(active storage tier)에 정상적으로 투입된 것이다.\n$ disk unfail 14.4 **** Disk 14.4 is already available. Disk x.x is already available. 메세지가 출력된다면 disk unfail 명령어를 실행시킬 필요가 없다.\n7. Disk 상태 재확인 # unfail 조치한 디스크의 상태를 최종 확인한다.\n$ disk show state 참고자료 # Dell EMC Data Domain Hardware Features and Specifications Guide (6.2 버전) - Dell EMC\n디스크 교체 시 Data Domain이 동작하는 과정 - Dell Community\n","date":"Jan 8, 2022","permalink":"/blog/replacing-failed-disk-in-dd4500/","section":"Blogs","summary":"개요 # DD4500 백업장치의 폴트 디스크 교체 작업 시, 인적실수를 방지하고 안전하게 교체하는 정식 절차를 소개합니다.","title":"DD4500 디스크 교체와 spare disk 설정"},{"content":"","date":"Jan 8, 2022","permalink":"/tags/storage/","section":"Tags","summary":"","title":"storage"},{"content":"개요 # 넷워커 백업서버에서 nsports 명령어를 통해 EMC Networker의 서비스 포트 범위를 변경할 수 있다.\n발단 # Networker 백업을 받을 때마다 \u0008간헐적으로 auth failed 에러 메세지가 발생하며 백업에 실패하는 증상이 있었다.\n이 이슈를 해결하기 위해서 나는 Networker 백업서버를 점검하던 중, Networker 서비스 포트의 설정값이 쓰던 값과 상이하게 달라졌음을 발견했다.\n야간에 같은 시간대에 동시 구동되는 백업이 많을 경우, 백업 서버의 Service port 범위를 다 써버려서 auth failed와 함께 실패할 수도 있다.\n백업을 정상화하기 위해 해당 Networker 관리서버에 접속한 후 networker service port 설정 범위를 변경해보자.\n환경 # OS : Red Hat Enterprise Linux Server release 6.x\nShell : Bash\n백업 솔루션 : EMC Networker 9.1.1.1\n작업대상 : Networker 백업 관리서버\n해결방법 # Networker 백업서버 접속 # $ hostname backup-server Networker 백업서버는 Netwokrer 에이전트가 설치된 백업 대상(Target)이 아니라 Networker가 설치된 중앙 서버를 의미한다.\n기존 Networker 서비스 포트정보 확인 # $ nsrports Service ports: 7937-7999 Connection ports: 0-0 현재 설정된 Networker용 서비스 포트 범위가 7937-9999이다.\n기본적으로 Networker 서비스 포트의 기본값은 7937-9936 이지만 현재 시나리오의 서버는 너무 포트 범위가 좁다. 정상적인 넓은 포트 범위인 기본값 7937-9936 으로 변경한다.\n아래 사진은 nsrports 명령어에 대한 메뉴얼(man) 페이지이다.\nNetworker 서비스 포트 변경 # 명령어 형식\n$ nsrports -s \u0026lt;백업서버의 Hostname\u0026gt; -S \u0026lt;서비스 포트 범위\u0026gt; ... \u0026lt;서비스 포트 범위\u0026gt; 실제 명령어\n$ nsrports -s networker-server -S 7937-9936 Networker 서비스 포트 확인 # $ nsrports Service ports: 7937-9936 Connection ports: 0-0 Networker가 사용할 서비스 포트 범위가 7937-7999 에서 7937-9936 으로 변경되었다.\nNetworker 서비스 포트 변경 작업은 생각보다 간단하다. 끝! 결론 # 사실 백업은 힘들고 지루하고 쓸쓸한 작업이다. (물론 난 파트 내에서 백업이 재밌어서 메인으로 담당하고 제일 잘 알고 있지만)\n백업은 개발자와 서비스 자체와 전혀 동떨어진 작업이며, 평소에는 백업 해놓은 티가 하나도 나질 않는다.\n그러나 사실 예고없이 장애가 발생했을 때 그 무엇보다도 중요한 게 백업이 아닐까. 몇 년간 (의미 없는 것처럼 보이는) 백업을 받아놓으면 반드시 언젠가는 쓸 일이 생기곤 한다.\n그 상황에서 백업을 받지 않았다면 끔찍하게 많은 작업과 노력이 들어가게 된다.\n이슈 해결은 다방면 다각도로 보자\n트러블슈팅을 할 때 서버와 클라이언트만 생각하면 안된다. 이슈를 처리할 때 다양한 시각과 다른 분야의 담당자들과 협업해서 처리할 필요가 있다.\n백업의 경우를 예로 들면 백업이 안되는 이유는 너무 다양하다.\n방화벽이나 IPS 등의 보안 장비에서 트래픽 차단. 최근에 보안 정책을 잘못 입력한 네트워크 담당자의 인적 실수. 중앙 백업서버 자체의 설정 문제. 또는 네트워크 스위치의 이슈. 백업 대상(클라이언트)의 에이전트가 다운되는 등의 에이전트 문제. 클라이언트의 설정의 문제일 수도 있다. 심지어는 (최근에 직접 경험한 케이스다) 백업 대상의 호스트네임을 시스템 관리자가 잘못 설정해서 백업 실행이 안되는 일도 있었다.\n백업 서버와 대상 클라이언트만 들여다봐서 해결 안되는 경우는 생각보다 많다. 빠른 문제해결을 위해서는 다각도로 다방면으로 봐야 한다.\n트러블슈팅에 있어서 가장 중요한 것은 이슈와 관련된 정보와 흔적들을 수집해서 끈질기게 추적하는 거라고 생각한다. 클라이언트와 서버에서 발생한 에러 로그 정보를 읽고, 증상, 관련 팀들의 최근 작업 이력 등의 정보를 하나 둘씩 수집해서 찾아가다보면 결국 이슈는 해결하게 되어있는 것 같다.\n어쩌다보니 잔소리가 길어졌다.\n아무튼 이 글을 읽고 계실 시스템 엔지니어 분들, 다들 티는 안나지만 열심히 백업하시고 장애 없는 인프라를 기원합니다. 안녕!\n","date":"Jan 6, 2022","permalink":"/blog/changing-networker-service-ports/","section":"Blogs","summary":"개요 # 넷워커 백업서버에서 nsports 명령어를 통해 EMC Networker의 서비스 포트 범위를 변경할 수 있다.","title":"Networker 서비스 포트 범위 변경"},{"content":"개요 # EMC사의 VTL(Virtual Tape Library) 장비인 DD2500에서 디스크 여유공간이 부족할 때 파일시스템 정리(Filesystem cleanining) 조치를 통해 용량 확보하는 방법을 설명한다.\nDD2500\nEMC사에서 만든 백업 및 재해 복구(DR, Disaster Recovery) 기능을 제공하는 중복 제거 스토리지 제품의 모델명이다.\n환경 # Model : Data Domain 2500\nDD OS : Data Domain OS 5.7.3.0-548132\nNetworker : Networker 9.1.1.1\n문제점 # 평화롭던 어느날. 평소처럼 Data Domain에 SSH 로그인을 했더니 \u0026ldquo;알람이 발생했으니 확인해보라\u0026quot;는 메세지가 출력됐다.\nLast login: Wed Jan 5 14:16:30 KST 2022 from xx.xxx.x.xx on pts/0 Welcome to Data Domain OS 5.7.3.0-548132 ---------------------------------------- ** ** NOTICE: There is 1 outstanding alert. Run \u0026#34;alerts show current\u0026#34; ** to display outstanding alert(s). ** $ system show version Data Domain OS 5.7.3.0-548132 NOTICE 안내문이 알려준대로 쉘에서 alerts show current 명령어를 실행한다.\n현재 발생한 알람 보기\n$ alert show current Id Post Time Severity Class Object Message ----- ------------------------ -------- ---------- ------------- --------------------------------------------------------------------------- p0-59 Wed Jan 5 13:02:58 2022 CRITICAL Filesystem FilesysType=2 EVT-SPACE-00004: Space usage in Data Collection has exceeded 90% threshold. ----- ------------------------ -------- ---------- ------------- --------------------------------------------------------------------------- There is 1 active alert. 데이터 저장공간의 용량이 90% 초과해서 부족하다는 경보(Alert)가 발생했다.\nAlert의 심각도는 Critical이라서 그냥 무시하기에는 부담스럽다.\n해결방안 # 1. DD 용량 확인 # $ df Active Tier: Resource Size GiB Used GiB Avail GiB Use% Cleanable GiB* ---------------- -------- -------- --------- ---- -------------- /data: pre-comp - 68789.7 - - - /data: post-comp 10436.7 9348.8 1087.9 90% 3576.0 /ddvar 49.2 7.2 39.5 15% - /ddvar/core 49.2 0.2 46.5 0% - ---------------- -------- -------- --------- ---- -------------- * Estimated based on last cleaning of 2022/01/04 11:06:35. Data Domain의 파일시스템 사용률이 현재 90%이다.\n2. 테이프 라벨링 # 테이프에 라벨을 부여하는 작업은 테이프에 담긴 데이터를 한번 밀어서 정리하는 작업이다.\nMode가 recyclable인 테이프의 라벨을 다른 라벨이나 이전과 동일한 라벨로 설정하면 해당 테이프들이 초기화되면서 재활용 분으로 들어간다.\n이 방식으로 Cleanble GiB를 확보해야한다.\nNetworker 프로그램을 실행 → 로그인 → devices 메뉴로 들어간다.\nMode가 recyclable 인 테이프들이 20개 이상 보인다. recyclable은 테이프에 담긴 데이터 보존기간이 지나서 재활용 가능한 테이프라는 의미이다.\nrecyclable 모드인 테이프에 라벨링을 새롭게 한다.\nTape 선택 → 우클릭 → Label\u0026hellip;\n라벨을 설정할 때는 새로운 라벨로 지정하거나 또는 기존과 같은 라벨로 지정해도 전혀 무관하다.\n테이프에 라벨을 지정한다는 것은 해당 테이프에 있는 데이터를 밀고 새로운 테이프처럼 재활용해서 쓰겠다는 의미와 동일하다.\n라벨 메뉴에서 Target Media Pool → 기존과 동일한 라벨 or 원하는 라벨로 지정 → OK\n이제 지정된 테이프들에 라벨이 새롭게 붙고 데이터를 밀어낼 것이다.\nOK를 누르면 새로운 Label 작업(operation: \u0026quot;Label\u0026quot;)이 생성되고 작업을 시작한다.\n약 1분 정도 기다리면 작업이 완료(succeeded)된다.\n다시 Data Domain의 shell로 돌아온다.\n테이프에 라벨을 다시 붙이는 작업이 끝났다면 df 명령어를 실행해서 Cleanable GiB 값이 늘어난 것을 확인할 수 있다.\n3. Filesystem Cleaning # Filesystem cleaning 스케줄 확인\n$ filesys clean show config 50 Percent Throttle Filesystem cleaning is scheduled to run \u0026#34;Tue\u0026#34; at \u0026#34;0600\u0026#34;. 이 글을 작성하는 현재 시간은 수요일이다.\n매주 화요일 오전 6시(\u0026quot;Tue\u0026quot; at \u0026quot;0600\u0026quot;.)마다 Filesystem cleaning 작업이 시작되도록 설정된 상태이다.\n결과적으로 다음 Filesystem cleaning 작업은 다음주 화요일 오전 6시에 실행될 것이다.\nData Domain의 파일시스템 사용률이 90% 초과하면 위험하다.\n다음주 화요일까지 기다릴 수 없는 상태이기 때문에 수동으로 클리닝 작업을 실행하고 여유 용량을 확보하자.\nCleaning 작업 수동 시작\n$ filesys clean start Cleaning started. Use \u0026#39;filesys clean watch\u0026#39; to monitor progress. Filesystem cleaning이 시작됐다.\n4. Cleaning 모니터링 # filesys clean watch 명령어를 실행하면 Cleaning 작업 상태를 중간에 모니터링할 수 있다.\n$ filesys clean watch Beginning \u0026#39;filesys clean\u0026#39; monitoring. Use Control-C to stop monitoring. Cleaning: phase 1 of 12 (pre-merge) 100.0% complete, 1088 GiB free; time: phase 0:01:03, total 0:01:04 filesys clean 모니터링 모드에서 빠져나오고 싶다면 Ctrl + C 키를 누르면 된다.\n... Cleaning: phase 3 of 12 (pre-enumeration) 21.1% complete, 1088 GiB free; time: phase 0:57:01, total 1:03:05 Cleaning은 총 12단계로 구성되어 있다. 첫 단계(phase 1)는 약 2분 안에 완료된다.\nCleaning 단계(phase)가 높아질 수록 점점 장시간 소요된다.\n$ filesys clean watch Beginning \u0026#39;filesys clean\u0026#39; monitoring. Use Control-C to stop monitoring. Cleaning: phase 4 of 12 (pre-filter) 65.4% complete, 1088 GiB free; time: phase 0:03:07, total 1:15:51 1시간 15분이 지나도 전체 12단계 중 4단계를 진행중인 상황이다.\n참을성을 가지고 시간날 때마다 가끔씩 Filesystem cleaning의 진행사항을 모니터링한다.\n이제 긴 시간을 기다려본다.\n5. Cleaning 결과 확인 # Filesystme cleaning을 실행한 지 약 3시간이 지난 후 다시 Data Domain에 접속한다.\n$ filesys clean watch **** Cleaning is not in progress. 진행중인 Cleaning 작업이 없다고 한다.\n$ filesys clean status Cleaning finished at 2022/01/05 17:49:06. Cleaning이 완료된 시간을 확인할 수 있다.\ncleaning이 완료된 후 df 명령어로 사용률(Use%)이 낮아지고 여유공간이 확보되었는지 다시 확인한다.\n$ df Active Tier: Resource Size GiB Used GiB Avail GiB Use% Cleanable GiB* ---------------- -------- -------- --------- ---- -------------- /data: pre-comp - 25769.9 - - - /data: post-comp 10436.7 8866.3 1570.4 85% 2213.3 /ddvar 49.2 7.1 39.6 15% - /ddvar/core 49.2 0.2 46.5 0% - ---------------- -------- -------- --------- ---- -------------- * Estimated based on last cleaning of 2022/01/05 17:49:06. /data: post-comp(중복제거 후 용량)의 사용률(Use%)이 90%에서 85%로 5% 감소했다.\n이제 끝!\n","date":"Jan 5, 2022","permalink":"/blog/resolving-space-utilization-issue-in-dd2500/","section":"Blogs","summary":"개요 # EMC사의 VTL(Virtual Tape Library) 장비인 DD2500에서 디스크 여유공간이 부족할 때 파일시스템 정리(Filesystem cleanining) 조치를 통해 용량 확보하는 방법을 설명한다.","title":"DD2500 용량 부족시 Filesystem cleaning"},{"content":"개요 # 도커에서 \u0026lt;none\u0026gt; 태그가 붙은 이미지dangling image들이 많이 쌓인 상황일 경우, 명령어를 통해 한 번에 dangling image를 정리하는 방법을 설명합니다.\n발단 # 도커를 사용하던 중 \u0026lt;none\u0026gt; 태그가 붙은 쓸모없는 이미지들이 쌓여버렸다.\n4개의 도커 이미지 정리가 필요하다.\n$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; a52cae4543de 10 minutes ago 522MB younsunglee/docker-nodejs-toyproject 746c681 5dee88486167 2 days ago 1GB younsunglee/docker-nodejs-toyproject \u0026lt;none\u0026gt; 3a8850aa21bf 3 days ago 1.01GB younsunglee/docker-nodejs-toyproject bd2f64b 4ece06c282a8 3 days ago 1GB younsunglee/docker-nodejs-toyproject 8d27e5f49 7e7bf7400489 3 days ago 1GB younsunglee/docker-nodejs-toyproject latest 7e7bf7400489 3 days ago 1GB younsunglee/docker-nodejs-toyproject \u0026lt;none\u0026gt; 7e989628d966 3 days ago 1GB jenkins-docker latest 141715b7bf00 3 days ago 522MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 0bd92a6415d2 3 days ago 522MB node latest a283f62cb84b 2 weeks ago 993MB mysql latest 3218b38490ce 2 weeks ago 516MB jenkins/jenkins lts 2a4bbe50c40b 4 weeks ago 441MB gradle jdk8-alpine 8017d8c2ba74 2 years ago 204MB node 4.6 e834398209c1 5 years ago 646MB 환경 # Architecture : x86_64 OS : Ubuntu 20.04.3 LTS Shell : bash Docker : version 20.10.12, build e91ed57 TL;DR # 시간이 없는 분들을 위해서 조치방법만 요약한 내용입니다.\n자세한 해결방법은 아래 해결방법을 참조하세요.\n# 이미지 목록 조회 $ docker images # dangling 이미지 삭제 $ docker rmi $(docker images -f \u0026#34;dangling=true\u0026#34; -q) # 위 명령어가 실행 안될 경우는 강제 삭제(-f, --force) 시도 $ docker rmi -f $(docker images -f \u0026#34;dangling=true\u0026#34; -q) # 삭제 결과 확인 $ docker images 해결방법 # 1. 이미지 확인 # \u0026lt;none\u0026gt; 태그가 붙은 쓸모없는 이미지들을 먼저 확인한다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; a52cae4543de 16 minutes ago 522MB younsunglee/docker-nodejs-toyproject 746c681 5dee88486167 2 days ago 1GB younsunglee/docker-nodejs-toyproject \u0026lt;none\u0026gt; 3a8850aa21bf 3 days ago 1.01GB younsunglee/docker-nodejs-toyproject bd2f64b 4ece06c282a8 3 days ago 1GB younsunglee/docker-nodejs-toyproject 8d27e5f49 7e7bf7400489 3 days ago 1GB younsunglee/docker-nodejs-toyproject latest 7e7bf7400489 3 days ago 1GB younsunglee/docker-nodejs-toyproject \u0026lt;none\u0026gt; 7e989628d966 3 days ago 1GB jenkins-docker latest 141715b7bf00 3 days ago 522MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 0bd92a6415d2 3 days ago 522MB node latest a283f62cb84b 2 weeks ago 993MB mysql latest 3218b38490ce 2 weeks ago 516MB jenkins/jenkins lts 2a4bbe50c40b 4 weeks ago 441MB gradle jdk8-alpine 8017d8c2ba74 2 years ago 204MB node 4.6 e834398209c1 5 years ago 646MB 나는 심한 편은 아니지만 약 4개정도의 \u0026lt;none\u0026gt; 태그가 붙은 이미지들이 있다.\nDocker에서는 이런 이미지를 댕글링 이미지(dangling image)라고 부른다.\nDangling Image # 댕글링 이미지는 같은 이름과 태그의 새 이미지로 덮어쓸 때 생성된다.\n동일한 태그를 가진 Docker 이미지가 빌드될 경우, 기존에 있던 이미지는 삭제되지는 않고 tag가 \u0026lt;none\u0026gt;으로 변경된 상태로 남아 있는다.\n댕글링 이미지는 태그가 지정된 이미지와 관련이 없는 불필요한 레이어이기 때문에 정리가 필요하다.\n댕글링 이미지는 더 이상 이미지 빌드에 사용하지 못하고 디스크 공간만 차지할 뿐이다. 관리적 측면에서도 docker images 명령어로 도커 이미지를 확인하는 상황에서 헷갈리게 만든다. 이런 여러가지 이유로 주기적으로 댕글링 이미지(dangling image)를 정리할 필요가 있다.\n2. 삭제 전 IMAGE ID 확인 # $ docker images -f \u0026#34;dangling=true\u0026#34; -q a52cae4543de 3a8850aa21bf 7e989628d966 0bd92a6415d2 \u0026lt;none\u0026gt; 태그가 붙은 도커 이미지의 Image ID만 출력된다.\n명령어 옵션\n-f \u0026lt;Key=Value\u0026gt; : 이미지를 조회할 때 특정 조건(filter)을 걸어서 조건에 해당되는 이미지만 출력한다.\n-q (--quiet) : 이미지 ID만 출력한다.\n3. 이미지 삭제 # \u0026lt;none\u0026gt; 태그가 붙은 dangling된 이미지들만 필터링해서 삭제한다.\n$ docker rmi $(docker images -f \u0026#34;dangling=true\u0026#34; -q) Deleted: sha256:a52cae4543de475309ec97e30d7e1ec26b837bd6b1c34d8964e50f6d32be2742 Deleted: sha256:aad7d00427d8fdddfe06e6163e2a0696954bc9a0561f52e035b200b554f2e910 Deleted: sha256:f28088ff254ca63f5eb796dc7cb6cd34a51b9f6904f5b46ab36b84e817369c95 Deleted: sha256:2683a7c7d842eab9795d164c7a5271753eac60024d8a95bcb74c27a150c9438f Untagged: younsunglee/docker-nodejs-toyproject@sha256:52baff698058be6b7bd961857038e19ad019a03e95210ca053176bee6eb37f9e Deleted: sha256:3a8850aa21bfc17d9ddd3bb2a1cbbae7c752b69ed62ef15d9f6d5220f9ae7d02 Deleted: sha256:b69cba1495e82131fc54da34b136fa88f8d4066e8cb18354b25f2b3dd0ecf84d Deleted: sha256:69e70dbd2172741b5215a4ccc127d14a3c44d17ceba5d4d214f8cbe0b931c64f Deleted: sha256:1d850ae78effc483872e1f594733ef7858b09c9c98276d30c6350674527d48e9 Deleted: sha256:bad8a7081865b98d6d799161ca21c3e4328b235d30f3510d0ef78e586a63d836 Deleted: sha256:06e0c9e5ff491fd138f7762d1d1a2803e545aff3d22bfdcaace5d9a44d2542ed Deleted: sha256:0bd92a6415d2e368042811b8a1cb4b2481b5fab4fd5e82bd5e7af83d154e0eff Deleted: sha256:ac00a7709991e802542a5ef86251f01e1aced9369d6bbcc9dbcae9fa1f8f3a21 Deleted: sha256:254da0fd6eee155fdd01d19c02efde3ed476d495925c02ddd463147978d7df84 특정 컨테이너 이미지가 삭제 안될 경우 # 증상\n컨테이너 이미지 삭제 과정에서 image is being used by stopped container ... 에러 메세지와 함께 특정 이미지가 삭제되지 않는다.\n$ docker rmi $(docker images -f \u0026#34;dangling=true\u0026#34; -q) Deleted: sha256:a52cae4543de475309ec97e30d7e1ec26b837bd6b1c34d8964e50f6d32be2742 Deleted: sha256:aad7d00427d8fdddfe06e6163e2a0696954bc9a0561f52e035b200b554f2e910 Deleted: sha256:f28088ff254ca63f5eb796dc7cb6cd34a51b9f6904f5b46ab36b84e817369c95 Deleted: sha256:2683a7c7d842eab9795d164c7a5271753eac60024d8a95bcb74c27a150c9438f Untagged: younsunglee/docker-nodejs-toyproject@sha256:52baff698058be6b7bd961857038e19ad019a03e95210ca053176bee6eb37f9e Deleted: sha256:3a8850aa21bfc17d9ddd3bb2a1cbbae7c752b69ed62ef15d9f6d5220f9ae7d02 Deleted: sha256:b69cba1495e82131fc54da34b136fa88f8d4066e8cb18354b25f2b3dd0ecf84d Deleted: sha256:69e70dbd2172741b5215a4ccc127d14a3c44d17ceba5d4d214f8cbe0b931c64f Deleted: sha256:1d850ae78effc483872e1f594733ef7858b09c9c98276d30c6350674527d48e9 Deleted: sha256:bad8a7081865b98d6d799161ca21c3e4328b235d30f3510d0ef78e586a63d836 Deleted: sha256:06e0c9e5ff491fd138f7762d1d1a2803e545aff3d22bfdcaace5d9a44d2542ed Deleted: sha256:0bd92a6415d2e368042811b8a1cb4b2481b5fab4fd5e82bd5e7af83d154e0eff Deleted: sha256:ac00a7709991e802542a5ef86251f01e1aced9369d6bbcc9dbcae9fa1f8f3a21 Deleted: sha256:254da0fd6eee155fdd01d19c02efde3ed476d495925c02ddd463147978d7df84 Error response from daemon: conflict: unable to delete 7e989628d966 (cannot be forced) - image is being used by stopped container 234d5e511f5f 원인\n해당 이미지를 중지된 상태(stopped container)의 컨테이너가 참조하고 있어서 삭제가 불가능하다는 의미이다.\n조치방법\n도커 이미지를 삭제 명령어인 docker rmi를 실행할 때 강제 삭제-f, --force 옵션을 주면 된다.\n$ docker rmi -f $(docker images -f \u0026#34;dangling=true\u0026#34; -q) Untagged: younsunglee/docker-nodejs-toyproject@sha256:a7098ab967d86ee2b2d7431cc4be1c940b100cb964e86fb29fb6f964ba67d381 Deleted: sha256:7e989628d966c9c6c79477d85551cf72deac685286b3a598369e31465335b50e Deleted: sha256:172a4bf7f894bd7531cf11c24462aa3445667ec4ad80f5b7dec91b2326f70291 Deleted: sha256:8aaa4b338fc57490904b23497b579b4f760291a565d27f72d951c94b87c344fd Deleted: sha256:e00dfa43e425da0e1a3d3493b40cb6a298cf3ab82621531f4a87edb0542d4ece 아까 삭제 실패했던 이미지가 -f 옵션을 주니까 삭제 처리된다.\n4. 이미지 삭제결과 확인 # $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE younsunglee/docker-nodejs-toyproject 746c681 5dee88486167 2 days ago 1GB younsunglee/docker-nodejs-toyproject bd2f64b 4ece06c282a8 3 days ago 1GB younsunglee/docker-nodejs-toyproject 8d27e5f49 7e7bf7400489 3 days ago 1GB younsunglee/docker-nodejs-toyproject latest 7e7bf7400489 3 days ago 1GB jenkins-docker latest 141715b7bf00 3 days ago 522MB node latest a283f62cb84b 2 weeks ago 993MB mysql latest 3218b38490ce 2 weeks ago 516MB jenkins/jenkins lts 2a4bbe50c40b 4 weeks ago 441MB gradle jdk8-alpine 8017d8c2ba74 2 years ago 204MB node 4.6 e834398209c1 5 years ago 646MB 이것으로 \u0026lt;none\u0026gt; 태그가 붙은 dangling 이미지들이 모두 삭제되었다.\n조치 완료.\n","date":"Jan 5, 2022","permalink":"/blog/removing-dangling-image-in-docker/","section":"Blogs","summary":"개요 # 도커에서 \u0026lt;none\u0026gt; 태그가 붙은 이미지dangling image들이 많이 쌓인 상황일 경우, 명령어를 통해 한 번에 dangling image를 정리하는 방법을 설명합니다.","title":"Docker dangling 이미지 삭제"},{"content":"발단 # submodule 레포지터리인 seyslee.github.io의 branch name을 master에서 main으로 변경하던 작업중 꼬여버려서 에러가 발생했다.\nseyslee.github.io 레포지터리와 연결된 submodule인 public 디렉토리를 다시 등록할 필요가 있었다.\n환경 # OS : macOS Monterey 12.1\nGit : git version 2.32.0 (Apple Git-132)\n터미널 프로그램 : iTerm2 v3.4.15 (brew로 설치됨)\nShell : zsh\n해결방법 # 1. submodule 삭제 # submodule 목록 확인 # submodule을 완전삭제하기 전에 submodule 목록을 확인한다.\n$ git submodule -47b65cc887e6b6f4a00b439906aa669ade23ee8c public 9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da themes/hugo-theme-codex (v1.6.0) 2개의 submodule 중에서 public 이라는 이름의 submodule을 삭제할 것이다.\nsubmodule deinit # public이라는 이름의 submodule을 등록해제(Unregister)한다.\n$ git submodule deinit -f public 명령어 옵션\n-f (--force) : 명령어를 강제로 실행한다. -f 옵션은 add, update, deinit 명령어에서만 사용 가능하다. 로컬에서 submodule 디렉토리 삭제 # ./git/modules/ 폴더 안의 모듈 디렉토리를 삭제한다.\n$ rm -rf .git/modules/public git에서 submodule 디렉토리 삭제 # 마지막으로 git에서 해당 실제 폴더를 삭제한다.\n$ git rm -f public submodule 목록 재확인 # 다시 submodule 목록을 확인해보자.\n$ git submodule 9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da themes/hugo-theme-codex (v1.6.0) 기존에 있던 public이라는 이름의 submodule이 삭제된 걸 확인할 수 있다.\n2. submodule 등록 # submodule 다시 추가 # 삭제한 submodule을 다시 등록하고 싶다면 아래 명령어를 입력한다.\n명령어 형식\n$ git submodule add -b \u0026lt;branch 이름\u0026gt; https://github.com/\u0026lt;Github 유저명\u0026gt;/\u0026lt;레포지터리 이름\u0026gt;.git \u0026lt;로컬에 생성할 디렉토리 이름\u0026gt; 실제 명령어\n삭제한 submodule을 다시 로컬에 생성하고 submodule로 등록하는 과정이다.\nBranch 이름과 레포지터리 이름, 연결할 디렉토리 명은 개개인의 환경마다 다르기 때문에 명령어를 잘 참고해서 변경해 쓰면 된다.\n$ git submodule add -b main https://github.com/seyslee/seyslee.github.io.git public Cloning into \u0026#39;/Users/ive/githubrepos/blog/public\u0026#39;... remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (3/3), done. 마지막 라인에 출력된 Receiving objects: ..., done. 메세지가 출력된 걸 보면 문제없이 복제가 된 것 같다.\n이제 실제 디렉토리를 받아왔는지 로컬에서 확인해보자.\nsubmodule 추가 결과 확인 # 내 로컬 PC에서 블로그 작업경로를 확인한다.\n$ pwd /Users/ive/githubrepos/blog $ ls -lh total 16 drwxr-xr-x 3 ive staff 96B 7 30 20:00 archetypes -rw-r--r-- 1 ive staff 2.5K 1 2 22:41 config.toml drwxr-xr-x 6 ive staff 192B 12 7 23:45 content -rwxrwxrwx@ 1 ive staff 585B 1 2 22:05 deploy.sh drwxr-xr-x 5 ive staff 160B 12 8 00:39 layouts drwxr-xr-x 16 ive staff 512B 1 3 00:01 public drwxr-xr-x 4 ive staff 128B 8 1 14:43 resources drwxr-xr-x 5 ive staff 160B 1 2 23:22 static drwxr-xr-x 4 ive staff 128B 7 30 20:03 themes public 디렉토리가 새로 생성되었다.\n$ git submodule ebfe49d9e92afef6bd11df4eb077439a8df18808 public (heads/main) 9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da themes/hugo-theme-codex (v1.6.0) seyslee.github.io 레포지터리에 연결된 public 디렉토리가 잘 복제(Clone)되었다!\nbranch 이름도 master가 아닌 main 으로 변경됐다.\nGithub에 로그인한 후 레포지터리에 들어가서 직접 확인해본 결과도 동일하다.\npublic 디렉토리에 파란색 링크가 걸려있다는 것은 submodule로 다른 Repository와 연결되어 있다는 걸 의미한다.\n참고자료 # http://snowdeer.github.io/git/2018/08/01/how-to-remove-git-submodule/\nGit - git-submodule Documentation\n","date":"Jan 3, 2022","permalink":"/blog/removing-submodule-in-github/","section":"Blogs","summary":"발단 # submodule 레포지터리인 seyslee.","title":"submodule 삭제 후 재등록"},{"content":"개요 # Jenkins와 Slack을 Incoming webhooks를 이용해서 연동해보자.\nIncoming webhooks는 Slack 자체 플러그인(앱)이다.\n전제조건 # 컨테이너 형태로 구축된 Jenkins Server : 실습에서 중요한 핵심 요소\nSlack 계정 : Jenkins와 Slack을 연동할 때 필요\nGithub 계정 : Github Repository를 만들고 Jenkinsfile 코드를 업로드할 때 필요\n환경 # 시스템 구성도 # DigitalOcean이라는 클라우드 플랫폼 위에 Ubuntu 20.04.3 OS가 설치된 Virtual Server 1대를 생성해놓았다.\n$ cat /etc/os-release | grep VERSION VERSION=\u0026#34;20.04.3 LTS (Focal Fossa)\u0026#34; VERSION_ID=\u0026#34;20.04\u0026#34; VERSION_CODENAME=focal $ arch x86_64 그 리눅스 서버 위에 Docker를 설치후 Jenkins를 도커 컨테이너로 올린 구성이다.\n$ ps -ef | grep docker root 2924 1 0 2021 ? 00:01:10 /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c269a4e8a445 jenkins-docker \u0026#34;/sbin/tini -- /usr/…\u0026#34; 26 hours ago Up 26 hours 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp, 0.0.0.0:50000-\u0026gt;50000/tcp, :::50000-\u0026gt;50000/tcp jenkins 이 포스팅 자체가 Jenkins 서버를 구축하는 과정을 다루지는 않는다.\nJenkins Server를 직접 구축하고 싶다면 다른 글을 참고하고 오자.\nVirtual Server 환경 상세정보 # Cloud Service Provider : DigtialOcean\nOS : Ubuntu 20.04.3 LTS\nLinux Kernel : 5.4.0-88-generic\nDocker : Docker version 20.10.12, build e91ed57\nJenkins : 2.319.1\n젠킨스 컨테이너를 생성할 때 사용한 컨테이너 이미지는 Jenkins 공식 도커허브 이미지( jenkins/jenkins:lts)이다.\nJenkins Plugin : Slack Notification 2.49\n방법 # 1. Jenkins 플러그인 설치 # Jenkins 관리 페이지로 로그인한다. Jenkins 기본포트는 8080이다.\nJenkins 메인화면 → Jenkins 관리 → 플러그인 관리 메뉴로 들어가자.\n설치 가능 탭으로 이동한다.\n설치 가능 탭에서 slack으로 검색한다.\nSlack Notification 2.49 버전이 검색된다.\nSlack Notification에 Install 체크 → Download now and install after restart 클릭\n젠킨스가 Slack Notification 플러그인을 설치한 후 자동으로 재시작할 것이다.\n잠시 기다린 후 다시 젠킨스에 로그인한다.\n2. Slack setup # slack 공식 홈페이지로 접속한후 새로운 계정으로 가입한다.\n이미 Slack 계정을 가지고 있다면 또 가입할 필요는 없다.\n새 워크스페이스를 만든 후, 워크스페이스에 사용할 앱을 추가하기 위해 앱 화면으로 들어간다.\n검색창에 webhook을 입력한다.\nIncoming Webhooks를 워크스페이스에 연결하기 위해 추가한다.\nIncoming Webhooks의 설정에 들어간다.\n채널에 포스트 값에 alerts를 입력한다. #alerts 채널이 웹후크 메세지를 받겠다는 것이다.\n[수신 웹후크 통합 앱 추가] 버튼을 클릭한다.\n하단에 내려보면 웹후크 URL이 보인다.\n지금 바로 사용할 것은 아니기 때문에 웹후크 URL을 메모장에 따로 복사해놓는다.\n3. Jenkins Slack 설정 # Jenkins 메인화면 → 시스템 설정 메뉴로 들어간다.\n환경설정에서 하단으로 쭉 내려보면, Slack 섹션이 있다. 여기서 Credential 값에 아까 복사한 웹후크 URL이 들어갈 것이다.\n[고급\u0026hellip;] 버튼을 눌러서 전체 속성을 볼 수 있게 화면을 확장 시킨다.\nCredential → Add → Jenkins 를 눌러서 Slack용 로그인 정보를 새로 생성하자.\n아까 복사한 웹후크 URL을 보자.\n웹후크 URL의 앞부분(https://hooks.slack.com/services/)을 제외하고 뒷부분만 복사한다.\nSlack Credential 생성 # [Add] 버튼을 누르면 아래와 같은 Credential 생성화면이 나타난다.\nKind : Secret text\nSecret : 웹후크 URL의 뒷부분을 복사 붙여넣기한다.\nID : slack-alerts (본인이 원하는 걸로 바꿔도 무방하다.)\nDescription : ID에 대한 부가설명\n참고로 이렇게 Credential을 쓰는 이유는 보안을 강화하기 위해서이다.\n실제 운영 환경에서 Webhook URL이라던지 ID/PW 값을 공개할 수는 없으니, Jenkins가 따로 Credential을 안전하게 관리한다.\nSlack 세팅 점검 # Credential : slack-alerts (우리가 방금 만든)\nDefault channel / member id : #alerts\nOverride url : https://hooks.slack.com/services/\n위 설정값들을 입력 후 Test Connection 버튼 클릭\n설정값을 올바르게 넣었다면 접속 성공이라는 메세지(Success)가 떨어진다. Test Connection 버튼을 여러번 눌러본다.\nTest Connection 버튼을 누를 때마다 #alert 채널로 메세지가 들어온다.\n최종적으로는 Jenkins build 실패시에도 Slack 메세지를 자동으로 보내주도록 구현할 것이다.\n4. Jenkinsfile 준비 # Jenkinsfile은 젠킨스 파이프라인이 동작하는 전체 과정을 코드로 적어놓은 명세서라고 생각하면 된다. Jenkinsfile은 groovy라는 언어를 사용한다.\n새 레포지터리를 만들고 Jenkinsfile 을 올려놓는다. 내 경우 환경은 아래와 같다.\n레포지터리 이름 : jenkins-pipeline-toyproj 바로가기\nJenkinsfile 경로 : /5-slack-notifications/Jenkinsfile (깃허브 레포 기준)\n내 경우 로컬 환경에서 Jenkinsfile 코드를 작성하고 레포지터리를 연결한 다음, git push로 코드(Jenkinsfile)를 미리 레포지터리에 밀어넣은 상태이다.\nJenkinsfile 코드는 다음과 같다.\nnode { // job try { stage(\u0026#39;build\u0026#39;) { println(\u0026#39;so far so good...\u0026#39;) } stage(\u0026#39;test\u0026#39;) { println(\u0026#39;A test has failed!\u0026#39;) sh \u0026#39;exit 1\u0026#39; } } catch(e) { // mark build as failed currentBuild.result = \u0026#34;FAILURE\u0026#34;; // send slack notification slackSend (color: \u0026#39;#FF0000\u0026#39;, message: \u0026#34;FAILED: Job \u0026#39;${env.JOB_NAME} [${env.BUILD_NUMBER}]\u0026#39; (${env.BUILD_URL})\u0026#34;) // throw the error throw e; } } Jenkinsfile 실행시 진행순서\nJenkins에서 github에 올라가있는 Jenkinsfile을 통해 build를 시작한다.\n파이프라인에서 build stage는 통과한다.\n파이프라인의 test stage에서 에러가 발생할 수 밖에 없다. (exit 1을 반환하도록 짜여져 있으니까)\nJenkins는 빌드에 에러 발생을 감지한다.\nJenkins는 slackSend()를 통해 미리 설정된 Slack webhook로 에러 메세지를 보낸다.\nSlack의 #alert 채널로 Jenkins가 보낸 빌드에러 메세지가 들어온다.\n5. Jenkins pipeline 생성 # Jenkins 메인화면 → create a new job으로 들어간다.\nItem name : slack notification (본인이 하고 싶은걸로 바꿔도 무방함)\n종류 : Pipeline\n[OK] 버튼을 누르면 다음 화면으로 넘어간다.\nPipeline 정보를 넣는다.\nBranch Specifier 값이 master가 아닌 main이라는 점을 체크한다.\nDefinition : Pipeline script from SCM\nSCM : Git\nRepository URL : https://github.com/seyslee/jenkins-pipeline-toyproj.git\n스스로 구축한 Github Repository가 있다면 그걸로 바꿔서 입력. Branch Specifier : */main\n2020년 10월을 기점으로 github는 default branch 이름인 master가 인종 차별적 단어라는 이유로 main이 기본 생성되도록 변경했다. 원문 : The default branch for newly-created repositories is now main | GitHub Changelog Script Path : 5-slack-notification/Jenkinsfile (해당 레포 기준에서의 Jenkinsfile의 경로를 의미한다.) 입력 후 저장한다.\nslack notification 초기화면 → [Build Now] 버튼 클릭\nBuild를 시작하면 Jenkins이 Github Repository에 저장되어 있는 Jenkinsfile을 기반으로 파이프라인을 실행한다.\n우리가 의도한대로 build stage는 문제없이 통과하고 test stage에서 실패한다.\n에러화면\n아랫줄에 출력된 로그를 자세히 보자.\n..... [Pipeline] // stage [Pipeline] slackSend Slack Send Pipeline step running, values are - baseUrl: https://hooks.slack.com/services/, teamDomain: \u0026lt;empty\u0026gt;, channel: #alert, color: #FF0000, botUser: false, tokenCredentialId: slack-alerts, notifyCommitters: false, iconEmoji: \u0026lt;empty\u0026gt;, username: \u0026lt;empty\u0026gt;, timestamp: \u0026lt;empty\u0026gt; [Pipeline] } [Pipeline] // node ..... baseUrl, channel 값이 jenkins slack 설정값과 동일하게 제대로 넘어갔는지 체크한다.\nbuild가 실패하자 #alert 채널에 FAILED: Job 메세지가 출력되었다.\n전달받은 slack 메세지에 딸린 URL을 클릭하면 해당 오류를 발생시킨 Jenkins build 페이지로 연결된다!\n정상 접속된 Jenkins 웹페이지\n결론 # ChatOps가 요즘 운영 트렌드이다. 트렌드가 바뀌면서 DevOps Engineer에게 이제 Jenkins는 도커와 동등한 수준을 요구하는 핵심 스킬이 된 것 같다.\nJenkins와 Slack의 다양한 앱을 통합하면 서비스 모니터링과 알람 자동화를 할 수 있다. 그리고 무엇보다도 Jenkins는 모든 걸 다 붙일 수 있는 만능툴이니 꼭 딥하게 배워놓자.\n이상으로 긴 글 읽어주신 독자분들 감사합니다.\nPS. 실습을 따라 진행하시다가 잘 안될 경우, 댓글을 달아주시면 같이 해결을 도와드리겠습니다.\n","date":"Jan 2, 2022","permalink":"/blog/slack-notifications-in-jenkins/","section":"Blogs","summary":"개요 # Jenkins와 Slack을 Incoming webhooks를 이용해서 연동해보자.","title":"Jenkins와 Slack webhook 연동"},{"content":"","date":"Dec 29, 2021","permalink":"/tags/database/","section":"Tags","summary":"","title":"database"},{"content":"","date":"Dec 29, 2021","permalink":"/tags/oracle/","section":"Tags","summary":"","title":"oracle"},{"content":"발단 # 2주라는 짧은 기간동안 특정 테이블스페이스의 사용률이 65%에서 90%로 급증했다.\n환경 # Architecture : x86_64 Database : Oracle Database 12c Standard Edition Release 12.x.x.x.x Shell : bash sqlplus : sqlplus 실행시 관리자 권한(sys) 필요 원인 # 이미지 파일이나 음성 등의 비정형 데이터를 담는 LOBSEGMENT 사이즈의 증가가 원인이었다.\n테이블스페이스의 용량 산정에 오류가 발생한 것이 아니라, 정말로 용량이 늘어난 것이다.\nLOB 데이터 # LOB이란 Large Object의 약자로, 대용량 데이터를 저장하고 관리하기 위해 오라클에서 제공하는 기본 데이터 타입이다.\nLOB 데이터는 사진, 음성, 이미지 등 비구조화된 큰 용량의 파일을 저장하기 때문에 일반 SELECT ... FROM ... SQL문으로는 조회가 불가능하다.\nLOB의 종류 (Types of LOBs)\nCLOB : Character Large Objects 의 약자. 문자 대형객체(Character), Oracle은 CLOB과 VARCHAR2 사이에 암시적 변환을 수행함. BLOB : Binary Large Objects 의 약자. 이진 대형객체(Binary), 이미지, 동영상, MP3 등 NCLOB : Nation Character Large Objects 의 약자. 내셔널 문자 대형객체, Oracle에서 정의되는 National Character set을 따르는 문자 BFILE : Binary File Objects 의 약자. OS에 저장되는 이진 파일의 이름과 위치를 저장. 읽기 전용(Read Only) 모드로만 액세스 가능 확인방법 # 특정 테이블스페이스의 용량 분석 과정은 아래와 같다.\n1. 테이블별 용량 확인 # 입력 명령어\n-- SQL 실행결과를 가독성 있게 잘 정렬해서 출력하는 부분 SET LINESIZE 300; -- 실제 수행 SQL문 SELECT TOTAL.TABLESPACE_NAME, ROUND(TOTAL.MB, 2) AS TOTAL_MB, ROUND(TOTAL.MB - FREE.MB, 2) AS USED_MB, ROUND((1 - FREE.MB / TOTAL.MB) * 100, 2) || \u0026#39;%\u0026#39; AS USED_PER FROM (SELECT TABLESPACE_NAME, SUM(BYTES)/1024/1024 AS MB FROM DBA_FREE_SPACE GROUP BY TABLESPACE_NAME) FREE, (SELECT TABLESPACE_NAME, SUM(BYTES)/1024/1024 AS MB FROM DBA_DATA_FILES GROUP BY TABLESPACE_NAME) TOTAL WHERE FREE.TABLESPACE_NAME = TOTAL.TABLESPACE_NAME ORDER BY USED_PER DESC; 명령어 결과\nTABLESPACE_NAME TOTAL_MB USED_MB USED_PER ------------------------------ ---------- ---------- ----------------------------------------- SYSTEM 1320 1310.06 99.25% UNDOTBS1 1320 1227.88 93.02% SYSAUX 900 731.69 81.3% USERS 5 4 80% SECRET 30720 24201.69 78.78% SECRET_IDX 10240 6.56 .06% 6 행이 선택되었습니다. 문의 오기 전에 이미 개발자 요청에 의해서 SECRET 테이블스페이스에 5GB를 추가해서 사이즈를 30GB로 늘린 상황이다.\n2. 개발자의 문의 # 테이블스페이스의 사용률을 낮춰서 안전하게 조치는 끝났다.\n하지만 이후 개발자는 근본적인 테이블스페이스 용량 급증의 원인을 물어봤다.\nDeveloper : \u0026ldquo;왜 SECRET 테이블스페이스의 용량이 급증한걸까요?\u0026rdquo;\nSysadmin (me) : (그건 저도 의문입니다.)\n이제 본격적으로 테이블스페이스에 연결된 테이블별 용량을 확인하는 단계로 들어간다.\n3. 테이블스페이스 용량 확인 (SQL) # 입력 명령어\n-- SQL 실행결과를 가독성 있게 잘 정렬해서 출력하는 부분 SET LINESIZE 300; SET PAGES 1000; COL FILE_NAME FOR A50; -- 실제 수행 SQL문 SELECT TABLESPACE_NAME, FILE_NAME, ROUND(BYTES/1024/1024/1024, 2) AS GB, ROUND(MAXBYTES/1024/1024/1024, 2) AS MAXGB, AUTOEXTENSIBLE FROM DBA_DATA_FILES; 명령어 결과\nTABLESPACE_NAME FILE_NAME GB MAXGB AUT ------------------------------ -------------------------------------------------- ---------- ---------- --- SYSTEM /data/SECRETDB/system01.dbf 1.29 32 YES SYSAUX /data/SECRETDB/sysaux01.dbf .88 32 YES UNDOTBS1 /data/SECRETDB/undotbs01.dbf 1.29 32 YES USERS /data/SECRETDB/users01.dbf 0 32 YES SECRET /data/SECRET 30 31 YES SECRET_IDX /data/SECRET_IDX 10 20 YES 6 행이 선택되었습니다. SECRET 테이블스페이스의 데이터 파일은 /data/SECRET에 위치해있고, 현재 할당된 용량은 30GB이다.\n4. 테이블스페이스 용량 확인 (OS) # $ ls -lh /data/SECRET -rw-r----- 1 oracle dba 31G 12월 29 09:22 /data/SECRET ls -lh 명령어로 확인해본 결과도 sqlplus에서 확인한 결과처럼 동일하게 31GB이다.\n5. 테이블스페이스에 속한 테이블 조회 # 입력 명령어\n-- SQL 실행결과를 가독성 있게 잘 정렬해서 출력하는 부분 COL OWNER FOR A10; COL TABLE_NAME FOR A20; -- 실제 수행 SQL문 SELECT OWNER, TABLE_NAME, TABLESPACE_NAME FROM DBA_TABLES WHERE TABLESPACE_NAME=\u0026#39;SECRET\u0026#39;; 명령어 결과\nOWNER TABLE_NAME TABLESPACE_NAME ---------- -------------------- ------------------------------ DEVXXX XXXXX_XXX SECRET DEVXXX XXXX_XXXXX_TBL SECRET DEVXXX XXXXXXX_TBL SECRET DEVXXX XXXXXXX_XXXX_TBL SECRET DEVXXX XXXX_XXXXX_TBL SECRET DEVXXX XXXX_XXX_TBL SECRET DEVXXX XXXXXX_XXXXX_TBL SECRET 7 행이 선택되었습니다. SECRET 테이블스페이스에는 7개의 테이블이 속한다.\n6. 테이블별 용량 조회 # 입력 명령어\n-- SQL 실행결과를 가독성 있게 잘 정렬해서 출력하는 부분 SET PAGESIZE 1000; SET LINESIZE 1000; COL OWNER FOR A10; COL SEGMENT_TYPE FOR A15; COL SEGMENT_NAME FOR A30; COL TABLESPACE_NAME FOR A15; -- 실제 수행 SQL문 SELECT OWNER, SEGMENT_TYPE, SEGMENT_NAME, TABLESPACE_NAME, BYTES/1024/1024 MB FROM DBA_SEGMENTS WHERE TABLESPACE_NAME=\u0026#39;SECRET\u0026#39; ORDER BY MB DESC; SQL문 상세 설명\nORDER BY MB DESC : MB 단위로 내림차순 정렬 WHERE TABLESPACE_NAME='\u0026lt;테이블스페이스 이름\u0026gt;' : 특정 테이블스페이스와 관련된 테이블, LOB 객체들만 검색한다. 명령어 결과\nOWNER SEGMENT_TYPE SEGMENT_NAME TABLESPACE_NAME MB ---------- --------------- ------------------------------ --------------- ---------- DEVXXX LOBSEGMENT SYS_LOB0000233727C00003$$ SECRET 24190.125 DEVXXX TABLE XXXX_XXX_XXX SECRET 5 DEVXXX TABLE XXXXXXX_XXX SECRET 3 DEVXXX INDEX XXXX_XXX_XXX_PK SECRET 1 DEVXXX TABLE XXXXX_XXX SECRET 1 DEVXXX LOBSEGMENT SYS_LOB0000083246C00003$$ SECRET .125 DEVXXX TABLE XXXXXXX_XXXX_XXX SECRET .125 DEVXXX LOBINDEX SYS_IL0000233727C00003$$ SECRET .0625 DEVXXX TABLE XXXX_XXXXX_XXX SECRET .0625 DEVXXX TABLE XXXX_XXXXX_XXX SECRET .0625 DEVXXX LOBINDEX SYS_IL0000083246C00003$$ SECRET .0625 DEVXXX TABLE XXXXXXX_XXXX_XXX SECRET .0625 12 행이 선택되었습니다. 확인결과\nLOBSEGMENT 타입인 SYS_LOB0000233727C00003$$의 용량이 24190MB(=23.6GB)로 늘어난 상태인걸 확인할 수 있다.\n일반적인 테이블(TABLE TYPE)들은 정형 데이터를 보관하고 있기 때문에 5MB 이하의 용량이다.\n그러나 LOBSEGMENT는 다른 테이블들과 용량 비교를 했을 때 독보적으로 큰 용량임을 알 수 있다.\n","date":"Dec 29, 2021","permalink":"/blog/reason-for-the-rapid-increase-tablespace-size-in-oracle/","section":"Blogs","summary":"발단 # 2주라는 짧은 기간동안 특정 테이블스페이스의 사용률이 65%에서 90%로 급증했다.","title":"Oracle 테이블스페이스 용량 급증의 원인"},{"content":"","date":"Dec 20, 2021","permalink":"/tags/vmware/","section":"Tags","summary":"","title":"vmware"},{"content":"개요 # vSphere HA 구성시 발생하는 네트워크 이중화 관련 알람을 강제로 끌 수 있다.\n환경 # vSphere : vSphere Client 5.x 호스트 클러스터 : vSphere HA 구성됨 문제점 # 여러 대의 호스트로 구성된 클러스터에서 vSphere HA를 구성한 후 느낌표 아이콘과 함께 네트워크 이중화 관련 알람이 발생한다.\n한글 메세지 : 현재 이 호스트에 관리 네트워크 이중화가 없음\n영문 메세지 : Host \u0026lt;xxx\u0026gt; currently has no management network redundancy\n# 해결방안 # 근본적 해결책 : VMware에서는 vSphere HA 구성시 호스트의 네트워크 이중화를 권장한다. 알람을 없애는 근본적인 방법으로는 실제 호스트의 네트워크를 이중화해서 안정적 구성을 완성한다.\n대안 : 네트워크 이중화가 어렵다면, 대책방안으로 vSphere HA의 고급 파라미터를 설정해 네트워크 이중화 알람을 끌 수 있다.\n이 글에서는 2. 대안 방법에 대해서 설명한다.\n\u0008상세 절차 # vSphere Client 로그인 # vSphere Client 프로그램에 로그인한다.\n현재 시나리오에서는 개별 호스트로 접속하는 것이 아닌 전체 호스트를 관리하는 vCenter로 접속한다.\n호스트 상태 확인 # vSphere HA 설정후 네트워크 이중화 구성된 상태가 아니기 때문에 호스트 전체에 경보가 발생한 상태이다. (호스트 아이콘 옆에 노란색 느낌표 표지판)\n호스트 → 구성 → 하드웨어 → 네트워킹 메뉴로 들어가본다.\n실제로 Management Network가 연결된 물리적 어댑터(호스트 서버의 물리 포트)가 vmnic4 1개 뿐인 구성인 점을 확인할 수 있다.\n만약 Management Network에 물리적 어댑터가 2개 연결되어 있을 경우, 네트워크 이중화 조건을 충족했기 때문에 네트워크 이중화 알람은 발생하지 않는다.\n설정 편집 # 클러스터 아이콘 우클릭 → 설정 편집(E)\u0026hellip;\nvSphere HA 고급 옵션 # vSphere HA의 파라미터 설정을 위해 고급 옵션 메뉴로 들어간다.\nvSphere HA 메뉴 → 고급 옵션(O)\u0026hellip; 메뉴로 들어간다.\n파라미터 설정 # 고급 옵션 (vSphere HA)에는 파라미터 값을 설정해 디테일한 설정을 적용해 운영할 수 있다.\n기본값은 입력된 파라미터가 없는 상태이다.\n네트워크 이중화 구성 알람을 끄기 위해 아래 값을 입력한다. 옵션과 값 입력시 대소문자에 주의한다.\n옵션 : das.ignoreRedundantNetWarning\n값 : true\n파라미터 입력 → 확인 vSphere HA 재구성 # 새로 설정한 das.ignoreRedundantNetWarning 파라미터를 vSphere HA 클러스터에 적용을 위해 vSphere HA 구성을 해제했다가 재구성하도록 한다.\n클러스터 기능 메뉴 → vSphere HA 설정 체크해제 → 확인\nvSphere Client 프로그램 하단의 최근 작업 리스트에 vSphere 구성 해제 작업이 진행중인 걸 확인할 수 있다.\nvSphere HA를 다시 구성한다.\n클러스터 기능 메뉴 → vSphere HA 설정 체크 → 확인\n알람 해제 확인 # vSphere HA 구성이 끝나면 새롭게 설정한 파라미터가 클러스터에 적용된다. 이 때 네트워크 이중화 구성 알람이 사라진다.\n위 사진처럼 vSphere HA 클러스터를 구성하는 호스트 전체에 노란색 느낌표 아이콘이 사라진 걸 확인할 수 있다.\n결론 # 알람 끄는 조치방법은 반드시 대안(Alternative)으로만 사용한다. 제조사 권고사항은 물리적 네트워크 이중화 구성임을 잊지 말자.\n참고자료 # VMware의 Knowledge Base 공식문서 1004700\nNetwork redundancy message when configuring vSphere High Availability in vCenter Server (1004700)\n","date":"Dec 20, 2021","permalink":"/blog/configuring-ignore-network-redundancy-in-vsphere-ha/","section":"Blogs","summary":"개요 # vSphere HA 구성시 발생하는 네트워크 이중화 관련 알람을 강제로 끌 수 있다.","title":"vSphere 네트워크 이중화 알람 끄기"},{"content":" # 개요 # 리눅스 서버에 할당한 스토리지 LUN을 multipath를 이용해 연결하는 작업을 설명한다. 참고로 해당 시나리오에서는 LVM(Linux Volume Manager)를 사용하지 않는다.\n환경 # Architecture : x86_64 OS : CentOS release 5.x (Final) Shell : bash Package : device-mapper-multipath-0.4.7-59.el5 전제조건 # 서버, SAN 스위치, 스토리지 간의 물리적 연결이 사전에 완료된 상태\n서버의 OS, 아키텍처 버전에 맞는 multipath 패키지(device-mapper-multipath)가 설치된 상태여야함\n본론 # SAN 구성 # 현재 시나리오의 SAN(Storage Area Network) 구성은 다음과 같다.\nFABRIC ARCHITECTURE +------------------+ | 1 LUN | | +-----------+ | =================== | | Storage | | * LUN 용량 : 1TB | +-----------+ | =================== | ^ | | | | | v | | +-----------+ | =================== | |SAN Switch | | * 8개의 다중경로로 구성함 | +-----------+ | =================== | ^ | | | | | v | | multipath | | +-----------+ | =================== | | Server | | * OS : CentOS 5.x | +-----------+ | =================== | | +------------------+ Storage : Server Host 등록, LUN 할당 작업이 완료된 상태\nSAN Switch : 서버와 스토리지가 광 케이블로 SAN 포트에 연결된 상태, 서버와 스토리지간 Zoning 작업이 완료된 상태\nServer : SAN Switch와 물리적으로 케이블이 연결된 상태\n사전작업 # 1. HBA 카드 확인 # $ lspci | grep -i fibre 40:00.0 Fibre Channel: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA (rev 02) 40:00.1 Fibre Channel: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA (rev 02) 서버에 HBA 카드가 1개 장착된 상태이다.\n2. multipath 패키지 설치여부 확인 # $ rpm -qa | grep device-mapper-multipath device-mapper-multipath-0.4.7-59.el5 이 서버에서는 device-mapper-multipath 0.4.7 버전이 이미 설치된 상태이다.\nCentOS 5.x 기준으로 multipath 패키지는 기본적으로 OS 설치시 포함되어 있기 때문에, 별도로 multipath 패키지 설치가 필요 없었다.\n3. multipathd 실행여부 확인 # $ ps -ef | grep multipathd root 8474 8427 0 18:14 pts/1 00:00:00 grep multipathd multipath 데몬(multipathd)이 동작하고 있지 않다.\n4. multipathd 자동시작 활성화 # $ chkconfig --list multipathd multipathd 0:off 1:off 2:off 3:off 4:off 5:off 6:off 모든 Run Level 1 ~ 5에서 multipathd 자동시작이 비활성화된 상태이다.\n$ chkconfig --level 35 multipathd on Run Level 3과 Run Level 5 일 경우에만 multipathd가 자동시작 되도록 설정한다.\nRun Level\n3 : Full multi user mode. CLI 기반의 다중 사용자 모드. 그래픽 사용자 환경(GUI)을 지원하지 않는다.\n5 : Run Level 3과 유사하나, X11 GUI를 지원.\n$ chkconfig --list multipathd multipathd 0:off 1:off 2:off 3:on 4:off 5:on 6:off 앞으로 multipathd는 Run Level 3과 5일 경우 자동시작된다.\n5. multipathd 실행 # $ service multipathd start Starting multipathd daemon: [ OK ] multipathd를 시작한다.\n$ ps -ef | grep multipathd | grep -v grep root 16274 1 0 20:50 ? 00:00:00 /sbin/multipathd multipathd 가 작동중이다.\nmultipath 설정 # 6. 모듈 생성 # $ modprobe dm-multipath multipath 사용을 위해 dm-multipath 모듈을 올린다.\n$ partprobe partprobe 명령어를 실행해서 리부팅 없이도 파티션 변경사항을 적용할 수 있다. 명령어가 정상적으로 실행 완료되면 별도로 출력 메세지는 나오지 않는다.\n7. LUN Rescan # systool 명령어를 이용해서 FC 포트의 구성과 이름 정보를 수집한다. 참고로 systool 명령어는 sysfsutils 패키지가 설치되어야 사용 가능하다.\n$ systool -c fc_host Class = \u0026#34;fc_host\u0026#34; Class Device = \u0026#34;host7\u0026#34; Device = \u0026#34;host7\u0026#34; Class Device = \u0026#34;host8\u0026#34; Device = \u0026#34;host8\u0026#34; 해당 서버에서 host7, host8 이라는 이름의 2개의 광 포트(FC)를 사용중이다.\n스토리지에서 LUN 할당 작업이 이미 끝난 상태라면, 서버에서 LUN을 재인식(Rescan)하도록 명령어를 실행한다.\n$ echo \u0026#34;1\u0026#34; \u0026gt; /sys/class/fc_host/host7/issue_lip $ echo \u0026#34;1\u0026#34; \u0026gt; /sys/class/fc_host/host8/issue_lip 위의 host7, host8 값은 각 서버마다 다르기 때문에 변경될 수 있다.\nsystool -c fc_host 명령어로 반드시 각자의 서버 환경에서 확인한다.\n8. WWID 확인 # $ scsi_id -g -u -s /block/sdb 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sdc 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sdd 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sde 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sdf 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sdg 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sdh 36006016035303900df9eaf55ec5bec11 $ scsi_id -g -u -s /block/sdi 36006016035303900df9eaf55ec5bec11 /block/sdb 부터 /block/sdi 까지 모두 동일한 WWID를 가지고 있다. /block/sda는 OS가 설치된 로컬 디스크이다.\n여러개의 장치의 WWID가 같다는 것은 물리적으로 동일한 스토리지의 LUN이 여러개로 나타났다는 의미이다.\n9. multipath 설정파일 수정 # blacklist 해제\n$ cat /etc/multipath.conf | grep -v ^# | grep -v ^$ blacklist { devnode \u0026#34;*\u0026#34; } defaults { user_friendly_names yes } multipath 설정파일의 기본값은 모든 Device를 차단하도록 설정되어 있다. 라인 맨 앞에 #을 붙여서 blacklist { ... } 영역을 주석처리한다.\n$ cat /etc/multipath.conf | grep -v ^# | grep -v ^$ #blacklist { # devnode \u0026#34;*\u0026#34; #} defaults { user_friendly_names yes } 이제 multipaths { ... } 구문에 동일한 WWID 값을 추가한다.\nmultipaths 작성\n$ cat /etc/multipath.conf | grep -v ^$ #blacklist { # devnode \u0026#34;*\u0026#34; #} defaults { user_friendly_names yes } multipaths { multipath { wwid 36006016035303900df9eaf55ec5bec11 alias mpath1 } } OS가 설치된 영역이자 로컬 디스크인 /dev/sda의 WWID를 확인한다.\n로컬디스크 제외\n/dev/sda 장치는 OS가 설치된 로컬디스크다.\nmultipath에서 로컬 디스크를 자동으로 설정을 잡을 경우 문제가 발생할 수 있으므로, 로컬 디스크를 multipath에서 잡지 않도록 blacklist 설정한다.\n/sda의 WWID를 조회한다.\n$ scsi_id -g -u -s /block/sda 3600508e000000000d7ca736cf72d740c $ cat /etc/multipath.conf | grep -v ^$ | grep -v ^# blacklist { wwid 3600508e000000000d7ca736cf72d740c } defaults { user_friendly_names yes } multipaths { multipath { wwid 36006016035303900df9eaf55ec5bec11 alias mpath1 } } blacklist의 주석을 해제한 후 multipath에서 자동 설정을 잡지 않도록, /dev/sda의 WWID를 추가해준다.\n$ service multipathd reload Reloading multipathd: [ OK ] 변경된 설정을 적용하기 위해 multipathd를 reload 한다.\n$ service multipathd status multipathd (pid 12269) is running... multipathd 가 정상 실행중이다.\n$ multipath -v2 명령어 결과로 아무런 출력이 없으면 현재 서버와 스토리지의 구성 문제는 없다는 의미이다.\n이제 LUN이 멀티패스 구성이 됐는지 확인한다.\n10. multipath 구성 확인 # $ multipath -ll mpath1 (36006016035303900df9eaf55ec5bec11) dm-0 DGC,VRAID [size=1.0T][features=1 queue_if_no_path][hwhandler=1 emc][rw] \\_ round-robin 0 [prio=1][active] \\_ 7:0:2:0 sdd 8:48 [active][ready] \\_ 7:0:3:0 sde 8:64 [active][ready] \\_ 8:0:2:0 sdh 8:112 [active][ready] \\_ 8:0:3:0 sdi 8:128 [active][ready] \\_ round-robin 0 [prio=0][enabled] \\_ 7:0:0:0 sdb 8:16 [active][ready] \\_ 7:0:1:0 sdc 8:32 [active][ready] \\_ 8:0:0:0 sdf 8:80 [active][ready] \\_ 8:0:1:0 sdg 8:96 [active][ready] 1개의 LUN이 8개의 경로로 멀티패스가 구성되었다. multipath 경로 개수는 각자 구축한 환경에 따라 다르다.\n해당 시나리오의 경우, 8개의 multipath 경로를 잡도록 SAN 스위치와 스토리지 부분에서 Zoning 설정을 의도했다.\n파일시스템 작업 # 11. 파일시스템 생성 # $ mkfs.ext3 /dev/mapper/mpath1 mke2fs 1.39 (29-May-2006) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) 134217728 inodes, 268435456 blocks 13421772 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=4294967296 8192 block groups 32768 blocks per group, 32768 fragments per group 16384 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848 Writing inode tables: done Creating journal (32768 blocks): done Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 38 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 이 시나리오의 경우는 스토리지의 볼륨을 ext3 타입으로 파일시스템을 생성해서 사용한다. 만약 ext4 타입으로 생성하고 싶다면, mkfs.ext4 /dev/mapper/mpath1 명령어를 이용하면 된다.\n12. 파티션 구성 # $ fdisk -l Disk /dev/sda: 298.9 GB, 298999349248 bytes 255 heads, 63 sectors/track, 36351 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/sda1 * 1 131 1052226 83 Linux /dev/sda2 132 17624 140512522+ 83 Linux /dev/sda3 17625 21801 33551752+ 82 Linux swap / Solaris /dev/sda4 21802 36351 116872875 5 Extended /dev/sda5 21802 24412 20972826 83 Linux /dev/sda6 24413 27023 20972826 83 Linux /dev/sda7 27024 29634 20972826 83 Linux /dev/sda8 29635 36351 53954271 83 Linux ..... Disk /dev/dm-0: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/dm-0 의 전체 용량을 파티션 1개로 잡을 예정이다.\n$ fdisk /dev/mapper/mpath1 /dev/mapper/mpath1 디스크를 파티션 설정하기 위해 fdisk 명령어를 실행한다.\np 키를 입력해서 파티션 정보를 출력한다.\n$ fdisk /dev/mapper/mpath1 The number of cylinders for this disk is set to 133674. There is nothing wrong with that, but this is larger than 1024, and could in certain setups cause problems with: 1) software that runs at boot time (e.g., old versions of LILO) 2) booting and partitioning software from other OSs (e.g., DOS FDISK, OS/2 FDISK) Command (m for help): p Disk /dev/mapper/mpath1: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System 아직 아무런 파티션 설정이 없기 때문에, 출력값은 없다.\n새 파티션을 생성하기 위해 n 키를 입력\nCommand (m for help): n primary partition으로 생성을 위해 p 키를 입력한다.\nCommand (m for help): n Command action e extended p primary partition (1-4) p 지금부터는 파티션 생성시 설정 단계이다.\n[Enter] 키는 기본값(default ...) 을 쓰기 위해 입력해준다.\nPartition number (1-4): 1 First cylinder (1-133674, default 1): [Enter] Using default value 1 Last cylinder or +size or +sizeM or +sizeK (1-133674, default 133674): [Enter] Using default value 133674 p 키 입력 : 생성된 파티션 목록 출력(print)\nCommand (m for help): p Disk /dev/mapper/mpath1: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/mapper/mpath1p1 1 133674 1073736373+ 83 Linux mpath1p1 : 해당 디스크(mpath1)의 첫번째 파티션(p1)을 의미한다.\nw 입력 : 생성한 파티션을 영구적으로 디스크에 적용(write)\nCommand (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table. WARNING: Re-reading the partition table failed with error 22: Invalid argument. The kernel still uses the old table. The new table will be used at the next reboot. Syncing disks. 정상적으로 파티션 구성이 완료되면 fdisk 프롬프트에서 Shell 프롬프트로 돌아온다.\n..... WARNING: Re-reading the partition table failed with error 22: Invalid argument. The kernel still uses the old table. The new table will be used at the next reboot. Syncing disks. $ 파티션 설정을 적용한 후 shell로 자동으로 돌아왔다.\n$ partprobe partprobe 명령어로 파티션의 변경사항을 전체 멀티패스 볼륨에 반영한다.\n$ fdisk -l Disk /dev/sda: 298.9 GB, 298999349248 bytes 255 heads, 63 sectors/track, 36351 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/sda1 * 1 131 1052226 83 Linux /dev/sda2 132 17624 140512522+ 83 Linux /dev/sda3 17625 21801 33551752+ 82 Linux swap / Solaris /dev/sda4 21802 36351 116872875 5 Extended /dev/sda5 21802 24412 20972826 83 Linux /dev/sda6 24413 27023 20972826 83 Linux /dev/sda7 27024 29634 20972826 83 Linux /dev/sda8 29635 36351 53954271 83 Linux Disk /dev/sdd: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/sdd1 1 133674 1073736373+ 83 Linux Disk /dev/sde: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/sde1 1 133674 1073736373+ 83 Linux Disk /dev/sdh: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/sdh1 1 133674 1073736373+ 83 Linux Disk /dev/sdi: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/sdi1 1 133674 1073736373+ 83 Linux Disk /dev/dm-0: 1099.5 GB, 1099511627776 bytes 255 heads, 63 sectors/track, 133674 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/dm-0p1 1 133674 1073736373+ 83 Linux partprobe 명령어를 실행한 후, 멀티패스로 구성된 디바이스 /dev/sdd ~ /dev/sdi 전체가 /dev/dm-0 과 동일한 파티션 정보를 갖게 되었다.\n13. 마운트 # 마운트 지점이 될 디렉토리를 새로 생성한다.\n$ mkdir /appdata multipath 구성이된 device(/dev/mapper/mpath1)를 마운트 지점(디렉토리)에 마운트한다.\n$ mount /dev/mapper/mpath1 /appdata 파일시스템 확인\n파일시스템 리스트 결과를 보기 편하게 정렬하려면 column -t 명령어를 조합해서 실행한다.\n$ df -Ph | column -t Filesystem Size Used Avail Use% Mounted on /dev/sda8 50G 499M 47G 2% / /dev/sda7 20G 2.7G 16G 15% /usr /dev/sda6 20G 238M 19G 2% /var /dev/sda5 20G 174M 19G 1% /home /dev/sda2 130G 188M 123G 1% /test /dev/sda1 996M 41M 904M 5% /boot tmpfs 7.9G 0 7.9G 0% /dev/shm /dev/mapper/mpath1 1008G 200M 957G 1% /appdata /dev/mapper/mpath1 파일시스템이 새로 생성되었다. 마운트 지점은 /appdata 이다.\n14. 자동 마운트 설정 # 서버 리부팅 후에도 스토리지 볼륨이 자동으로 마운트되도록 파일시스템 설정파일(/etc/fstab)에 등록한다.\n$ cat /etc/fstab LABEL=/ / ext3 defaults 1 1 LABEL=/usr /usr ext3 defaults 1 2 LABEL=/var /var ext3 defaults 1 2 LABEL=/home /home ext3 defaults 1 2 LABEL=/test /test ext3 defaults 1 2 LABEL=/boot /boot ext3 defaults 1 2 /dev/mapper/mpath1 /appdata ext3 defaults 0 0 tmpfs /dev/shm tmpfs defaults 0 0 devpts /dev/pts devpts gid=5,mode=620 0 0 sysfs /sys sysfs defaults 0 0 proc /proc proc defaults 0 0 LABEL=SWAP-sda3 swap swap defaults 0 0 15. 파일 생성/삭제 테스트 # $ dd if=/dev/zero of=/appdata/write_test.tmp bs=1G count=10 10+0 records in 10+0 records out 10737418240 bytes (11 GB) copied, 15.7843 seconds, 680 MB/s dd 명령어를 이용해서 새롭게 만든 파일시스템 안에 10GB 크기의 write_test.tmp 파일을 생성(Write)해본다.\n$ ls -lh /appdata/ total 11G drwx------ 2 root root 16K Dec 13 18:51 lost+found -rw-r--r-- 1 root root 10G Dec 13 19:08 write_test.tmp 10GB의 write_test.tmp 파일이 생성되었다.\n$ rm /appdata/write_test.tmp rm: remove regular file `/appdata/write_test.tmp\u0026#39;? y 생성한 테스트 파일을 삭제한다.\n$ ls -lh /appdata/ total 16K drwx------ 2 root root 16K Dec 13 18:51 lost+found 파일 삭제도 문제 없이 진행된다.\n","date":"Dec 14, 2021","permalink":"/blog/implementing-storage-lun-on-linux/","section":"Blogs","summary":"# 개요 # 리눅스 서버에 할당한 스토리지 LUN을 multipath를 이용해 연결하는 작업을 설명한다.","title":"리눅스 서버와 스토리지 볼륨 연결"},{"content":"TL;DR # 마크다운 에디터 Typora가 유료화되었기 때문에 대안으로 mark text를 설치 후 사용한다.\n$ brew install --cask mark-text 개요 # 불만없이 잘 사용하던 마크다운 에디터 Typora가 유료화 선언을 해버렸다.\nTypora 라이센스 등록 화면\n13일 남았다고 경고까지 해주는 무서운 모습.\n이번 기회에 Typora 대안으로 오픈소스 마크다운 에디터인 mark text를 골랐다. 이 포스팅 또한 mark down으로 작성중이다.\nmark text의 장점\n호환성 : Linux, Windows, MacOS 모두 지원한다. 오픈소스 : 무료 오픈소스(MIT License). mark text의 개발자는 Mark Text가 영원히 오픈소스로 남을거라고 README 파일에 적어놓았다. 편리함 : 깔끔하고 간결한 인터페이스, 불필요한 기능을 덜어냄. 총 6개의 기본 테마가 존재하는데 모두 이쁘고 깔끔함. 글쓰기에 집중할 수 있는 환경 환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 패키지 관리자 : Homebrew 3.3.7 설치할 패키지 : mark-text 0.16.3 본문 # 1. mark-text 설치 # 패키지 관리자인 Homebrew를 통해 mark text를 다운로드 받자.\n$ brew install --cask mark-text Running `brew update --preinstall`... ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; Updated Formulae Updated 1 formula. ==\u0026gt; Downloading https://github.com/marktext/marktext/releases/download/v0.16.3/m ==\u0026gt; Downloading from https://objects.githubusercontent.com/github-production-rel ######################################################################## 100.0% ==\u0026gt; Installing Cask mark-text ==\u0026gt; Moving App \u0026#39;Mark Text.app\u0026#39; to \u0026#39;/Applications/Mark Text.app\u0026#39; 🍺 mark-text was successfully installed! 2. 설치결과 확인 # homebrew # homebrew 명령어를 통해 mark text 패키지의 설치정보를 확인한다.\n$ brew info mark-text mark-text: 0.16.3 https://marktext.app/ /opt/homebrew/Caskroom/mark-text/0.16.3 (123B) From: https://github.com/Homebrew/homebrew-cask/blob/HEAD/Casks/mark-text.rb ==\u0026gt; Name Mark Text ==\u0026gt; Description None ==\u0026gt; Artifacts Mark Text.app (App) ==\u0026gt; Analytics install: 773 (30 days), 1,104 (90 days), 2,594 (365 days) 확인한 패키지 정보\n버전 : 0.16.3 설치경로 : /opt/homebrew/Caskroom/mark-text/0.16.3 런치패드 확인 # 설치가 완료 약 10초 후 런치패드에 Mark Text 아이콘이 새로 생성된다.\n3. 설정 # 마크다운 파일 작성시 수정사항을 실시간 반영하기 위한 자동 저장 설정을 활성화 해보자.\n환경설정\n좌측 상단의 Mark Text \u0026gt; 환경설정(Preferences) 클릭\n문서 자동저장 설정\nGeneral \u0026gt; Automatically save document changes \u0026gt; On 으로 토글값을 변경한다.\n기본값은 Off이다.\n결론 # Mark Text는 오픈소스 마크다운 에디터 치고 괜찮은 편이지만, 아직 1일차라 더 지켜볼 필요가 있다.\n문서 자동저장 설정을 해도 반영이 살짝 느린 단점 그리고 한글 입력시 밀림 관련 버그가 발생하고 있는 점은 아쉽다.\n그래도 Mark Text는 오픈소스(MIT License)이며 지속적으로 업데이트될 예정이라 미래가 밝다. 가볍게 사용하면서 릴리즈, 패치 등의 진행사항을 계속 모니터링할 예정이다.\n","date":"Dec 8, 2021","permalink":"/blog/review-of-marktext/","section":"Blogs","summary":"TL;DR # 마크다운 에디터 Typora가 유료화되었기 때문에 대안으로 mark text를 설치 후 사용한다.","title":"마크다운 에디터 mark text"},{"content":"TL;DR # 확인방법 1\n# HBA 카드 정보 확인 $ lspci -nn | grep -i hba # HBA 포트 정보 확인 $ ls -l /sys/class/fc_host # HBA Port 상태확인 $ more /sys/class/fc_host/host?/port_state # WWN 정보 확인 $ more /sys/class/fc_host/host?/port_name 확인방법 2\n# systool 명령어 관련 패키지 설치 $ yum install sysfsutils # HBA 구성정보 확인 $ systool -c fc_host # HBA Port 상태확인 $ systool -c fc_host -v | grep port_state # WWN 정보 확인 $ systool -c fc_host -v | grep port_name 개요 # 리눅스 OS 환경에서 HBA 카드 정보, HBA 포트 구성과 WWN(World Wide Name) 정보를 수집할 수 있다.\n환경 # OS : Red Hat Enterprise Linux Server release 5.x (Tikanga) Architecture : x86_64 Shell : bash 필요한 패키지 : sysfsutils-2.1.0-1.el5 본론 # 방법1. /sys/class 이용 # HBA 카드 목록 확인 # 서버에 장착된 HBA 카드 정보를 확인한다.\n$ lspci -nn | grep -i hba 40:00.0 Fibre Channel [0c04]: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA [1077:2532] (rev 02) 40:00.1 Fibre Channel [0c04]: QLogic Corp. ISP2532-based 8Gb Fibre Channel to PCI Express HBA [1077:2532] (rev 02) 명령어 옵션\n-nn : PCI ID를 같이 출력한다. PCI ID는 제조사 ID(Vendor ID)와 장치 ID(Device ID)를 조합한 값이다.\nVendor ID : Device ID 1077 : 2532 ---- ---- | +---\u0026gt; ISP2532-based 8Gb Fibre Channel to PCI Express HBA +-----------\u0026gt; QLogic Corp. 40:00 : number는 1개의 HBA 카드를 의미한다.\n40:00.0, 40:00.1 : 1개 HBA 카드의 HBA 포트 2개를 의미한다.\n현재구성은 1개의 HBA 카드(QLogic Corp. ISP2532-based 8Gb)에 2개의 포트로 구성된 상태이다.\nQLogic Corp. ISP2532-based (출처 : Alibaba.com) HBA 포트목록 확인 # $ ls -l /sys/class/fc_host total 0 drwxr-xr-x 3 root root 0 Dec 7 10:42 host7 drwxr-xr-x 3 root root 0 Dec 7 10:45 host8 2개의 HBA 포트(host7, host8)를 사용중이다.\nHBA 포트상태 확인 # $ more /sys/class/fc_host/host?/port_state :::::::::::::: /sys/class/fc_host/host7/port_state :::::::::::::: Online :::::::::::::: /sys/class/fc_host/host8/port_state :::::::::::::: Online 각 포트의 상태값은 Online 또는 Offline으로 결정된다.\n현재 서버의 HBA 포트(host7, host8) 2개 전부 사용중인 상태(Online)로 확인된다.\nWWN 확인 # WWN\nWWN은 World Wide Name의 줄임말이다. SAN(Storage Area Network)에서 FC(Fibre Channel) 포트를 인식하기 위한 고유한 ID 주소 값을 의미한다. WWN은 FC 포트마다 각각 부여된다.\nLAN 카드의 고유한 주소인 MAC Address나 휴대전화의 IMEI와 마찬가지로 동일한 WWN은 존재할 수 없다.\n$ more /sys/class/fc_host/host?/port_name :::::::::::::: /sys/class/fc_host/host7/port_name :::::::::::::: 0x21000024ff4cf9c6 :::::::::::::: /sys/class/fc_host/host8/port_name :::::::::::::: 0x21000024ff4cf9c7 WWN 확인결과\nhost7 포트의 WWN : 21:00:00:24:ff:4c:f9:c6 host8 포트의 WWN : 21:00:00:24:ff:4c:f9:c7 방법2. systool 이용 # systool 명령어는 sysfsutils 패키지에 포함되어 있다. systool 명령어를 사용하기 위해서는 sysfsutils 패키지를 먼저 설치해야한다.\nsysfsutils 패키지 설치 # yum 패키지 관리자를 통해서 최신 버전의 sysfsutils 패키지를 설치한다.\n$ yum install sysfsutils $ which systool /usr/bin/systool systool 명령어를 사용 가능한 상태이며, systool 명령어 파일의 절대경로는 /usr/bin/systool 이다.\n$ rpm -ql sysfsutils /usr/bin/get_module /usr/bin/systool /usr/share/doc/sysfsutils-2.1.0 /usr/share/doc/sysfsutils-2.1.0/AUTHORS /usr/share/doc/sysfsutils-2.1.0/COPYING /usr/share/doc/sysfsutils-2.1.0/CREDITS /usr/share/doc/sysfsutils-2.1.0/ChangeLog /usr/share/doc/sysfsutils-2.1.0/GPL /usr/share/doc/sysfsutils-2.1.0/NEWS /usr/share/doc/sysfsutils-2.1.0/README /usr/share/doc/sysfsutils-2.1.0/libsysfs.txt /usr/share/man/man1/systool.1.gz RPM(Redhat Package Manager) 명령어로 확인해본 결과 systool 명령어는 sysfsutils 패키지에 포함되어 있다.\n명령어 옵션\n-ql : 설치된 패키지를 구성하는 파일들의 절대 경로를 확인\nHBA 포트 목록확인 # 현재 사용 가능한 HBA 포트 목록을 확인한다.\n$ systool -c fc_host Class = \u0026#34;fc_host\u0026#34; Class Device = \u0026#34;host7\u0026#34; Device = \u0026#34;host7\u0026#34; Class Device = \u0026#34;host8\u0026#34; Device = \u0026#34;host8\u0026#34; 2개의 HBA 포트(host7, host8)가 존재한다.\nHBA 포트 상태확인 # $ systool -c fc_host -v | grep port_state port_state = \u0026#34;Online\u0026#34; port_state = \u0026#34;Online\u0026#34; HBA 포트의 상태값은 Online 또는 Offline으로 결정된다.\n현재 서버의 2개 HBA 포트(host7, host8)는 모두 Online 상태로 확인된다.\nWWN 확인 # 각 HBA 포트의 고유한 WWN 주소를 확인한다.\n$ systool -c fc_host -v | grep port_name port_name = \u0026#34;0x21000024ff4cf9c6\u0026#34; port_name = \u0026#34;0x21000024ff4cf9c7\u0026#34; WWN 확인결과\nhost7 포트의 WWN : 21:00:00:24:ff:4c:f9:c6 host8 포트의 WWN : 21:00:00:24:ff:4c:f9:c7 ","date":"Dec 7, 2021","permalink":"/blog/identifying-hba-cards-ports-and-wwn-in-linux/","section":"Blogs","summary":"TL;DR # 확인방법 1","title":"linux WWN 확인"},{"content":"개요 # macOS 환경에서 kind를 설치한 후 간편하게 로컬 쿠버네티스 환경을 구축할 수 있다.\nkind\nkind는 Kubernetes IN Docker의 줄임말로 도커 컨테이너를 이용해 손쉽게 로컬 환경에서 kubernetes 클러스터를 구축할 수 있는 소프트웨어이다. 비슷한 소프트웨어로는 minikube, k3s 등이 있다. 로컬 환경에서 쿠버네티스 관련 개발하거나 쿠버네티스 클러스터 실습이 필요할 경우 kind를 이용하면 간편하고 빠르게 확인할 수 있다.\n아키텍쳐\nkind의 아키텍처는 다음과 같다.\nkind가 생성한 1대의 쿠버네티스 노드는 알고보면 1개의 도커 컨테이너다.\n출처 : https://kind.sigs.k8s.io/docs/design/initial/\n환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 패키지 관리자 : Homebrew 3.3.6 docker desktop 4.3.0 (Docker version 20.10.11, build dea9396) : 쿠버네티스 기능 활성화, 패키지 관리자 Homebrew를 이용한 설치 kind v0.11.1 : 패키지 관리자 Homebrew를 이용한 설치 kubectl v1.22.4 : 패키지 관리자 Homebrew를 이용한 설치 전제조건 # macOS용 패키지 관리자인 brew가 미리 설치되어 있어야 한다.\n본론 # 단일 클러스터 # 1. docker 설치 # kind로 쿠버네티스 클러스터를 생성하려면 먼저 docker desktop이 설치되어 있어야 한다.\nbrew로 docker desktop을 설치한다.\n$ brew install --cask docker Running `brew update --preinstall`... ==\u0026gt; Auto-updated Homebrew! Updated 2 taps (homebrew/core and homebrew/cask). ==\u0026gt; Updated Formulae Updated 48 formulae. ==\u0026gt; Updated Casks Updated 21 casks. ==\u0026gt; Downloading https://desktop.docker.com/mac/main/arm64/71786/Docker.dmg ######################################################################## 100.0% ==\u0026gt; Installing Cask docker ==\u0026gt; Moving App \u0026#39;Docker.app\u0026#39; to \u0026#39;/Applications/Docker.app\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker-compose.bash-completion\u0026#39; to \u0026#39;/opt/homebrew/etc/bash_completion.d/docker-compose\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker.zsh-completion\u0026#39; to \u0026#39;/opt/homebrew/share/zsh/site-functions/_docker\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker.fish-completion\u0026#39; to \u0026#39;/opt/homebrew/share/fish/vendor_completions.d/docker.fish\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker-compose.fish-completion\u0026#39; to \u0026#39;/opt/homebrew/share/fish/vendor_completions.d/docker-compose.f ==\u0026gt; Linking Binary \u0026#39;docker-compose.zsh-completion\u0026#39; to \u0026#39;/opt/homebrew/share/zsh/site-functions/_docker_compose\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker.bash-completion\u0026#39; to \u0026#39;/opt/homebrew/etc/bash_completion.d/docker\u0026#39; 🍺 docker was successfully installed! 패키지 관리자인 Homebrew로 도커를 설치할 때 --cask 옵션을 줘서 GUI 환경에서 docker를 설치한다.\n$ brew info docker Warning: Treating docker as a formula. For the cask, use homebrew/cask/docker docker: stable 20.10.11 (bottled), HEAD Pack, ship and run any application as a lightweight container https://www.docker.com/ Conflicts with: docker-completion (because docker already includes these completion scripts) Not installed From: https://github.com/Homebrew/homebrew-core/blob/HEAD/Formula/docker.rb License: Apache-2.0 ==\u0026gt; Dependencies Build: go ✘, go-md2man ✘ ==\u0026gt; Options --HEAD Install HEAD version ==\u0026gt; Analytics install: 52,619 (30 days), 154,260 (90 days), 509,552 (365 days) install-on-request: 52,245 (30 days), 153,216 (90 days), 503,828 (365 days) build-error: 4 (30 days) docker 안정화 버전(stable) 20.10.11이 설치된 상태이다.\n런치패드에서 확인해본 결과 도커 아이콘이 새로 생성되었다.\n2. kind 설치 # 설치 # kind도 docker desktop과 동일하게 brew를 이용해 최신버전을 설치한다.\n$ brew install kind Running `brew update --preinstall`... ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; Updated Formulae Updated 2 formulae. ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/kind/manifests/0.11.1 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/kind/blobs/sha256:836dda92f4ab17324edd3ebc8614fb84a55923388df87dc2be4 ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:836dda92f4ab17324edd3ebc8614fb84a ######################################################################## 100.0% ==\u0026gt; Pouring kind--0.11.1.arm64_monterey.bottle.tar.gz ==\u0026gt; Caveats zsh completions have been installed to: /opt/homebrew/share/zsh/site-functions ==\u0026gt; Summary 🍺 /opt/homebrew/Cellar/kind/0.11.1: 8 files, 8.4MB ==\u0026gt; Running `brew cleanup kind`... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`) 버전 확인 # kind를 설치 완료한 후 버전을 확인해본다.\n$ kind version kind v0.11.1 go1.17.2 darwin/arm64 3. kubectl 설치 # $ brew install kubectl homebrew 패키지 관리자를 이용해 kubectl을 설치한다.\n$ kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;22\u0026#34;, GitVersion:\u0026#34;v1.22.4\u0026#34;, GitCommit:\u0026#34;b695d79d4f967c403a96986f1750a35eb75e75f1\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-11-17T15:41:42Z\u0026#34;, GoVersion:\u0026#34;go1.16.10\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/arm64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.1\u0026#34;, GitCommit:\u0026#34;5e58841cce77d4bc13713ad2b91fa0d961e69192\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-05-21T23:06:30Z\u0026#34;, GoVersion:\u0026#34;go1.16.4\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/arm64\u0026#34;} kubectl 명령어의 동작 상태를 확인하기 위해 version을 체크해본다.\n4. cluster 생성 # $ kind create cluster Creating cluster \u0026#34;kind\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.21.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community 🙂 kind 라는 이름의 Kubernetes cluster가 생성됐다.\ncluster를 생성할 때 이름을 따로 지정해주지 않으면 기본값으로 kind라는 이름으로 생성된다.\n생성할 클러스터의 이름을 정할 수도 있다.\n$ kind create cluster --name kind-2 Creating cluster \u0026#34;kind-2\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.21.1) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 Set kubectl context to \u0026#34;kind-kind-2\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-kind-2 Have a nice day! 👋 이번에는 kind-2 라는 이름으로 지정해서 클러스터를 생성해본다.\n5. cluster 목록 확인 # $ kind get clusters kind kind-2 처음에 생성한 kind 클러스터, 이후 생성한 kind-2 클러스터가 존재한다.\n6. 조작할 cluster 변경 # $ kubectl cluster-info --context kind-kind-2 Kubernetes control plane is running at https://127.0.0.1:61332 CoreDNS is running at https://127.0.0.1:61332/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. kind-kind-2 클러스터를 조작하기 위해서는 context를 kind-kind-2로 변경한다.\n이제 현재 context 상태를 확인해보자.\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop kind-kind kind-kind kind-kind * kind-kind-2 kind-kind-2 kind-kind-2 현재 context 값이 kind-kind-2로 변경되었다.\n7. 클러스터 상태 확인 # node 상태 확인 # $ kubectl get node NAME STATUS ROLES AGE VERSION kind-2-control-plane Ready control-plane,master 64s v1.21.1 1대의 control-plane(kind-2-control-plane) 노드가 생성되었다.\n현재 이 노드의 상태는 정상 동작중(Ready)이다.\npod 상태 확인 # $ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-558bd4d5db-4v54r 1/1 Running 0 76s kube-system coredns-558bd4d5db-6dsqk 1/1 Running 0 76s kube-system etcd-kind-2-control-plane 1/1 Running 0 88s kube-system kindnet-cwbkv 1/1 Running 0 77s kube-system kube-apiserver-kind-2-control-plane 1/1 Running 0 88s kube-system kube-controller-manager-kind-2-control-plane 1/1 Running 0 88s kube-system kube-proxy-6nkmp 1/1 Running 0 77s kube-system kube-scheduler-kind-2-control-plane 1/1 Running 0 88s local-path-storage local-path-provisioner-547f784dff-266sv 1/1 Running 0 76s 쿠버네티스 운영을 위해 생성되는 기본 시스템 pod 들을 확인한다. 모든 pod가 정상적으로 동작중(Running)인 상태이다.\n8. 클러스터 삭제 # $ kind delete cluster Deleting cluster \u0026#34;kind\u0026#34; ... 클러스터 삭제시 --name \u0026lt;클러스터 이름\u0026gt; 옵션으로 클러스터 이름을 지정해주지 않으면, 기본 클러스터인 kind를 삭제하게 된다.\n$ kind get clusters kind-2 kind-2 클러스터는 삭제되지 않고 남아있다.\n$ kind delete cluster --name kind-2 Deleting cluster \u0026#34;kind-2\u0026#34; ... kind-2 클러스터도 지워준다.\n$ kind get clusters No kind clusters found. kind와 kind-2 클러스터를 모두 삭제했기 때문에 조회 결과는 없다.\n응용 # 멀티 클러스터 # 생성 # 멀티노드는 단일 노드가 아닌 여러 대의 노드가 구성된 클러스터 환경을 의미한다.\n멀티노드 운영을 위해서는 호스트 환경의 리소스(CPU, Memory)가 많이 필요하기 때문에 해당 실습 전에 미리 Docker deskop의 리소스 제한(Resources)을 풀도록 한다. 도커에 할당하는 메모리 용량은 최소 8GB를 권장한다.\nyaml 작성 # 쿠버네티스 클러스터 설정이 담긴 yaml 파일을 작성한다.\n$ cat kind-multi-node-clusters.yaml # three node (two workers) cluster config kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 nodes: - role: control-plane - role: worker - role: worker 클러스터 구성은 1대의 control-plane + 2대의 worker node 이다.\n클러스터 생성 # 작성한 클러스터 yaml 파일을 적용하여 클러스터를 생성한다.\nkind create cluster \\ --config kind-multi-node-clusters.yaml \\ --name multinode 미리 작성한 매니페스트 파일(.yaml)을 사용해서 클러스터를 생성할 때에는 --config \u0026lt;설정파일 이름\u0026gt; 옵션을 사용하면 된다.\n클러스터 생성이 완료되는 데에 약 1분 정도 소요된다.\n$ kind create cluster \\ --config kind-multi-node-clusters.yaml \\ --name multinode Creating cluster \u0026#34;multinode\u0026#34; ... ✓ Ensuring node image (kindest/node:v1.21.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜 Set kubectl context to \u0026#34;kind-multinode\u0026#34; You can now use your cluster with: kubectl cluster-info --context kind-multinode Thanks for using kind! 😊 multinode 클러스터 생성이 완료되면 자동적으로 context가 새로 생성된 클러스터로 변경된다.\n$ kubectl config current-context kind-multinode 현재 위치한 context는 kind-multinode이다.\n노드 확인 # $ kubectl get no NAME STATUS ROLES AGE VERSION multinode-control-plane Ready control-plane,master 4m32s v1.21.1 multinode-worker Ready \u0026lt;none\u0026gt; 4m2s v1.21.1 multinode-worker2 Ready \u0026lt;none\u0026gt; 4m1s v1.21.1 3대의 노드 상태가 모두 정상 동작중(Ready)이다.\nROLES 값에 control-plane,master가 있으면 Master Node다. ROLES 값이 \u0026lt;none\u0026gt;이면 Worker Node다. 물론 NAME에 붙은 control-plane과 worker로도 구분 가능하다. 정리 # 멀티노드 클러스터 실습이 끝났다면 클러스터 환경을 정리clean-up한다.\nkind 클러스터 전체 목록을 확인한다.\n$ kind get clusters kind multinode multinode 클러스터를 삭제한다.\n$ kind delete cluster --name multinode Deleting cluster \u0026#34;multinode\u0026#34; ... multinode 클러스터가 삭제되었다.\n$ kind get clusters kind 결론 # 멀티노드 실습을 마지막으로 kind의 설치 및 사용법에 대한 포스팅을 마치겠다.\nM1 호환성 문제\n2021년 12월 6일 기준으로 Apple SiliconM1이 탑재된 맥북은 아직 출시된 지 얼마 지나지 않아서 호환성이 박살난 상태로, Virtual Box 설치를 지원하지 않아 쿠버네티스 실습을 하는 데에 제한이 있다.\n만약 macOS에서 docker 컨테이너가 아닌 virtualbox 환경과 같은 가상화 클러스터로 구성하고 싶다면 macOS 전용 하이퍼바이저인 hyperkit을 활용하도록 하자.\n나도 여러 방면에서 kubernetes 구축 방법을 찾아보고 있지만 버그와 아직 막막한 M1의 호환성에 막혀 절망하고 있는 중이다.\nM1 호환성 문제는 시간이 차차 해결해줄 거라고 예상한다.\n참고자료 # kind 공식 Quick Start 가이드 문서 : https://kind.sigs.k8s.io/docs/user/quick-start/\n","date":"Dec 6, 2021","permalink":"/blog/k8s/kind-quickstart/","section":"Blogs","summary":"개요 # macOS 환경에서 kind를 설치한 후 간편하게 로컬 쿠버네티스 환경을 구축할 수 있다.","title":"kind 쿠버네티스 구축"},{"content":"","date":"Nov 27, 2021","permalink":"/tags/mongodb/","section":"Tags","summary":"","title":"mongodb"},{"content":"개요 # M1 칩셋이 장착된 맥북에서 Homebrew를 이용해 mongodb를 설치한다.\nmongodb를 설치하는 방법은 여러가지가 있다.\nmongodb 공식 사이트에서 제공하는 tar ball을 받아서 설치하는 방법도 있지만 복잡하다. macOS를 사용한다면 Homebrew로 설치하는게 간편하고 빠르다.\n환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 패키지 관리자 : Homebrew 3.3.5 MongoDB 5.0 : Homebrew를 통한 설치 전제조건 # Homebrew 설치가 완료된 macOS\n본문 # 설치 # 1. tap 확인 # homebrew에서 tap은 homebrew의 기본 레포지터리(Master Repository)에 포함되지 않은 다른 레포지터리를 의미한다. 레포지터리는 여러 패키지가 모여있는 저장소라고 보면 된다.\n현재 brew에 등록된 탭 목록을 확인해보자.\n$ brew tap homebrew/cask homebrew/core homebrew/cask, homebrew/core는 homebrew 설치시 기본적으로 제공되는 tap이다.\n2. mongodb tap 등록 # mongodb community edition을 다운로드 받기 위해 mongodb에서 공식적으로 운영하는 The MongoDB Homebrew tap을 등록한다.\n$ brew tap mongodb/brew ==\u0026gt; Tapping mongodb/brew Cloning into \u0026#39;/opt/homebrew/Library/Taps/mongodb/homebrew-brew\u0026#39;... remote: Enumerating objects: 794, done. remote: Counting objects: 100% (291/291), done. remote: Compressing objects: 100% (208/208), done. remote: Total 794 (delta 144), reused 139 (delta 80), pack-reused 503 Receiving objects: 100% (794/794), 173.59 KiB | 1.19 MiB/s, done. Resolving deltas: 100% (382/382), done. Tapped 14 formulae (30 files, 238.3KB). mongodb tap이 등록되었는지 확인한다.\n$ brew tap homebrew/cask homebrew/core mongodb/brew mongodb/brew라는 이름의 tap이 잘 등록되었다.\n3. mongodb 설치 # 등록한 mongodb tap에서 mongodb community edition을 다운로드 받는다.\n최신 버전의 mongodb 설치\n$ brew install mongodb-community 설치시 따로 버전을 표기하지 않으면 자동적으로 최신 버전(latest)의 mongodb community edition을 설치한다.\n특정 버전의 mongodb 설치\nmongodb-community edition 5.0 버전을 설치한다. 참고로 MongoDB 5.0 Community Edition은 macOS 10.14 버전부터 지원한다.\n$ brew install mongodb-community@5.0 Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 1 tap (homebrew/core). ==\u0026gt; Updated Formulae Updated 1 formula. ==\u0026gt; Downloading https://fastdl.mongodb.org/tools/db/mongodb-database-tools-macos ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/brotli/manifests/1.0.9 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/brotli/blobs/sha256:5e9bddd862b ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/c-ares/manifests/1.18.1 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/c-ares/blobs/sha256:7b1eacc9efb ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/icu4c/manifests/69.1 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/icu4c/blobs/sha256:3771949f1799 ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/macos-term-size/manifests/1.0.0 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/macos-term-size/blobs/sha256:f4 ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/node/14/manifests/14.18.1 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/node/14/blobs/sha256:92ea528d60 ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/mongosh/manifests/1.1.4 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/mongosh/blobs/sha256:b3ccc98848 ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://fastdl.mongodb.org/osx/mongodb-macos-x86_64-5.0.3.tgz ######################################################################## 100.0% ==\u0026gt; Installing mongodb-community from mongodb/brew ==\u0026gt; Installing dependencies for mongodb/brew/mongodb-community: mongodb-database-tools, brotli, c-ares, icu4c, macos-term-size, node@14 and mongosh ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: mongodb-database- 🍺 /opt/homebrew/Cellar/mongodb-database-tools/100.5.1: 13 files, 115.7MB, built in 3 seconds ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: brotli ==\u0026gt; Pouring brotli--1.0.9.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/brotli/1.0.9: 25 files, 2.3MB ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: c-ares ==\u0026gt; Pouring c-ares--1.18.1.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/c-ares/1.18.1: 87 files, 665.3KB ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: icu4c ==\u0026gt; Pouring icu4c--69.1.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/icu4c/69.1: 259 files, 73.3MB ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: macos-term-size ==\u0026gt; Pouring macos-term-size--1.0.0.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/macos-term-size/1.0.0: 5 files, 36.9KB ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: node@14 ==\u0026gt; Pouring node@14--14.18.1.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/node@14/14.18.1: 3,923 files, 52.6MB ==\u0026gt; Installing mongodb/brew/mongodb-community dependency: mongosh ==\u0026gt; Pouring mongosh--1.1.4.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/mongosh/1.1.4: 5,617 files, 32.5MB ==\u0026gt; Installing mongodb/brew/mongodb-community ==\u0026gt; Caveats To start mongodb/brew/mongodb-community now and restart at login: brew services start mongodb/brew/mongodb-community Or, if you don\u0026#39;t want/need a background service you can just run: mongod --config /opt/homebrew/etc/mongod.conf ==\u0026gt; Summary 🍺 /opt/homebrew/Cellar/mongodb-community/5.0.3: 11 files, 180.7MB, built in 2 seconds ==\u0026gt; Caveats ==\u0026gt; mongodb-community To start mongodb/brew/mongodb-community now and restart at login: brew services start mongodb/brew/mongodb-community Or, if you don\u0026#39;t want/need a background service you can just run: mongod --config /opt/homebrew/etc/mongod.conf mongodb 설치가 문제없이 끝났다.\nmongodb 경로안내 # M1 맥북 기준으로 중요파일, 디렉토리의 경로는 아래와 같다. Intel CPU가 장착된 맥북은 M1 맥북의 경로와 다르다.\n설정파일 : /opt/homebrew/etc/mongod.conf 로그 디렉토리 : /opt/homebrew/var/log/mongodb 데이터 디렉토리 : /opt/homebrew/var/mongodb 4. mongodb 실행 # $ brew services start mongodb-community@5.0 ==\u0026gt; Successfully started `mongodb-community` (label: homebrew.mxcl.mongodb-community) mongodb-community 서비스를 시작한다.\n$ brew services list Name Status User File emacs stopped mongodb-community started xxx ~/Library/LaunchAgents/homebrew.mxcl.mongodb-community.plist unbound stopped mongodb-community가 실행중이다.\n$ ps -ef | grep mongo | grep -v grep 501 43139 1 0 1:04AM ?? 0:02.63 /opt/homebrew/opt/mongodb-community/bin/mongod --config /opt/homebrew/etc/mongod.conf ps 명령어로도 mongodb용 서비스 데몬인 mongod가 실행되는 걸 확인할 수 있다.\nError: Unknown command: services 에러 발생시 해결방법 # 증상\n설치한 mongodb를 실행할 수 없다.\n$ brew services start mongodb-community@5.0 Error: Unknown command: services 원인\nbrew services 명령어를 찾지 못해서 발생하는 오류다. brew services는 숨겨진 명령어이기 때문에 homebrew를 설치한 후 따로 설정을 해줘야 사용이 가능하다.\n해결방법\ntap에 homebrew/services를 새로 추가한다.\n$ brew tap homebrew/services ==\u0026gt; Tapping homebrew/services Cloning into \u0026#39;/opt/homebrew/Library/Taps/homebrew/homebrew-services\u0026#39;... remote: Enumerating objects: 1535, done. remote: Counting objects: 100% (414/414), done. remote: Compressing objects: 100% (303/303), done. remote: Total 1535 (delta 171), reused 283 (delta 101), pack-reused 1121 Receiving objects: 100% (1535/1535), 448.79 KiB | 1.82 MiB/s, done. Resolving deltas: 100% (647/647), done. Tapped 1 command (38 files, 558.8KB). $ brew tap homebrew/cask homebrew/core homebrew/services mongodb/brew tap 목록에 homebrew/services가 새로 추가되었다.\n$ brew services Name Status User File emacs stopped mongodb-community stopped unbound stopped homebrew/services tap을 새로 추가한 후 brew services 명령어를 실행하면 오류는 해결된다.\n5. mongodb 접속 # mongodb에 접속하려면 mongosh명령어를 사용한다. mongodb는 기본적으로 TCP 27017 포트를 사용한다.\nmongosh의 예전버전 명령어인 mongo는 mongosh로 대체(Superseded)된 후 차기 버전에서 삭제될 예정이므로 가급적이면 사용하지 말자.\n$ mongosh Current Mongosh Log ID:\t61a1070d11290afddd3fa6d8 Connecting to:\tmongodb://127.0.0.1:27017/?directConnection=true\u0026amp;serverSelectionTimeoutMS=2000 Using MongoDB:\t5.0.3 Using Mongosh:\t1.1.4 For mongosh info see: https://docs.mongodb.com/mongodb-shell/ ------ The server generated these startup warnings when booting: 2021-11-27T01:04:58.779+09:00: Access control is not enabled for the database. Read and write access to data and configuration is unrestricted ------ Warning: Found ~/.mongorc.js, but not ~/.mongoshrc.js. ~/.mongorc.js will not be loaded. You may want to copy or rename ~/.mongorc.js to ~/.mongoshrc.js. test\u0026gt; mongodb에 제대로 접속했다.\nmongodb에 접속하면 기본적으로 test DB를 사용한다.\ntest\u0026gt; db test 이제 전체 DB 목록을 확인한다.\ntest\u0026gt; show dbs admin 41 kB config 111 kB local 73.7 kB test\u0026gt; use admin switched to db admin 사용중인 DB를 test에서 admin으로 변경해본다.\ntest\u0026gt; exit $ mongosh에서 빠져나와 macOS의 쉘로 돌아오고 싶다면 exit 명령어를 입력한다.\n$ mongotop 2021-11-27T01:16:06.812+0900\tconnected to: mongodb://localhost/ ns total read write 2021-11-27T01:16:07+09:00 admin.system.version 0ms 0ms 0ms config.system.sessions 0ms 0ms 0ms config.transactions 0ms 0ms 0ms local.system.replset 0ms 0ms 0ms ns total read write 2021-11-27T01:16:08+09:00 admin.system.version 0ms 0ms 0ms config.system.sessions 0ms 0ms 0ms config.transactions 0ms 0ms 0ms local.system.replset 0ms 0ms 0ms ns total read write 2021-11-27T01:16:09+09:00 admin.system.version 0ms 0ms 0ms config.system.sessions 0ms 0ms 0ms config.transactions 0ms 0ms 0ms local.system.replset 0ms 0ms 0ms mongotop은 리눅스 서버를 모니터링할 때 사용하는 top 명령어의 mongodb 버전이다. mongotop을 실행하면 동작중인 mongod와 연결된 후 DB 사용량 통계를 주기적으로 뽑아낸다.\n6. 실습환경 정리 # mongodb 실습이 끝난 후에는 반드시 Homebrew를 이용해서 mongodb 서비스를 종료해준다.\nmongodb 포트가 계속 열려있으면 보안에 문제가 될수도 있고 또한 개인 컴퓨터의 리소스 낭비를 막을 수 있다.\nmongodb 서비스 종료\n$ brew services stop mongodb-community@5.0 Stopping `mongodb-community`... (might take a while) ==\u0026gt; Successfully stopped `mongodb-community` (label: homebrew.mxcl.mongodb-community) mongodb 서비스 상태확인\n$ brew services list Name Status User File emacs stopped mongodb-community stopped unbound stopped mongodb-community 서비스가 중지된 상태(stopped)다.\n$ ps -ef | grep mongo | grep -v grep mongod도 중지된 것이 확인됐다. 끝!\n추가설정 # 외부접속 허용하기 # mongodb를 설치하면 기본값으로 설치한 로컬(127.0.0.1)에서만 접속이 가능하도록 설정되어 있다. 방화벽이 열려있어도 외부에서 들어올 수 없는 상태이므로, 외부에서 접속이 필요하다면 mongodb 설정파일(mongod.conf)을 열어서 접속 가능한 IP 설정값(bindIp)을 변경하도록 한다.\n설정파일 확인\nmongodb 설정파일의 이름은 mongodb.conf 이다.\nM1 mac 기준으로 설정파일의 디폴트 위치는 /opt/homebrew/etc/mongod.conf 이다.\n$ cat /opt/homebrew/etc/mongod.conf systemLog: destination: file path: /opt/homebrew/var/log/mongodb/mongo.log logAppend: true storage: dbPath: /opt/homebrew/var/mongodb net: bindIp: 127.0.0.1 mongodb 포트확인\n$ netstat -antp tcp | grep 27017 tcp4 0 0 127.0.0.1.27017 *.* LISTEN 4번째 칸에 위치한 Local Address 값이 127.0.0.1.27017 이다.\n참고사항 : macOS용 netstat은 Linux에서의 netstat과 명령어 옵션 체계가 살짝 다르다.\n설정파일 변경\nvi 편집기를 이용해서 bindIp 값을 127.0.0.1에서 0.0.0.0 으로 변경해준다.\n$ vi /opt/homebrew/etc/mongod.conf systemLog: destination: file path: /opt/homebrew/var/log/mongodb/mongo.log logAppend: true storage: dbPath: /opt/homebrew/var/mongodb net: bindIp: 0.0.0.0 서비스 재시작\n변경된 bindIp 설정을 적용하기 위해 homebrew 명령어로 mongodb를 재시작한다.\n$ brew services restart mongodb-community@5.0 Stopping `mongodb-community`... (might take a while) ==\u0026gt; Successfully stopped `mongodb-community` (label: homebrew.mxcl.mongodb-community) ==\u0026gt; Successfully started `mongodb-community` (label: homebrew.mxcl.mongodb-community) mongodb 포트 재확인\n$ netstat -antp tcp | grep 27017 tcp4 0 0 *.27017 *.* LISTEN 4번째 칸에 위치한 Local Address 값이 127.0.0.1.27017에서 *.27017로 변경된 걸 확인할 수 있다.\n이제 mongodb로 모든 IP가 접근할 수 있다는 걸 의미한다.\n결론 # 사실 설치, 구성보다 중요한건 운영이 지속 가능하도록 유지해주는 보안 설정이다.\nmongodb 보안 설정은 이 글에 설명하기엔 너무 방대하기 때문에 기본적인 설치 과정만 다루었다.\n자료를 조사하는 과정에서 개인 목적의 개발용 mongodb 서버가 랜섬웨어로 털리는 케이스가 은근히 많은 것 같다. 이 글을 통해 계속 상시 운영되는 개발용 mongodb를 설치했다면, 반드시 다른 보안관련 포스트들을 참고해서 관리자용 계정 생성, Security authorization enabled 등의 DB 보안 설정을 적용해 운영하도록 하자. 아무리 개인 학습용 mongodb일지라도 해커한테 털리면 귀찮아지니까. 끝!\n참고자료 # https://docs.mongodb.com/manual/tutorial/install-mongodb-on-os-x/ : MongoDB 공식페이지의 설치 가이드 문서\nhttps://github.com/mongodb/homebrew-brew : mongodb tap 공식페이지\nhttps://apple.stackexchange.com/questions/150300/need-help-using-homebrew-services-command : brew services 명령어 에러 발생 관련 해결방법\n","date":"Nov 27, 2021","permalink":"/blog/installing-mongodb-on-m1-mac/","section":"Blogs","summary":"개요 # M1 칩셋이 장착된 맥북에서 Homebrew를 이용해 mongodb를 설치한다.","title":"mongodb 설치"},{"content":"개요 # minikube 기반의 로컬 쿠버네티스 환경에서 prometheus와 grafana를 설치, 구축하는 방법을 설명합니다.\n환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 패키지 관리자 : brew v3.3.5 Kubernetes 환경 Docker Desktop v4.2.0 (70708) : Kubernetes 기능 활성화됨 (Enable Kubernetes) minikube v1.24.0 : brew를 이용하여 설치. 단일 노드 1개 생성. kubectl v1.22.4 : brew를 이용하여 설치 helm v3.7.1 : brew를 이용하여 설치 prometheus v2.31.1 : helm을 이용하여 설치 및 배포 grafana v8.2.5 : helm을 이용하여 설치 및 배포 본문 # 1. minikube, kubectl 설치 # macOS용 채키지 관리자인 brew를 이용해 minikube와 kubectl을 설치합니다.\n$ brew install minikube $ brew install kubectl 2. helm 3 설치 # helm은 쿠버네티스의 패키지 관리자(Kubernetes Package Manager)입니다. helm을 이용하면 prometheus와 grafana를 설치와 동시에 자동 구성할 수 있습니다.\nhelm 설치 # macOS용 패키지 관리자인 brew를 이용해 helm을 설치합니다. brew를 이용해 설치하는 이유는 패키지 관리가 편리하기 때문입니다.\n$ brew install helm Updating Homebrew... ==\u0026gt; Auto-updated Homebrew! Updated 2 taps (homebrew/core and homebrew/cask). [...] ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/helm/manifests/3.7.1 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/helm/blobs/sha256:8587566f16cef ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Pouring helm--3.7.1.arm64_monterey.bottle.tar.gz ==\u0026gt; Caveats zsh completions have been installed to: /opt/homebrew/share/zsh/site-functions ==\u0026gt; Summary 🍺 /opt/homebrew/Cellar/helm/3.7.1: 60 files, 51MB helm 3.7.1이 정상적으로 설치되었습니다. 60개의 파일이 포함되어 있고, helm의 용량은 51MB 입니다.\nhelm 설치 정보 확인 # helm의 안정화된 버전(Stable)인 3.7.1이 설치되었습니다.\n버전정보 아래에는 Kubernetes package manager라는 간단한 설명이 적혀있습니다.\n$ brew info helm helm: stable 3.7.1 (bottled), HEAD Kubernetes package manager https://helm.sh/ /opt/homebrew/Cellar/helm/3.7.1 (60 files, 51MB) * Poured from bottle on 2021-11-25 at 20:48:43 From: https://githubcom/Homebrew/homebrew-core/blob/HEAD/Formula/helm.rb License: Apache-2.0 ==\u0026gt; Dependencies Build: go ✘ ==\u0026gt; Options --HEAD Install HEAD version ==\u0026gt; Caveats zsh completions have been installed to: /opt/homebrew/share/zsh/site-functions ==\u0026gt; Analytics install: 35,802 (30 days), 121,935 (90 days), 487,738 (365 days) install-on-request: 35,054 (30 days), 119,382 (90 days), 477,316 (365 days) build-error: 17 (30 days) helm 버전 확인 # helm 명령어가 잘 실행되는 지 확인합니다.\n$ helm version version.BuildInfo{Version:\u0026#34;v3.7.1\u0026#34;, GitCommit:\u0026#34;1d11fcb5d3f3bf00dbe6fe31b8412839a96b3dc4\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.17.2\u0026#34;} 3. minikube 구성 # minikube를 이용해 단일 노드로 구성된 kubernetes 환경을 먼저 만들고, 그 1대의 노드 위에 prometheus와 grafana를 배포해서 서비스할 계획입니다.\n이번 시나리오는 2대 이상의 멀티 노드가 아니라 마스터 노드 1개만으로 구성됩니다.\nminikube 시작 # docker 환경을 이용해서 minikube 노드 1개를 생성합니다. 해당 노드에 리소스는 2코어(CPUs=2)에 메모리가 1988MB가 할당되었습니다.\n$ minikube start -p prom-demo 😄 [prom-demo] Darwin 12.0.1 (arm64) 의 minikube v1.24.0 ✨ 자동적으로 docker 드라이버가 선택되었습니다. 다른 드라이버 목록: virtualbox, ssh 👍 prom-demo 클러스터의 prom-demo 컨트롤 플레인 노드를 시작하는 중 🚜 베이스 이미지를 다운받는 중 ... 🔥 Creating docker container (CPUs=2, Memory=1988MB) ... 🐳 쿠버네티스 v1.22.3 을 Docker 20.10.8 런타임으로 설치하는 중 ▪ 인증서 및 키를 생성하는 중 ... ▪ 컨트롤 플레인이 부팅... ▪ RBAC 규칙을 구성하는 중 ... 🔎 Kubernetes 구성 요소를 확인... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 🌟 애드온 활성화 : storage-provisioner, default-storageclass 🏄 끝났습니다! kubectl이 \u0026#34;prom-demo\u0026#34; 클러스터와 \u0026#34;default\u0026#34; 네임스페이스를 기본적으로 사용하도록 구성되었습니다. -p 는 profile 옵션입니다. minikube에서는 profile을 통해 여러 개의 실습환경을 편하게 관리할 수 있습니다.\nprofile 목록 확인 # 제 경우는 예전에 이미 생성해놓은 mnlab profile이 존재합니다. 그 아래에 방금 생성한 prom-demo profile이 보입니다.\n$ minikube profile list |-----------|-----------|---------|--------------|------|---------|---------|-------| | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | |-----------|-----------|---------|--------------|------|---------|---------|-------| | mnlab | docker | docker | 192.168.49.2 | 8443 | v1.22.3 | Stopped | 4 | | prom-demo | docker | docker | 192.168.49.2 | 8443 | v1.22.3 | Running | 1 | |-----------|-----------|---------|--------------|------|---------|---------|-------| prom-demo profile은 현재 1대의 노드로 구성되어 있으며 동작중(Running)입니다.\nprofile 상태 확인 # $ minikube status -p prom-demo prom-demo type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured prom-demo 라는 이름의 마스터 노드(Control Plane) 1대만 존재하고, 잘 실행되고 있는 상태입니다.\n$ minikube node list -p prom-demo prom-demo\t192.168.49.2 4. prometheus 설치 및 구성 # $ kubectl create ns prometheus namespace/prometheus created prometheus라는 이름의 namespace를 새로 생성합니다. 명령어의 ns는 namespace의 약어입니다.\n$ kubectl config set-context --current --namespace prometheus Context \u0026#34;prom-demo\u0026#34; modified. namespace 환경을 새로 생성한 prometheus로 변경해줍니다. 이제 prometheus namespace 위에 prometheus와 grafana를 설치할 것입니다.\n$ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE docker-desktop docker-desktop docker-desktop * prom-demo prom-demo prom-demo prometheus NAMESPACE 값이 prometheus로 변경된 걸 확인할 수 있습니다.\nrepo 등록 # $ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts \u0026#34;prometheus-community\u0026#34; has been added to your repositories prometheus-community라는 이름으로 repo(repository)를 등록합니다.\nrepo 확인 # $ helm repo list NAME URL prometheus-community\thttps://prometheus-community.github.io/helm-charts prometheus-community repo가 새롭게 생성되었습니다.\nprometheus 설치 # $ helm install prometheus prometheus-community/prometheus NAME: prometheus LAST DEPLOYED: Thu Nov 25 21:30:28 2021 NAMESPACE: prometheus STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The Prometheus server can be accessed via port 80 on the following DNS name from within your cluster: prometheus-server.prometheus.svc.cluster.local Get the Prometheus server URL by running these commands in the same shell: export POD_NAME=$(kubectl get pods --namespace prometheus -l \u0026#34;app=prometheus,component=server\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace prometheus port-forward $POD_NAME 9090 The Prometheus alertmanager can be accessed via port 80 on the following DNS name from within your cluster: prometheus-alertmanager.prometheus.svc.cluster.local Get the Alertmanager URL by running these commands in the same shell: export POD_NAME=$(kubectl get pods --namespace prometheus -l \u0026#34;app=prometheus,component=alertmanager\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace prometheus port-forward $POD_NAME 9093 ################################################################################# ###### WARNING: Pod Security Policy has been moved to a global property. ##### ###### use .Values.podSecurityPolicy.enabled with pod-based ##### ###### annotations ##### ###### (e.g. .Values.nodeExporter.podSecurityPolicy.annotations) ##### ################################################################################# The Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster: prometheus-pushgateway.prometheus.svc.cluster.local Get the PushGateway URL by running these commands in the same shell: export POD_NAME=$(kubectl get pods --namespace prometheus -l \u0026#34;app=prometheus,component=pushgateway\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace prometheus port-forward $POD_NAME 9091 For more information on running Prometheus, visit: https://prometheus.io/ helm을 이용해 prometheus를 설치합니다. helm으로 prometheus를 설치하면 prometheus는 pod 형태로 쿠버네티스 노드 위에 배포됩니다.\npod 배포 확인 # prometheus 설치가 완료되면서 prometheus와 관련된 pod 5개가 배포된 걸 확인할 수 있습니다.\n현재 prometheus가 배포된 namespace 위치는 저희가 아까 새로 생성한 prometheus입니다.\n$ kubectl get po -n prometheus NAME READY STATUS RESTARTS AGE prometheus-alertmanager-74674b7775-gjhdm 2/2 Running 0 136m prometheus-kube-state-metrics-58c5cd6ddb-2vjp5 1/1 Running 0 136m prometheus-node-exporter-72m9k 1/1 Running 0 136m prometheus-pushgateway-88fd4899d-xqtzr 1/1 Running 0 136m prometheus-server-5d455cb759-62f44 2/2 Running 0 136m prometheus pod 별 역할 설명\nalertmanager : alertmanager는 prometheus로부터 전달받은 경보(Alert)를 Email, Slack, Pagerduty 등 여러 방법을 이용해 관리자에게 보내는 역할을 합니다. kube-state-metrics : Kubernetes Cluster 내부의 자원(CPU, 메모리, 디스크 및 각 컨테이너가 사용하고 있는 리소스 현황, 네트워크 I/O, 정상 컨테이너, 비정상 컨테이너 개수 등)에 대한 매트릭을 수집해주는 exporter입니다. node-exporter : 서버 노드의 자원에 대한 매트릭을 수집해주는 exporter입니다. pushgateway : 매트릭을 푸시할 수 있는 중간 서비스입니다. server : Prometheus WEB UI를 띄울 수 있는 서버입니다. port-forward # prometheus-server pod의 이름을 얻어서 POD_NAME이라는 변수에 저장하는 절차입니다.\n$ export POD_NAME=$(kubectl get pods --namespace prometheus -l \u0026#34;app=prometheus,component=server\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) $ echo $POD_NAME prometheus-server-5d455cb759-62f44 POD_NAME 변수를 저장하는 명령어는 helm을 이용해 prometheus를 설치할 때 NOTES 부분에 이미 적혀있습니다.\n외부에서 9090 포트로 접속이 들어올 경우, prometheus-server(prometheus-server-5d455cb759-62f44) pod의 9090 포트로 연결해주도록 port-forward 설정합니다.\n$ kubectl --namespace prometheus port-forward $POD_NAME 9090 Forwarding from 127.0.0.1:9090 -\u0026gt; 9090 Forwarding from [::1]:9090 -\u0026gt; 9090 Handling connection for 9090 Handling connection for 9090 접속 테스트 # 웹 브라우저를 열고 Prometheus 웹페이지 주소인 http://localhost:9090으로 접속합니다.\nPrometheus 웹이 잘 실행됩니다. 이제 Prometheus가 수집한 모니터링 데이터를 검색해서 테스트해봅시다.\n수집 데이터 확인 # prometheus가 쿠버네티스 노드로부터 데이터를 잘 가져왔는지 점검하는 단계입니다. 검색창(돋보기 아이콘)에 kube_node_info를 입력하고, Execute 버튼을 누릅니다.\n1대 생성된 노드의 전체 정보가 나옵니다.\n노드 관련 정보\ncontainer_runtime_version : docker://20.10.8 internal_ip : 192.168.49.2 os_image : Ubuntu 20.04.2 LTS node : prom-demo 이제 kubectl 명령어로 node 정보를 출력해서 실제로 prometheus에서 수집된 정보와 비교해봅니다. 명령어의 -o wide 옵션은 결과를 더 자세히 출력합니다. no는 node의 약자입니다.\n$ kubectl get no -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME prom-demo Ready control-plane,master 50m v1.22.3 192.168.49.2 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.10.47-linuxkit docker://20.10.8 prometheus 웹의 결과와 명령어 결과를 비교해보니 틀린 내용은 없습니다. prometheus가 쿠버네티스 노드에게서 데이터를 정상적으로 수집해왔습니다.\n5. grafana 설치 및 구성 # grafana는 prometheus가 보내주는 시계열 매트릭 데이터를 시각화하는데 가장 최적화된 오픈소스 대시보드 툴입니다.\nrepo 등록 # helm을 통해 grafana를 다운로드 받기 위해 먼저 repo를 등록합니다.\n$ helm repo add grafana https://grafana.github.io/helm-charts \u0026#34;grafana\u0026#34; has been added to your repositories repo 확인 # $ helm repo list NAME URL prometheus-community\thttps://prometheus-community.github.io/helm-charts grafana https://grafana.github.io/helm-charts grafana repo가 새롭게 등록되었습니다.\n설치 # helm을 이용해 grafana를 설치합니다.\n$ helm install grafana grafana/grafana W1125 22:23:14.691555 19170 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W1125 22:23:14.693706 19170 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W1125 22:23:14.773504 19170 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ W1125 22:23:14.773573 19170 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+ NAME: grafana LAST DEPLOYED: Thu Nov 25 22:23:14 2021 NAMESPACE: prometheus STATUS: deployed REVISION: 1 NOTES: 1. Get your \u0026#39;admin\u0026#39; user password by running: kubectl get secret --namespace prometheus grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo 2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster: grafana.prometheus.svc.cluster.local Get the Grafana URL to visit by running these commands in the same shell: export POD_NAME=$(kubectl get pods --namespace prometheus -l \u0026#34;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) kubectl --namespace prometheus port-forward $POD_NAME 3000 3. Login with the password from step 1 and the username: admin ################################################################################# ###### WARNING: Persistence is disabled!!! You will lose your data when ##### ###### the Grafana pod is terminated. ##### ################################################################################# helm을 통해 grafana를 설치하면 prometheus와 동일하게 쿠버네티스 노드 위에 pod 형태로 배포됩니다.\nNOTES: 부분에는 grafana 설정 가이드가 나와 있습니다. 미리 숙지하면 편합니다.\ngrafana 배포상태 확인 # helm을 통해 grafana를 설치하면 node에 pod, service, deployment, replicaset이 알아서 배포됩니다.\n$ kubectl get all -l app.kubernetes.io/instance=grafana NAME READY STATUS RESTARTS AGE pod/grafana-59f986bdc-mbrcg 1/1 Running 0 99m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana ClusterIP 10.102.81.160 \u0026lt;none\u0026gt; 80/TCP 99m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1/1 1 1 99m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-59f986bdc 1 1 1 99m -l 옵션에 grafana 라벨이 붙은 객체들만 보이도록 제한을 걸면 헷갈리지 않고 grafana 관련 리소스, 오브젝트 목록만 확인할 수 있습니다.\nhelm에서도 배포상태를 확인합니다.\n$ helm list NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION grafana prometheus\t1 2021-11-25 22:23:14.520143 +0900 KST\tdeployed\tgrafana-6.17.7 8.2.5 prometheus\tprometheus\t1 2021-11-25 21:30:28.306396 +0900 KST\tdeployed\tprometheus-14.12.0\t2.31.1 grafana v8.2.5는 정상적으로 prometheus 네임스페이스에 배포된 상태(deployed)로 확인됩니다.\nadmin 계정의 패스워드 찾기 # 자동 생성된 grafana 웹페이지의 admin 계정의 패스워드를 찾는 절차입니다.\ngrafana 설치시 참고사항(NOTES:)에 적힌 명령어 1번부터 차례대로 실행합니다.\n$ kubectl get secret --namespace prometheus grafana -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 --decode ; echo kJYxA6F19BN3TAWcHwsOpqcCOGMJ9sXM1cxOiWxK admin 계정의 암호가 출력됩니다. 이 암호는 잠시후 grafana 웹페이지에 admin 계정으로 로그인할 때 사용되므로 잘 기록해둡니다.\nport-forward # 외부에서 3000번 포트로 접속이 들어올 경우 grafana pod로 연결해주도록 port-forward 설정을 해줍니다.\n$ export POD_NAME=$(kubectl get pods --namespace prometheus -l \u0026#34;app.kubernetes.io/name=grafana,app.kubernetes.io/instance=grafana\u0026#34; -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) $ echo $POD_NAME grafana-59f986bdc-mbrcg $ kubectl --namespace prometheus port-forward $POD_NAME 3000 Forwarding from 127.0.0.1:3000 -\u0026gt; 3000 Forwarding from [::1]:3000 -\u0026gt; 3000 로그인 # http://localhost:3000 으로 접속하면 Grafana 로그인 화면이 나옵니다.\n아까 찾은 admin 계정의 패스워드를 입력한 후 로그인합니다.\nGrafana 초기화면입니다.\n데이터 소스 연결 # Prometheus는 데이터 수집, Grafana는 데이터 시각화를 담당합니다. Grafana에서 Data source라는 용어는 prometheus, influxDB와 같은 시계열 메트릭 데이터를 담고 있는 대상 데이터베이스라고 생각하면 됩니다. Grafana는 반드시 Data source와 네트워크 연결이 되어야 데이터 시각화를 할 수 있습니다.\n이제 Grafana에 데이터 소스를 등록하는 단계입니다.\nConfiguration → Data sources → Add data source 클릭\nPrometheus → Select 버튼 클릭\n데이터 소스를 설정하는 화면입니다.\nURL 확인방법 # data source를 등록하기 위해 먼저 prometheus-server의 IP, Port 정보를 알아야 합니다.\nprometheus-server의 Endpoint 정보를 확인합니다. 명령어에서 ep는 endpoint의 약자입니다.\n$ kubectl get ep prometheus-server NAME ENDPOINTS AGE prometheus-server 172.17.0.6:9090 12h 제 기준에서 prometheus-server의 Endpoint IP는 172.17.0.6, 포트는 9090 입니다. Endpoint IP는 언제든 바뀔수 있기 때문에 실무 환경에서는 절대 이렇게 사용하지 않습니다.\n주의사항 : prometheus-server의 Endpoint IP 주소는 각자 다를 수 있기 때문에 제 주소를 그대로 적어서 넣으면 통신불가로 인한 Bad gateway 오류가 발생합니다. 반드시 kubectl get ep prometheus-server 명령어로 직접 확인후 입력 바랍니다.\n입력결과\nURL : 위에서 확인한 prometheus-server의 endpoint 정보를 입력해줍니다. Access : Server (default) Browser 방식은 곧 사용 중단(Deprecated)될 예정이기 때문에 가급적이면 사용하지 않는 걸 권고합니다. URL 값을 입력했다면 맨 아래에 위치한 Save \u0026amp; test 버튼을 눌러줍니다.\nGrafana가 Prometheus와 정상 연결되었을 경우 Data source is working 메세지가 출력됩니다.\n이제 Prometheus로 수집한 데이터를 시각화할 대시보드만 생성해주면 작업은 끝납니다.\ngrafana dashboard import # Grafana Labs 공식 사이트에서 제공하는 대시보드 레이아웃들 중에서 마음에 드는걸 골라서 Import로 불러올 수 있습니다.\nGrafana Labs 사이트에 접속해서 메인화면에 보이는 Node Exporter Full를 클릭합니다.\nNode Exporter Full 대시보드는 Node Exporter가 수집한 노드 관련 정보를 디테일하게 표시해주는 레이아웃의 대시보드입니다.\nNode Exporter Full의 Dashboard ID 값인 1860을 기억해둡니다. 잠시후 Node Exporter Full 대시보드를 불러오는 단계(Import)에서 해당 ID 값 입력이 필요합니다.\n\u0026lsquo;+\u0026rsquo; 아이콘(Create) → Import 클릭\nDashboard ID인 1860 입력 → 우측 Load 버튼 클릭\nPrometheus 선택 → Import\nGrafana 공식 사이트에 업로드된 Node Exporter Full 대시보드를 받아 환경에 생성한 결과입니다.\nCPU Basic, Memory Basic, Network Traffic Basic 등의 노드 데이터 수집도 정상적으로 진행되고 있습니다.\n결론 # Kubernetes 환경에서 Prometheus + Grafana는 이제 선택을 넘어서 엔지니어에게 필수로 요구되는 모니터링 기술 스택입니다.\nPrometheus는 모니터링 데이터를 수집하는 역할, Grafana는 Prometheus가 수집한 데이터를 관리자가 보기 좋게 시각화하는 역할을 담당합니다. 컨테이너 인프라 환경에서는 많은 종류의 소규모 기능이 각각 작은 단위로 나뉘어진 마이크로서비스 아키텍쳐로 개발 및 배포되기 때문에 반드시 중앙 모니터링이 필요합니다. 이때 효율적으로 모니터링할 수 있는 테크 스택 중 하나가 Prometheus와 Grafana의 조합입니다. Prometheus와 Grafana는 컨테이너 형태로 패키징되어 동작하며 최소한의 자원으로 쿠버네티스 클러스터의 상태를 시각적으로 표현합니다.\n기업의 규모 가릴 것 없이 모니터링 목적으로 사용하는 테크 스택이기 때문에 심도 있게 배워두면 취업 뿐만 아니라 커리어에도 큰 도움이 될 거라고 생각합니다.\n참고자료 # https://prometheus.io/docs/introduction/overview/ : Prometheus Architecture\nhttps://stackoverflow.com/questions/48338122/grafana-http-error-bad-gateway-and-templating-init-failed-errors : Bad gateway 네트워크 이슈 발생시 참고\n","date":"Nov 25, 2021","permalink":"/blog/k8s/installing-prometheus-and-grafana-with-minikube/","section":"Blogs","summary":"개요 # minikube 기반의 로컬 쿠버네티스 환경에서 prometheus와 grafana를 설치, 구축하는 방법을 설명합니다.","title":"prometheus와 grafana 구축"},{"content":"","date":"Nov 18, 2021","permalink":"/tags/hardware/","section":"Tags","summary":"","title":"hardware"},{"content":"개요 # LSI 사의 RAID Controller가 장착된 리눅스 서버에서 MegaCLI 명령어를 이용해 RAID 관련 정보를 확인할 수 있다.\nmegacli는 MegaRaid를 CLI 환경에서 조작할 수 있도록 지원하는 관리 소프트웨어이다.\n환경 # OS : Red Hat Enterprise Linux Server release 6.2 (Santiago) Architecture : x86_64 Shell : bash 설치할 패키지 : MegaCLI v8.07.14 (MegaCli-8.07.14-1.noarch.rpm) 본문 # MegaCLI 패키지 설치 # 1. RAID Controller 제원 확인 # 우선 해당 서버의 RAID Controller가 LSI Logic에서 만든 MegaRAID 제품이어야 MegaCli 패키지를 설치후 명령어를 이용 가능하다. 그러니 우선 RAID Controller의 제조사와 모델명부터 확인한다.\n확인방법 1. lspci # $ rpm -qa pciutils pciutils-3.1.4-11.el6.x86_64 lspci 명령어를 사용하기 위해서는 lspci 패키지 설치가 선행되어야 한다.\n$ lspci | grep -i raid 06:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS 2208 [Thunderbolt] (rev 03) RAID Controller 제조사(Vendor)는 LSI Logic, 모델명은 MegaRAID SAS 2208 [Thunderbolt] 라는 정보를 얻을 수 있다.\n확인방법 2. lshw # $ rpm -qa lshw lshw-2.17-1.el6.rf.x86_64 lshw 명령어를 사용하기 위해서는 lshw 패키지 설치가 선행되어야 한다. 해당 서버는 LSI Logic사에서 만든 RAID Controller 모델 MegaRAID SAS 2208 [Thunderbolt]를 사용하고 있다.\n$ lshw -c storage *-storage description: RAID bus controller product: MegaRAID SAS 2208 [Thunderbolt] vendor: LSI Logic / Symbios Logic physical id: 0 bus info: pci@0000:06:00.0 logical name: scsi0 version: 03 width: 64 bits clock: 33MHz capabilities: storage pm pciexpress vpd msi msix bus_master cap_list rom configuration: driver=megaraid_sas latency=0 resources: irq:40 ioport:d000(size=256) memory:fbd60000-fbd63fff memory:fbd00000-fbd3ffff memory:fbd40000-fbd5ffff(prefetchable) lshw 명령어 설명\n-c \u0026lt;class\u0026gt; : 특정 파트에 대한 상세 정보를 출력한다.\n2. 설치파일 다운로드 # Broadcom 다운로드 링크 접속\nBroadcom 사의 공식 다운로드 홈페이지를 접속한다.\nhttps://www.broadcom.com/support/download-search?dk=megacli\nManagement Software and Tools 클릭\nMegaCLI 5.5 P2 (Version 8.07.14) 클릭\n클릭시 팝업 페이지가 뜨며 zip 확장자의 설치파일 다운로드가 시작된다.\n3. MegaCLI 패키지 업로드 # SFTP, FTP를 이용해 서버에 MegaCLI 설치파일(8-07-14_MegaCLI.zip)을 업로드한다.\n$ ls 8-07-14_MegaCLI.zip $ ls -lh total 7.6M -rw-rw-r-- 1 devuser1 devuser1 7.6M Nov 18 09:15 8-07-14_MegaCLI.zip MegaCLI 설치파일의 용량은 7.6MB 이다.\n설치파일 zip을 압축해제한다.\n$ unzip 8-07-14_MegaCLI.zip Archive: 8-07-14_MegaCLI.zip inflating: 8.07.14_MegaCLI.txt inflating: DOS/MegaCLI.exe extracting: FreeBSD/MegaCLI.zip extracting: FreeBSD/MegaCli64.zip inflating: Linux/MegaCli-8.07.14-1.noarch.rpm inflating: Solaris/MegaCli.pkg inflating: Windows/MegaCli.exe inflating: Windows/MegaCli64.exe 압축 해제후 생성된 Linux 디렉토리로 이동한다.\n$ ls 8-07-14_MegaCLI.zip 8.07.14_MegaCLI.txt DOS FreeBSD Linux Solaris Windows $ cd Linux Linux용 패키지 설치파일 확인\n$ ls MegaCli-8.07.14-1.noarch.rpm 4. 패키지 설치 # $ rpm -ivh MegaCli-8.07.14-1.noarch.rpm Preparing... ########################################### [100%] 1:MegaCli ########################################### [100%] 패키지 설치파일명의 noarch는 특정 아키텍쳐를 의미하지 않을 때 붙이는 키워드이다. 아키텍쳐 종류로는 alpha, sparc, sparc64, i386, i586, i686, ppc64 등이 있다. megacli 패키지가 설치되는 절대 경로는 /opt/MegaRAID/MegaCli/ 이다.\n5. 명령어 심볼릭 링크 연결 # MegaCli64 명령어에는 대소문자와 숫자까지 섞여있다. MegaCLI64 명령어를 편하게 사용하기 위해 MegaCli64 명령어 파일을 megacli로 심볼릭 링크 연결한다.\n$ ln -s /opt/MegaRAID/MegaCli/MegaCli64 /usr/bin/megacli 이제부터는 MegaCli64 명령어가 아닌 megacli로 입력해서 사용하면 된다.\n$ which megacli /usr/bin/megacli 6. 명령어 동작 테스트 # -v 옵션은 megacli 버전을 출력한다.\n$ megacli -v MegaCLI SAS RAID Management Tool Ver 8.07.14 Dec 16, 2013 (c)Copyright 2013, LSI Corporation, All Rights Reserved. Exit Code: 0x00 MegaCli 명령어 # 1. megacli 명령어 메뉴얼 확인 # 자세한 명령어 메뉴얼은 megacli -h 명령어로 확인 가능하다.\n$ megacli -h | more MegaCLI SAS RAID Management Tool Ver 8.07.14 Dec 16, 2013 (c)Copyright 2013, LSI Corporation, All Rights Reserved. NOTE: The following options may be given at the end of any command below: [-Silent] [-AppLogFile filename] [-NoLog] [-page[N]] [-] is optional. N - Number of lines per page. MegaCli -v MegaCli -help|-h|? MegaCli -adpCount MegaCli -AdpSetProp {CacheFlushInterval -val} | { RebuildRate -val} | {PatrolReadRate -val} | {BgiRate -val} | {CCRate -val} | {ForceSGPIO -val} | {ReconRate -val} | {SpinupDriveCount -val} | {SpinupDelay -val} | {CoercionMode -val} | {ClusterEnable -val} | {PredFailPollInterval -val} | {BatWarnDsbl -val} | {EccBucketSize -val} | {EccBucketLeakRate -val} | {AbortCCOnError -val} | AlarmEnbl | AlarmDsbl | AlarmSilence [...] MegaCli XD -FetchSafeId -iN | -iALL MegaCli XD -ApplyActivationKey \u0026lt;key\u0026gt; -iN Exit Code: 0x00 2. 시스템 요약정보 확인 # 확인 가능한 주요정보\n시스템 기본정보 : 리눅스 커널 버전, 드라이버 버전, megacli 패키지 버전 정보 RAID Controller : 제조사, 모델명, 펌웨어 버전, 상태 RAID 배터리(BBU, Battery Backup Unit) : BBU는 갑작스러운 전원 중단이 발생하더라도 캐시의 내용이 지워지지 않도록 보존하는 역할을 한다. 인클로저(Enclosure) 물리적 디스크(PD, Physical Drive) 구성정보 논리적 디스크(VD, Virtual Drive) 구성정보 : 논리적 디스크는 RAID를 의미한다. $ megacli -ShowSummary -aALL System Operating System: Linux version 2.6.32-220.el6.x86_64 Driver Version: 00.00.05.40-rh2 CLI Version: 8.07.14 Hardware Controller ProductName : LSI MegaRAID SAS 9266-8i(Bus 0, Dev 0) SAS Address : 500605b0057b9570 FW Package Version: 23.9.0-0023 Status : Optimal BBU BBU Type : Status : Healthy Enclosure Product Id : SGPIO Type : SGPIO Status : OK PD Connector : Port 0 - 3\u0026lt;Internal\u0026gt;: Slot 1 Vendor Id : SEAGATE Product Id : ST9300653SS State : Online Disk Type : SAS,Hard Disk Device Capacity : 278.464 GB Power State : Active Connector : Port 0 - 3\u0026lt;Internal\u0026gt;: Slot 0 Vendor Id : SEAGATE Product Id : ST9300653SS State : Online Disk Type : SAS,Hard Disk Device Capacity : 278.464 GB Power State : Active Storage Virtual Drives Virtual drive : Target Id 0 ,VD name Size : 278.464 GB State : Optimal RAID Level : 1 Exit Code: 0x00 3. 물리적 디스크 정보 확인 # PD는 물리 디스크(Physical Drive)의 약자이다.\n## MegaCli64 명령어를 심볼릭 링크로 연결하지 않았을 경우 $ /opt/MegaRAID/MegaCli/MegaCli64 -PDList -aALL ## MegaCli64 명령어를 심볼릭 링크로 연결했을 경우 $ megacli -PDList -aALL Adapter #0 Enclosure Device ID: 252 Slot Number: 0 Drive\u0026#39;s position: DiskGroup: 0, Span: 0, Arm: 1 Enclosure position: N/A Device Id: 9 WWN: 5000C5005EC4558C Sequence Number: 2 Media Error Count: 0 Other Error Count: 0 Predictive Failure Count: 0 Last Predictive Failure Event Seq Number: 0 PD Type: SAS Raw Size: 279.396 GB [0x22ecb25c Sectors] Non Coerced Size: 278.896 GB [0x22dcb25c Sectors] Coerced Size: 278.464 GB [0x22cee000 Sectors] Sector Size: 512 Firmware state: Online, Spun Up Device Firmware Level: 0004 Shield Counter: 0 Successful diagnostics completion on : N/A SAS Address(0): 0x5000c5005ec4558d SAS Address(1): 0x0 Connected Port Number: 1(path0) Inquiry Data: SEAGATE ST9300653SS 00046XN2SNLZ FDE Capable: Not Capable FDE Enable: Disable Secured: Unsecured Locked: Unlocked Needs EKM Attention: No Foreign State: None Device Speed: 6.0Gb/s Link Speed: 6.0Gb/s Media Type: Hard Disk Device Drive: Not Certified Drive Temperature :29C (84.20 F) PI Eligibility: No Drive is formatted for PI information: No PI: No PI Port-0 : Port status: Active Port\u0026#39;s Linkspeed: 6.0Gb/s Port-1 : Port status: Active Port\u0026#39;s Linkspeed: 6.0Gb/s Drive has flagged a S.M.A.R.T alert : No Enclosure Device ID: 252 Slot Number: 1 Drive\u0026#39;s position: DiskGroup: 0, Span: 0, Arm: 0 Enclosure position: N/A Device Id: 8 WWN: 5000C5005EC45560 Sequence Number: 2 Media Error Count: 0 Other Error Count: 0 Predictive Failure Count: 0 Last Predictive Failure Event Seq Number: 0 PD Type: SAS Raw Size: 279.396 GB [0x22ecb25c Sectors] Non Coerced Size: 278.896 GB [0x22dcb25c Sectors] Coerced Size: 278.464 GB [0x22cee000 Sectors] Sector Size: 512 Firmware state: Online, Spun Up Device Firmware Level: 0004 Shield Counter: 0 Successful diagnostics completion on : N/A SAS Address(0): 0x5000c5005ec45561 SAS Address(1): 0x0 Connected Port Number: 0(path0) Inquiry Data: SEAGATE ST9300653SS 00046XN2SNMA FDE Capable: Not Capable FDE Enable: Disable Secured: Unsecured Locked: Unlocked Needs EKM Attention: No Foreign State: None Device Speed: 6.0Gb/s Link Speed: 6.0Gb/s Media Type: Hard Disk Device Drive: Not Certified Drive Temperature :28C (82.40 F) PI Eligibility: No Drive is formatted for PI information: No PI: No PI Port-0 : Port status: Active Port\u0026#39;s Linkspeed: 6.0Gb/s Port-1 : Port status: Active Port\u0026#39;s Linkspeed: 6.0Gb/s Drive has flagged a S.M.A.R.T alert : No Exit Code: 0x00 4. 물리 디스크 개별 에러 카운트 확인 # 디스크의 Error Count 값이 1 이상이면 디스크 교체를 권고한다.\nPredictive Failure Count 값이 급증하는 디스크의 경우는 며칠 이내에 디스크 폴트가 발생할 수 있으니 미리 예비 디스크 파트를 확보해둔다.\n$ megacli -PDList -aALL | grep Count Media Error Count: 0 Other Error Count: 0 Predictive Failure Count: 0 Shield Counter: 0 Media Error Count: 0 Other Error Count: 0 Predictive Failure Count: 0 Shield Counter: 0 5. 논리적 디스크 정보 확인 # $ megacli -LDInfo -Lall -aALL Adapter 0 -- Virtual Drive Information: Virtual Drive: 0 (Target Id: 0) Name : RAID Level : Primary-1, Secondary-0, RAID Level Qualifier-0 Size : 278.464 GB Sector Size : 512 Mirror Data : 278.464 GB State : Optimal Strip Size : 64 KB Number Of Drives : 2 Span Depth : 1 Default Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBU Current Cache Policy: WriteBack, ReadAhead, Direct, No Write Cache if Bad BBU Default Access Policy: Read/Write Current Access Policy: Read/Write Disk Cache Policy : Disk\u0026#39;s Default Encryption Type : None PI type: No PI Is VD Cached: No Exit Code: 0x00 6. 어댑터 정보확인 # 확인 가능한 주요정보\nRAID Controller 모델명 RAID Controller Serial 번호 RAID Controller 펌웨어 버전 정보 RAID Controller가 지원하는 RAID Level 정보 RAID Controller가 지원하는 드라이브 타입 : SAS, SATA $ megacli -AdpAllInfo -aALL Adapter #0 ============================================================================== Versions ================ Product Name : LSI MegaRAID SAS 9266-8i Serial No : SV23924725 FW Package Build: 23.9.0-0023 Mfg. Data ================ Mfg. Date : 09/29/12 Rework Date : 00/00/00 Revision No : 09B Battery FRU : N/A Image Versions in Flash: ================ BIOS Version : 5.38.00_4.12.05.00_0x05180000 WebBIOS Version : 6.1-49-e_49-Rel Preboot CLI Version: 05.05-03:#%00011 FW Version : 3.220.75-2196 NVDATA Version : 2.1209.03-0117 Boot Block Version : 2.05.00.00-0010 BOOT Version : 07.26.13.219 Pending Images in Flash ================ None PCI Info ================ Controller Id : 0000 Vendor Id : 1000 Device Id : 005b SubVendorId : 1000 SubDeviceId : 9266 Host Interface : PCIE ChipRevision : C1 Link Speed : 0 Number of Frontend Port: 0 Device Interface : PCIE Number of Backend Port: 8 Port : Address 0 5000c5005ec45561 1 5000c5005ec4558d 2 0000000000000000 3 0000000000000000 4 0000000000000000 5 0000000000000000 6 0000000000000000 7 0000000000000000 HW Configuration ================ SAS Address : 500605b0057b9570 BBU : Present Alarm : Present NVRAM : Present Serial Debugger : Present Memory : Present Flash : Present Memory Size : 1024MB TPM : Absent On board Expander: Absent Upgrade Key : Absent Temperature sensor for ROC : Present Temperature sensor for controller : Absent ROC temperature : 46 degree Celsius Settings ================ Current Time : 9:44:50 11/18, 2021 Predictive Fail Poll Interval : 300sec Interrupt Throttle Active Count : 16 Interrupt Throttle Completion : 50us Rebuild Rate : 30% PR Rate : 30% BGI Rate : 30% Check Consistency Rate : 30% Reconstruction Rate : 30% Cache Flush Interval : 4s Max Drives to Spinup at One Time : 2 Delay Among Spinup Groups : 12s Physical Drive Coercion Mode : 1GB Cluster Mode : Disabled Alarm : Enabled Auto Rebuild : Enabled Battery Warning : Enabled Ecc Bucket Size : 15 Ecc Bucket Leak Rate : 1440 Minutes Restore HotSpare on Insertion : Disabled Expose Enclosure Devices : Enabled Maintain PD Fail History : Disabled Host Request Reordering : Enabled Auto Detect BackPlane Enabled : SGPIO/i2c SEP Load Balance Mode : Auto Use FDE Only : Yes Security Key Assigned : No Security Key Failed : No Security Key Not Backedup : No Default LD PowerSave Policy : Automatic Maximum number of direct attached drives to spin up in 1 min : 10 Auto Enhanced Import : Yes Any Offline VD Cache Preserved : No Allow Boot with Preserved Cache : No Disable Online Controller Reset : No PFK in NVRAM : No Use disk activity for locate : No POST delay : 90 seconds BIOS Error Handling : Pause on Errors Current Boot Mode :Normal Capabilities ================ RAID Level Supported : RAID0, RAID1, RAID5, RAID6, RAID00, RAID10, RAID50, RAID60, PRL 11, PRL 11 with spanning, SRL 3 supported, PRL11-RLQ0 DDF layout with no span, PRL11-RLQ0 DDF layout with span Supported Drives : SAS, SATA Allowed Mixing: Mix in Enclosure Allowed Mix of SAS/SATA of HDD type in VD Allowed Mix of SAS/SATA of SSD type in VD Allowed Status ================ ECC Bucket Count : 0 Limitations ================ Max Arms Per VD : 32 Max Spans Per VD : 8 Max Arrays : 128 Max Number of VDs : 64 Max Parallel Commands : 1008 Max SGE Count : 60 Max Data Transfer Size : 8192 sectors Max Strips PerIO : 42 Max LD per array : 64 Min Strip Size : 8 KB Max Strip Size : 1.0 MB Max Configurable CacheCade Size: 0 GB Current Size of CacheCade : 0 GB Current Size of FW Cache : 873 MB Device Present ================ Virtual Drives : 1 Degraded : 0 Offline : 0 Physical Devices : 3 Disks : 2 Critical Disks : 0 Failed Disks : 0 Supported Adapter Operations ================ Rebuild Rate : Yes CC Rate : Yes BGI Rate : Yes Reconstruct Rate : Yes Patrol Read Rate : Yes Alarm Control : Yes Cluster Support : No BBU : Yes Spanning : Yes Dedicated Hot Spare : Yes Revertible Hot Spares : Yes Foreign Config Import : Yes Self Diagnostic : Yes Allow Mixed Redundancy on Array : No Global Hot Spares : Yes Deny SCSI Passthrough : No Deny SMP Passthrough : No Deny STP Passthrough : No Support Security : No Snapshot Enabled : No Support the OCE without adding drives : Yes Support PFK : Yes Support PI : Yes Support Boot Time PFK Change : No Disable Online PFK Change : No Support LDPI Type1 : No Support LDPI Type2 : No Support LDPI Type3 : No PFK TrailTime Remaining : 0 days 0 hours Support Shield State : Yes Block SSD Write Disk Cache Change: No Supported VD Operations ================ Read Policy : Yes Write Policy : Yes IO Policy : Yes Access Policy : Yes Disk Cache Policy : Yes Reconstruction : Yes Deny Locate : No Deny CC : No Allow Ctrl Encryption: No Enable LDBBM : No Support Breakmirror : No Power Savings : No Supported PD Operations ================ Force Online : Yes Force Offline : Yes Force Rebuild : Yes Deny Force Failed : No Deny Force Good/Bad : No Deny Missing Replace : No Deny Clear : No Deny Locate : No Support Temperature : Yes NCQ : No Disable Copyback : No Enable JBOD : No Enable Copyback on SMART : No Enable Copyback to SSD on SMART Error : Yes Enable SSD Patrol Read : No PR Correct Unconfigured Areas : Yes Enable Spin Down of UnConfigured Drives : Yes Disable Spin Down of hot spares : No Spin Down time : 30 T10 Power State : No Error Counters ================ Memory Correctable Errors : 0 Memory Uncorrectable Errors : 0 Cluster Information ================ Cluster Permitted : No Cluster Active : No Default Settings ================ Phy Polarity : 0 Phy PolaritySplit : 0 Background Rate : 30 Strip Size : 64kB Flush Time : 4 seconds Write Policy : WB Read Policy : Adaptive Cache When BBU Bad : Disabled Cached IO : No SMART Mode : Mode 6 Alarm Disable : Yes Coercion Mode : 1GB ZCR Config : Unknown Dirty LED Shows Drive Activity : No BIOS Continue on Error : 1 Spin Down Mode : Internal Only Allowed Device Type : SAS/SATA Mix Allow Mix in Enclosure : Yes Allow HDD SAS/SATA Mix in VD : Yes Allow SSD SAS/SATA Mix in VD : Yes Allow HDD/SSD Mix in VD : No Allow SATA in Cluster : No Max Chained Enclosures : 16 Disable Ctrl-R : Yes Enable Web BIOS : Yes Direct PD Mapping : No BIOS Enumerate VDs : Yes Restore Hot Spare on Insertion : No Expose Enclosure Devices : Yes Maintain PD Fail History : No Disable Puncturing : No Zero Based Enclosure Enumeration : No PreBoot CLI Enabled : Yes LED Show Drive Activity : No Cluster Disable : Yes SAS Disable : No Auto Detect BackPlane Enable : SGPIO/i2c SEP Use FDE Only : Yes Enable Led Header : No Delay during POST : 0 EnableCrashDump : No Disable Online Controller Reset : No EnableLDBBM : No Un-Certified Hard Disk Drives : Allow Treat Single span R1E as R10 : No Max LD per array : 64 Power Saving option : All power saving options are enabled Default spin down time in minutes: 30 Enable JBOD : No TTY Log In Flash : Yes Auto Enhanced Import : Yes BreakMirror RAID Support : No Disable Join Mirror : No Enable Shield State : No Time taken to detect CME : 60s Exit Code: 0x00 7. BBU 정보 확인 # BBU(Battery Backup Unit)는 RAID Controller 전용 배터리로, 예고없이 전원 공급이 끊긴 상황에서 캐시 영역의 데이터가 손실되는 걸 방지하는 역할을 한다.\n확인 가능한 주요정보\n배터리 상태 : Battery State 제조년월 : Date of Manufacture 제조사 : Manufacture Name 동작정보 : 전압(Voltage), 온도(Temperature), Learn Cycle 상태(Learn Cycle Status) $ megacli -AdpBbuCmd -aALL BBU status for Adapter: 0 BatteryType: SuperCaP Voltage: 9438 mV Current: 0 mA Temperature: 28 C Battery State: Optimal BBU Firmware Status: Charging Status : None Voltage : OK Temperature : OK Learn Cycle Requested : No Learn Cycle Active : No Learn Cycle Status : OK Learn Cycle Timeout : No I2c Errors Detected : No Battery Pack Missing : No Battery Replacement required : No Remaining Capacity Low : No Periodic Learn Required : No Transparent Learn : No No space to cache offload : No Pack is about to fail \u0026amp; should be replaced : No Cache Offload premium feature required : No Module microcode update required : No BBU GasGauge Status: 0x002a Pack energy : 298 J Capacitance : 0 Remaining reserve space : 93 Battery backup charge time : 0 hours BBU Design Info for Adapter: 0 Date of Manufacture: 06/13, 2012 Design Capacity: 283 J Design Voltage: 9411 mV Serial Number: 2679 Manufacture Name: LSI Firmware Version : Device Name: SuperCaP Device Chemistry: Battery FRU: N/A TMM FRU: N/A Transparent Learn = 1 App Data = 0 BBU Properties for Adapter: 0 Auto Learn Period: 28 Days Next Learn time: Tue Dec 14 12:07:23 2021 Learn Delay Interval:0 Hours Auto-Learn Mode: Transparent Exit Code: 0x00 8. 컨트롤러 로그확인 # $ megacli -fwtermlog -dsply -aALL [...] 11/06/21 3:32:08: pii ccisue ld pii=1 ld=0 11/06/21 3:32:08: cmdBlockPrep: Command block started (host IOs blocked) 11/06/21 3:32:08: EVT#55239-11/06/21 3:32:08: 58=Consistency Check done on VD 00/0 11/06/21 3:32:08: pii modvd bvd pii=1 \u0026lt;- ld ld=0 11/06/21 3:32:08: ccScheduleSetNextStartTime: RTC_TimeStamp=2918b3b8, nextStartTime=2921e6ac 11/06/21 3:32:08: Next cc scheduled to start at 11/13/21 2:59:56 11/06/21 3:32:08: CC Schedule cycle complete 11/06/21 3:32:08: updateBlockDone: DDF update started 11/06/21 3:32:08: updateStartNext: DDF update complete 11/06/21 3:32:08: ld sync: all LDs sync\u0026#39;d 11/06/21 3:32:08: FP_NotifyAndChangeAll - Driver ldsync required 1 11/06/21 3:32:08: CmdUnBlock: Final command block released (host IOs were blocked for 266 ms) 11/06/21 3:32:08: ld sync: all LDs sync\u0026#39;d 11/06/21 3:32:08: LdDcmdSynchronize gLdSyncRequired:0 mfiCmdStatus:0 11/12/21 12:04:03: EVT#55240-11/12/21 12:04:03: 157=Battery relearn will start in 4 days 11/13/21 2:59:56: prDiskStart: starting Patrol Read on PD=08 11/13/21 2:59:56: EVT#55241-11/13/21 2:59:56: 94=Patrol Read progress on PD 08(e0xfc/s1) is 0.00%(0s) 11/13/21 2:59:56: prDiskStart: starting Patrol Read on PD=09 11/13/21 2:59:56: EVT#55242-11/13/21 2:59:56: 94=Patrol Read progress on PD 09(e0xfc/s0) is 0.00%(0s) 11/13/21 2:59:56: EVT#55243-11/13/21 2:59:56: 39=Patrol Read started 11/13/21 2:59:56: EVT#55244-11/13/21 2:59:56: 66=Consistency Check started on VD 00/0 11/13/21 2:59:56: pii modvd bvd pii=1 \u0026lt;- ld ld=0 11/13/21 2:59:56: cmdBlockPrep: Command block started (host IOs blocked) 11/13/21 2:59:56: prDiskCheckOkToRun: PR cannot run on this pd=8 init or CC running ld=0 11/13/21 2:59:56: prCallback: PR being stopped for pd=08 - state changed 11/13/21 2:59:56: EVT#55245-11/13/21 2:59:56: 445=Patrol Read aborted on PD 08(e0xfc/s1) 11/13/21 2:59:56: prDiskCheckOkToRun: PR cannot run on this pd=9 init or CC running ld=0 11/13/21 2:59:56: prCallback: PR being stopped for pd=09 - state changed 11/13/21 2:59:56: EVT#55246-11/13/21 2:59:56: 445=Patrol Read aborted on PD 09(e0xfc/s0) 11/13/21 2:59:56: PR cycle complete Exit Code: 0x00 RAID Controller 로그에는 PR(Patrol Read)이 수행된 기록, 일관성 검사(CC, Consistency Check) 기록, 논리적 디스크(LD, Logical Drive)의 동기화를 맺은 기록 등이 남아있다.\n일관성 검사 # 영어로 Consistency Check, 축약형은 CC. 일관성 검사는 데이터 및 중복이 올바르게 일치하는지 확인하기 위해 어레이의 모든 드라이브를 서로 비교하는 수동으로 활성해야 동작하는 기능이다. RAID 1으로 구성된 경우 한 드라이브의 데이터를 다른 드라이브와 비교하여 데이터가 오류 없이 복제(미러링)되는지 확인한다.\n9. Patrol Read # Patrol Read(PR)는 디스크 오류 상태가 악화되어 데이터가 손실되기 전에 디스크 오류를 미리 발견하는 자동검사 기능이다.\n기본값으로 새벽 시간마다 자동 수행되며, 수행 간격(Patrol Read Execution Delay)은 168시간이다. Patrol Read는 I/O 리소스의 최대 30%를 차지할 수 있다. PR 설정정보 확인 # $ megacli -AdpPR -Info -aALL Adapter 0: Patrol Read Information: Patrol Read Mode: Auto Patrol Read Execution Delay: 168 hours Number of iterations completed: 448 Next start time: 11/20/2021, 02:00:00 Current State: Stopped Patrol Read on SSD Devices: Disabled Exit Code: 0x00 해석\nPatrol Read Execution Delay : Patrol Read를 실행하는 간격(주기)를 의미합니다. Number of iterations completed : 지금까지 수행 완료된 Patrol Read 횟수를 의미합니다. Next start time : 다음 Patrol Read가 실행되는 시간 Patrol Read on SSD Devices : SSD(Solid State Drive)에도 Patrol Read를 수행하는지 여부 (Patrol Read는 기본적으로 SSD 장치에는 수행하지 않고, HDD 장치에서만 실행합니다.)\nPR 활성화 # $ megacli -AdpPR -EnblAuto -aALL Adapter 0: Patrol Read Mode is set to AUTO. Exit Code: 0x00 PR 비활성화 # $ megacli -AdpPR -Dsbl -aALL Adapter 0: Patrol Read Mode is set to DISABLED. Exit Code: 0x00 PR 스캔 수동실행 # Patrol Read 스캔 작업은 I/O 자원의 최대 30%까지 점유할 수 있다. 따라서 PR 스캔 수동실행 명령어를 실행할 경우 반드시 부하가 적은 시간대(off-peak times)에 시행해야 한다.\n$ megacli -AdpPR -Start -aALL PR 스캔 중지 # $ megacli -AdpPR -Stop -aALL 10. 디스크 LED 블링크 # 디스크 교체 작업을 진행할 때 교체 대상 디스크의 전면 LED를 점등시킨 후 교체 작업을 진행하면 정상 디스크를 잘못 제거하는 등의 인적 실수(Human Fault)를 방지할 수 있다.\n특정 디스크의 물리적 위치를 파악할 때도 사용 가능하다.\n1. Enclosure, Slot 번호 확인 # LED Blink를 실행시키려면 먼저 해당 디스크의 Enclosure 번호(Enclosure Device ID)와 디스크 슬롯 번호(Slot Number)를 먼저 파악해야한다.\n$ megacli -PDList -aALL Adapter #0 Enclosure Device ID: 252 Slot Number: 0 Drive\u0026#39;s position: DiskGroup: 0, Span: 0, Arm: 1 Enclosure position: N/A Device Id: 9 WWN: 5000C5005EC4558C Sequence Number: 2 Media Error Count: 0 Other Error Count: 0 Predictive Failure Count: 0 Last Predictive Failure Event Seq Number: 0 PD Type: SAS Enclosure Device ID: 252는 Enclosure 번호, Slot Number: 0은 디스크 슬롯 번호를 의미한다.\n2. LED Blinking 시작 # 252번 Enclosure의 0번 슬롯에 장착된 디스크는 -physdrv 옵션에서 [252:0]로 표기한다.\n[252:0] --- - | +----\u0026gt; 0 : (Disk) Slot Number | +-------\u0026gt; 252 : Enclosure Device ID $ megacli -PdLocate -start -physdrv[252:0] -aALL Adapter: 0: Device at EnclId-252 SlotId-0 -- PD Locate Start Command was successfully sent to Firmware Exit Code: 0x00 서버 전면부에 장착된 디스크의 LED가 깜빡이는지 확인한다.\n3. LED Blinking 중지 # $ megacli -PdLocate -stop -physdrv[252:0] -aALL Adapter: 0: Device at EnclId-252 SlotId-0 -- PD Locate Stop Command was successfully sent to Firmware Exit Code: 0x00 서버 전면부에 장착된 0번 디스크의 LED 점멸이 멈춘다.\n","date":"Nov 18, 2021","permalink":"/blog/installing-megacli-on-linux/","section":"Blogs","summary":"개요 # LSI 사의 RAID Controller가 장착된 리눅스 서버에서 MegaCLI 명령어를 이용해 RAID 관련 정보를 확인할 수 있다.","title":"MegaCLI 설치 및 사용법"},{"content":"발단 # 컨테이너를 관리하는 프로그램을 컨테이너 런타임(Container Runtime)이라고 부른다.\n쿠버네티스는 v1.20 이후 컨테이너 런타임으로서 도커 사용을 중단(deprecating)하기로 결정했다. 쿠버네티스 v1.20 버전부터는 dockershim이 지원 중단되었다(deprecated)는 경고 메세지가 출력된다.\nUsing dockershim is deprecated, please consider using a full-fledged CRI implementation 이 말은 도커에 내장된 컨테이너 런타임인 dockershim이 곧 삭제 된다는 의미이다. dockershim의 삭제 시점은 쿠버네티스 v1.24 버전에 예정되어 있다.\n쿠버네티스 공식 블로그 포스팅에는 \u0026ldquo;당황하지 마라. 생각보다 극적이지 않다.(You do not need to panic. It’s not as dramatic as it sounds.)\u0026rdquo; 라고 적혀있다.\n변경사항 # kubelet과 docker 사이에 위치한 dockershim이 사라진다.\nkubelet 아키텍쳐 비교\nkubelet에서 컨테이너 런타임으로 docker를 사용할 경우 containerd와 직접 통신한다. 아키텍쳐가 더 간결해지고 성능이 향상된다.\n파드 시작시 지연시간 비교\n위 그래프는 파드가 시작될 때의 지연시간이다. (낮을수록 좋음) dockershim을 이용할 때보다 Containerd를 이용할 때 파드 시작속도가 훨씬 빠른 걸 알 수 있다.\n이유 # 방치 : 개발사인 도커는 도커에 내장된 컨테이너 런타임인 dockershim을 2019년 3월 이후로 2년 넘게 업데이트 없이 방치했다. 쿠버네티스는 1년간 도커를 기다려주던 중 지원 중단 선언을 한 것이다. 호환성 : 쿠버네티스는 CRI(Container Runtime Interface) 표준을 사용해 컨테이너 런타임과 통신한다. 그러나 docker는 CRI 표준을 지키지 않고 별도로 dockershim을 통해 변환을 거쳐 도커 전용 인터페이스로 변환해 사용해왔다. dockershim으로 인해 생긴 불필요하고 복잡한 구조 때문에 쿠버네티스 측에서 유지보수가 어려워졌다. 영향/해결책 # Kubernetes 시스템 관리자의 관점\n도커 이미지 : 도커 이미지는 OCI(Open Container Initiative) 표준이기 때문에 계속해서 사용 가능하다. 조치가 필요한 사항은 없음. 노드의 컨테이너 런타임 : 마스터 노드와 워커 노드의 컨테이너 런타임을 dockershim에서 CRI를 지원하는 containerd나 CRI-O 중 하나를 선택해 변경하면 된다. 컨테이너 런타임을 containerd나 CRI-O로 변경하면 기존에 쓰던 docker 명령어는 사용할 수 없게 되니 참고하자. 컨테이너 런타임 설명\nCRI-O\n레드햇, 인텔, SUSE, Hyper, IBM의 관리자와 컨트리뷰터들이 만든 커뮤니티 중심의 오픈소스 프로젝트이다. CRI-O는 container를 실행하는 역할만 담당하고 있다. (도커의 containerd와 동일한 역할) CRI-O는 image build, CLI, image registry 생성 등의 부가기능은 수행하지 못한다.\ncontainerd\n단순성, 견고성 및 이식성을 강조하는 산업 표준 컨테이너 런타임\n","date":"Nov 16, 2021","permalink":"/blog/k8s/kubernetes-is-deprecating-docker/","section":"Blogs","summary":"발단 # 컨테이너를 관리하는 프로그램을 컨테이너 런타임(Container Runtime)이라고 부른다.","title":"쿠버네티스의 도커 지원 중단"},{"content":"개요 # cordon, uncordon, drain 명령어를 통해 쿠버네티스 클러스터를 구성하는 노드의 파드 스케줄링을 제어할 수 있다. 주로 노드의 패치 작업이나 하드웨어 유지보수 작업 시에 파드 스케줄링을 사용한다.\n환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 Docker Desktop 4.1.1 (쿠버네티스 기능 활성화됨) minikube v.1.24.0 (Homebrew로 설치) 멀티노드 구성 : 1대의 마스터 노드(Control Plane) + 3대의 워커 노드 본론 # cordon # kubectl cordon은 지정한 노드에 이미 배포된 Pod는 그대로 유지하면서 이후 추가적인 Pod 스케줄링에서 제외하는 명령어이다.\ncordon 설정된 시점 이후부터 해당 노드에는 추가로 pod가 배포되지 않는다.\n설정방법 # $ kubectl cordon \u0026lt;노드 이름\u0026gt; 예제 # 노드 확인 # $ kubectl get no NAME STATUS ROLES AGE VERSION mnlab Ready,SchedulingDisabled control-plane,master 2d16h v1.22.3 mnlab-m02 Ready \u0026lt;none\u0026gt; 43s v1.22.3 mnlab-m03 Ready \u0026lt;none\u0026gt; 29s v1.22.3 mnlab-m04 Ready \u0026lt;none\u0026gt; 16s v1.22.3 현재 시나리오에는 클러스터 노드 4대(Master node 1 + Worker node 3)가 존재한다.\ncordon 설정 # mnlab-m04 노드에 cordon을 설정했다.\n$ kubectl cordon mnlab-m04 node/mnlab-m04 cordoned $ kubectl get no NAME STATUS ROLES AGE VERSION mnlab Ready,SchedulingDisabled control-plane,master 2d17h v1.22.3 mnlab-m02 Ready \u0026lt;none\u0026gt; 12m v1.22.3 mnlab-m03 Ready \u0026lt;none\u0026gt; 11m v1.22.3 mnlab-m04 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 11m v1.22.3 cordon 처리한 mnlab-m04 노드가 스케줄링 비활성화(SchedulingDisabled) 상태로 빠졌다.\ndeployment 생성 # 이제 mnlab-m04 노드에 pod가 배포 안되는지 직접 확인하기 위해 deployment를 생성해본다.\n$ cat deploy-nginx.yaml apiVersion: apps/v1 kind: Deployment # 타입은 Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 5 # 5개의 Pod를 유지한다. selector: # Deployment에 속하는 Pod의 조건 matchLabels: # label의 app 속성의 값이 nginx 인 Pod를 찾아라. app: nginx template: metadata: labels: app: nginx # labels 필드를 사용해서 app: nginx 레이블을 붙힘 spec: containers: # container에 대한 정의 - name: nginx # container의 이름 image: nginx:1.7.9 # Docker Hub에 업로드된 nginx:1.7.9 이미지를 사용 ports: - containerPort: 80 5개의 nginx pod를 생성하는 deployment yaml 파일이다.\n컨테이너 이미지는 Docker Hub에 공식 등록된 nginx:1.7.9를 사용한다.\n$ kubectl apply -f deploy-nginx.yaml deployment.apps/nginx-deployment created 작성한 yaml 파일로 deployment를 생성했다.\n배포 결과확인 # deployment로 pod 5대를 생성했지만 mnlab-m04 노드에는 pod가 1대도 배포되지 않았다!\n$ kubectl get all -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deployment-5d59d67564-5jz9r 1/1 Running 0 12s 10.244.1.3 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-grwwc 1/1 Running 0 12s 10.244.2.3 mnlab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-pmsfr 1/1 Running 0 12s 10.244.2.2 mnlab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-qwnht 1/1 Running 0 12s 10.244.1.4 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-xc8tv 1/1 Running 0 12s 10.244.1.2 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 15h \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deployment 5/5 5 5 12s nginx nginx:1.7.9 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deployment-5d59d67564 5 5 5 12s nginx nginx:1.7.9 app=nginx,pod-template-hash=5d59d67564 cordon 해제 # $ kubectl uncordon mnlab-m04 node/mnlab-m04 uncordoned mnlab-m04 노드의 cordon이 해제되었다.\n$ kubectl get no NAME STATUS ROLES AGE VERSION mnlab Ready,SchedulingDisabled control-plane,master 2d17h v1.22.3 mnlab-m02 Ready \u0026lt;none\u0026gt; 15m v1.22.3 mnlab-m03 Ready \u0026lt;none\u0026gt; 14m v1.22.3 mnlab-m04 Ready \u0026lt;none\u0026gt; 14m v1.22.3 이후부터 mnlab-m04 노드에 pod가 다시 배포될 수 있다.\ndrain # kubectl drain은 지정한 노드에 이미 배포된 pod를 제거한 후 다른 노드에 재배치하는 명령어이다. (참고로 쿠버네티스에서 pod를 옮기는 기능은 존재하지 않는다. 파드를 다른 노드에 다시 생성할 뿐이다.)\ndrain 설정된 노드에는 pod가 다른 노드로 재배치된 후 스케줄링이 비활성화(SchedulingDisabled)되어 이후 어떠한 pod도 배포되지 않는다. 노드의 하드웨어 작업이나 리부팅이 필요할 경우, 먼저 drain으로 pod를 다른 노드로 재배치한 후 원하는 유지보수 작업을 수행하면 된다.\n주의사항 : ReplicationController, ReplicaSet, Deployment, StatefulSet 등의 컨트롤러에서 관리하지 않는 개별 파드(Static pod)의 경우 drain 실행시 다른 노드에 재배포 되지않고 삭제만 발생하므로 데이터가 손실될 수 있다.\ndrain을 실행하면 발생하는 일\n해당 노드에 더 이상 pod 배포를 하지 않도록 스케줄링 비활성화 (SchedulingDisabled) 노드에 이미 배포된 pod를 모두 삭제 삭제된 pod는 새로운 정상 노드에서 재생성됨 drain은 cordon 기능에 pod를 다른 노드로 재배치하는 기능 2개가 결합된 거라고 이해하면 쉽다.\n설정방법 # $ kubectl drain \u0026lt;노드 이름\u0026gt; --ignore-daemonsets 예제 # 노드 확인 # $ kubectl get no NAME STATUS ROLES AGE VERSION mnlab Ready,SchedulingDisabled control-plane,master 2d17h v1.22.3 mnlab-m02 Ready \u0026lt;none\u0026gt; 29m v1.22.3 mnlab-m03 Ready \u0026lt;none\u0026gt; 28m v1.22.3 mnlab-m04 Ready \u0026lt;none\u0026gt; 28m v1.22.3 현재 시나리오에는 클러스터 노드 4대(Master node 1 + Worker node 3)가 존재한다.\ndeployment 배포 # cordon 예제에서 쓴 deployment를 똑같이 배포해놓았다.\n$ kubectl get all -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deployment-5d59d67564-2jdjt 1/1 Running 0 6s 10.244.1.7 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-4qjvt 1/1 Running 0 6s 10.244.2.7 mnlab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-6qjg5 1/1 Running 0 6s 10.244.3.3 mnlab-m04 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-j78cp 1/1 Running 0 6s 10.244.2.8 mnlab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-j8wft 1/1 Running 0 6s 10.244.1.8 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 15h \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deployment 5/5 5 5 6s nginx nginx:1.7.9 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deployment-5d59d67564 5 5 5 6s nginx nginx:1.7.9 app=nginx,pod-template-hash=5d59d67564 5개의 pod가 mnlab-m02, mnlab-m03, mnlab-m04 노드에 골고루 배포되어 있는 상태이다. 이 상태에서 mnlab-m04 노드를 drain 해본다.\ndrain # $ kubectl drain mnlab-m04 --ignore-daemonsets node/mnlab-m04 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kindnet-jgdz8, kube-system/kube-proxy-9fxn7 evicting pod default/nginx-deployment-5d59d67564-6qjg5 pod/nginx-deployment-5d59d67564-6qjg5 evicted node/mnlab-m04 evicted 현재 시나리오 기준으로 Worker node마다 시스템 관련 Pod인 kindnet과 kube-proxy가 존재한다.\ndrain 실행시 --ignore-daemonsets 옵션을 줘서 데몬셋에 의해 생성된 시스템 Pod는 drain에서 제외한다.\n$ kubectl get no NAME STATUS ROLES AGE VERSION mnlab Ready,SchedulingDisabled control-plane,master 2d17h v1.22.3 mnlab-m02 Ready \u0026lt;none\u0026gt; 31m v1.22.3 mnlab-m03 Ready \u0026lt;none\u0026gt; 31m v1.22.3 mnlab-m04 Ready,SchedulingDisabled \u0026lt;none\u0026gt; 31m v1.22.3 mnlab-m04 노드가 스케줄링 비활성화 상태(SchedulingDisabled)로 빠졌다. 더 이상 해당 노드에 pod가 스케줄링되어 생성되지 않는다는 의미이다.\n결과확인 # mnlab-m04 노드에 있던 pod 1대가 삭제된 후 다른 노드로 재배포되었다.\n$ kubectl get all -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deployment-5d59d67564-2jdjt 1/1 Running 0 91s 10.244.1.7 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-4qjvt 1/1 Running 0 91s 10.244.2.7 mnlab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-j78cp 1/1 Running 0 91s 10.244.2.8 mnlab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-j8wft 1/1 Running 0 91s 10.244.1.8 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-nn699 1/1 Running 0 20s 10.244.1.9 mnlab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 15h \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deployment 5/5 5 5 91s nginx nginx:1.7.9 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deployment-5d59d67564 5 5 5 91s nginx nginx:1.7.9 app=nginx,pod-template-hash=5d59d67564 해제 # 해제 방법은 cordon과 동일하게 uncordon 명령어로 스케줄링 비활성화를 해제한다.\n$ kubectl uncordon mnlab-m04 node/mnlab-m04 uncordoned mnlab-m04 노드에 스케줄링 비활성화(SchedulingDisabled) 상태가 사라졌다. 이후부터는 다시 해당 노드로 pod가 배포될 것이다.\n$ kubectl get no NAME STATUS ROLES AGE VERSION mnlab Ready,SchedulingDisabled control-plane,master 2d17h v1.22.3 mnlab-m02 Ready \u0026lt;none\u0026gt; 48m v1.22.3 mnlab-m03 Ready \u0026lt;none\u0026gt; 48m v1.22.3 mnlab-m04 Ready \u0026lt;none\u0026gt; 47m v1.22.3 ","date":"Nov 14, 2021","permalink":"/blog/k8s/cordon-and-drain/","section":"Blogs","summary":"개요 # cordon, uncordon, drain 명령어를 통해 쿠버네티스 클러스터를 구성하는 노드의 파드 스케줄링을 제어할 수 있다.","title":"쿠버네티스 cordon, drain"},{"content":"개요 # 리눅스 서버에 lshw(list hardware) 패키지를 설치해 서버 모델, CPU, Memory 등의 하드웨어 정보를 원격으로 수집할 수 있다. 시스템 관리 유틸리티 패키지들을 적절한 상황에 활용하면 효율적인 시스템 관리가 가능하다. 환경 # OS : Red Hat Enterprise Linux Server release 6.x Architecture : x86_64 패키지 관리자 : rpm (Red hat Package Manager) 설치 패키지 lshw B.02.17 (lshw-2.17-1.el6.rf.x86_64.rpm)\n본론 # 패키지 설치 # 1. 설치전 정보 확인 # $ arch x86_64 해당 서버의 아키텍쳐는 Intel CPU 기반 64bit 아키텍쳐(x86_64)이다.\n2. 패키지 설치 # $ ls -lh 합계 1.7M -rw-rw-r-- 1 dev dev 1.7M 2021-11-12 08:35 lshw-2.17-1.el6.rf.x86_64.rpm 정보를 수집할 대상 서버의 아키텍쳐와 운영체제 버전에 맞는 lshw 패키지 설치파일을 업로드 해놓는다.\n패키지 설치파일은 반드시 검증된 패키지 저장소에서 받아야 한다.\n$ rpm -ivh lshw-2.17-1.el6.rf.x86_64.rpm 경고: lshw-2.17-1.el6.rf.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 6b8d79e6: NOKEY 준비 중... ########################################### [100%] 1:lshw ########################################### [100%] lshw 패키지가 설치 완료됐다.\n3. 설치결과 확인 # $ rpm -qa lshw lshw-2.17-1.el6.rf.x86_64 lshw 패키지가 RPM(Redhat Package Manaer)에서 조회된다. 설치된 패키지의 버전은 2.17.1 이다.\n$ lshw -version B.02.17 the latest version is B.02.18 버전 출력으로 명령어 동작을 테스트한다.\n조작법 # 1. 상세정보 출력 # 아무 옵션 없이 lshw 명령어를 실행하면 모든 하드웨어의 상세 정보를 출력한다.\n$ lshw dev-server1 description: Expansion Chassis product: UCSC-C220-M3S vendor: Cisco Systems Inc version: A serial: XXX1704X1X0 width: 64 bits capabilities: smbios-2.7 dmi-2.7 vsyscall64 vsyscall32 configuration: boot=normal chassis=expansion frontpanel_password=enabled keyboard_password=disabled power-on_password=di sabled uuid=31CB0CAD-EB83-4327-8599-83C27AD698A8 *-core description: Motherboard product: UCSC-C220-M3S vendor: Cisco Systems Inc physical id: 0 version: 74-10442-01 serial: XXX1703XXX7 *-firmware description: BIOS 2. 요약정보 출력 # -short 옵션을 붙이면 전체 하드웨어의 요약정보를 출력한다.\n$ lshw -short H/W path Device Class Description ======================================================== system UCSC-C220-M3S /0 bus UCSC-C220-M3S /0/0 memory 64KiB BIOS /0/1 memory /0/1/0 memory 8GiB DIMM DDR3 1600 MHz (0.6 ns) /0/2 memory /0/2/0 memory DIMM Synchronous [empty] /0/3 memory [...] /0/13.1 generic Sandy Bridge Ring to PCI Express Performance Monitor /0/13.4 generic Sandy Bridge QuickPath Interconnect Agent Ring Registers /0/13.5 generic Sandy Bridge Ring to QuickPath Interconnect Link 0 Performance Monitor /0/13.6 generic Sandy Bridge Ring to QuickPath Interconnect Link 1 Performance Monitor 3. 특정 파트 확인 # 클래스는 하드웨어의 특정 카테고리(Part)를 의미한다. 클래스에는 system, cpu, memory, disk, volume, storage 등이 있다.\n-c 또는 -class 옵션을 붙이면 특정 하드웨어 파트(클래스)의 정보만 출력한다.\n어떤 클래스들이 존재하는지 궁금하다면 lshw -short 명령어 결과의 Class 컬럼을 참조하자.\n$ lshw -c system dev-server1 description: Expansion Chassis product: UCSC-C220-M3S vendor: Cisco Systems Inc version: A serial: XXX0000X0X0 width: 64 bits capabilities: smbios-2.7 dmi-2.7 vsyscall64 vsyscall32 configuration: boot=normal chassis=expansion frontpanel_password=enabled keyboard_password=disabled power-on_password=disabled uuid=31CB0CAD-EB83-4327-8599-83C27AD698A8 서버 모델명(product), 제조사(vendor), 고유번호(serial)를 확인할 수 있다.\n$ lshw -c cpu *-cpu:0 description: CPU product: Xeon vendor: Intel Corp. physical id: 32 bus info: cpu@0 version: Intel(R) Xeon(R) CPU E5-2667 0 @ 2.90GHz slot: CPU1 size: 1200MHz capacity: 4GHz width: 64 bits clock: 100MHz capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm dca sse4_1 sse4_2 x2apic popcnt aes xsave avx lahf_lm ida arat epb xsaveopt pln pts tpr_shadow vnmi flexpriority ept vpid cpufreq configuration: cores=6 enabledcores=6 threads=12 *-cpu:1 DISABLED description: CPU [empty] physical id: 33 slot: CPU2 CPU와 관련된 상세정보를 확인할 수 있다.\n확인 가능한 정보\nCPU 소켓 수(cpu:0, cpu:1) CPU 아키텍쳐(x86-64) CPU 모델명(version) 코어 수(cores=6) 스레드 수(threads=12)\n$ lshw -c memory *-firmware description: BIOS vendor: Cisco Systems, Inc. physical id: 0 version: C220M3.1.4.7b.0.100520120256 date: 10/05/2012 size: 64KiB capacity: 4032KiB capabilities: pci upgrade shadowing cdboot bootselect socketedrom edd int13floppy1200 int13floppy720 int13floppy2880 int5printscreen int9keyboard int14serial int17printer acpi usb biosbootspecification uefi *-memory:0 UNCLAIMED physical id: 1 *-bank UNCLAIMED description: DIMM DDR3 1600 MHz (0.6 ns) product: M393B1K70DH0-YK0 vendor: Samsung physical id: 0 serial: 3592XX2X slot: DIMM_A1 size: 8GiB width: 64 bits clock: 1600MHz (0.6ns) *-memory:1 UNCLAIMED physical id: 2 *-bank UNCLAIMED description: DIMM Synchronous [empty] product: NO DIMM vendor: NO DIMM physical id: 0 serial: NO DIMM slot: DIMM_A2 Memory Bank 의 전체 개수, 사용중인 개수, 메모리 용량, 동작클럭, 메모리 모델명, 메모리 고유번호(serial)까지 확인할 수 있다.\n4. HTML 파일로 보기 # CLI 환경에서 보는게 불편한 엔지니어라면 하드웨어 정보를 HTML 파일로 저장해 웹브라우저에서 보는 방법도 있다.\n$ lshw -html \u0026gt;\u0026gt; /tmp/lshw.html CLI가 더 편한 엔지니어라면 저장된 html 파일을 SFTP 등을 이용해 웹 브라우저가 실행되는 환경으로 옮겨서 실행해야하는 게 번거롭기 때문에 굳이 이 방법을 쓸 필요가 있나 의구심이 든다.\n결론 # 이 글에서 소개하는 lshw 패키지는 하드웨어 정보를 확인하는 다양한 방법 중 한 가지일 뿐이다. lshw 말고도 다양한 기능을 가진 패키지들이 널려있다.\n시스템 관리에는 정답이 없기 때문에 엔지니어 각자가 잘 알고 익숙한 방법으로 하드웨어 정보를 수집하면 된다.\n","date":"Nov 12, 2021","permalink":"/blog/lshw-on-linux/","section":"Blogs","summary":"개요 # 리눅스 서버에 lshw(list hardware) 패키지를 설치해 서버 모델, CPU, Memory 등의 하드웨어 정보를 원격으로 수집할 수 있다.","title":"lshw 패키지로 리눅스 하드웨어 정보 확인"},{"content":"개요 # minikube를 이용해 3대의 노드(1 master node + 2 worker node)를 생성해서 Kubernetes 클러스터 구성하는 방법을 설명합니다.\n환경 # Hardware : macBook Pro (16\u0026quot;, M1 Pro, 2021) OS : macOS Monterey 12.4 minikube v1.26.0 Docker Desktop 4.8.2 (79419) 노드 3대를 생성할 예정이기 때문에 하드웨어의 메모리 리소스가 최소 8GB 이상은 되어야 안정적으로 실습할 수 있습니다.\n전제조건 # minikube가 설치되어 있어야 합니다.\ndocker desktop이 설치되어 있어야 합니다.\nkubectl이 설치되어 있어야 합니다.\n이 글에서는 필수 패키지 설치에 대한 가이드는 다루지 않습니다.\n실습하기 # 1. 멀티노드 생성 # 3대의 노드로 구성된 minikube 클러스터를 생성합니다.\n$ minikube start \\ --driver=\u0026#39;docker\u0026#39; \\ --profile=\u0026#39;multinode-lab\u0026#39; \\ --cni=\u0026#39;calico\u0026#39; \\ --kubernetes-version=\u0026#39;stable\u0026#39; \\ --nodes=3 명령어 옵션 설명 # --driver='docker' : 도커를 하이퍼바이저로 사용합니다.\n--profile='multinode-lab' : multinode-lab이라는 이름의 프로파일을 생성합니다.\n--cni='calico : 컨테이너 네트워크 인터페이스를 calico로 지정합니다.\nauto, bridge, calico, cilium, flannel, kindnet 중 하나를 선택할 수 있습니다. --kubernetes-version='stable' : 노드에 설치되는 쿠버네티스 버전을 안정화된 버전으로 지정합니다.\n--nodes=3 : 노드 3대로 구성된 클러스터를 생성합니다.\n2. 노드 상태 확인 # minikube # $ minikube status -p multinode-lab multinode-lab type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured multinode-lab-m02 type: Worker host: Running kubelet: Running multinode-lab-m03 type: Worker host: Running kubelet: Running 3대의 노드가 모두 정상 실행중(Running)이다. type: 값을 보면 해당 노드가 Master node(Control plane)인지 Worker node인지 구분할 수 있습니다.\nkubectl # kubectl 명령어를 사용해서도 쿠버네티스 클러스터 노드의 상태를 확인할 수 있습니다.\n$ kubectl get node NAME STATUS ROLES AGE VERSION multinode-lab Ready control-plane 37m v1.24.1 multinode-lab-m02 Ready \u0026lt;none\u0026gt; 36m v1.24.1 multinode-lab-m03 Ready \u0026lt;none\u0026gt; 36m v1.24.1 컨트롤 플레인 1대와 워커노드 2대로 구성된 걸 확인할 수 있습니다.\n전체 노드에는 2022년 6월 기준으로 안정화 버전인 kubernetes v1.24.1이 설치되었습니다.\n3. deployment 배포 # deployment yaml 작성 # 현재 경로에 nginx-deploy.yaml 파일을 생성합니다.\n$ cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ./nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment # 타입은 Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 # 3개의 Pod를 유지한다. selector: # Deployment에 속하는 Pod의 조건 matchLabels: # label의 app 속성의 값이 nginx 인 Pod를 찾아라. app: nginx template: metadata: labels: app: nginx # labels 필드를 사용해서 app: nginx 레이블을 붙힘 spec: containers: # container에 대한 정의 - name: nginx # container의 이름 image: nginx:1.7.9 # Docker Hub에 업로드된 nginx:1.7.9 이미지를 사용 ports: - containerPort: 80 EOF 매니페스트 내용을 간단하게 요약하자면 3개의 nginx 파드를 배포하는 deployment입니다.\ndeployment 배포 # 작성한 yaml 파일을 사용해서 deployment를 배포합니다.\n$ kubectl apply -f nginx-deploy.yaml deployment.apps/nginx-deployment created 상태확인 # nginx 파드 상태를 확인합니다.\n3개의 nginx 파드가 생성되고 있습니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-84df99548d-csxnp 0/1 ContainerCreating 0 8s nginx-deployment-84df99548d-fmnx9 0/1 ContainerCreating 0 8s nginx-deployment-84df99548d-nsmsf 0/1 ContainerCreating 0 8s 잠시 기다리면 상태가 Running으로 바뀌며 pod 생성이 완료됩니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-84df99548d-csxnp 1/1 Running 0 2m19s 10.244.150.129 multinode-lab-m02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-84df99548d-fmnx9 1/1 Running 0 2m19s 10.244.166.195 multinode-lab \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-84df99548d-nsmsf 1/1 Running 0 2m19s 10.244.148.65 multinode-lab-m03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 여기서 중요한 사실은 여러 노드에 걸쳐 3대의 파드가 배포된다는 사실입니다.\nNODE 컬럼을 보면 파드가 어디 노드에 배포되었는지 확인할 수 있습니다.\n현재 이 실습환경은 Control Plane이 NoSchedule 상태가 아니라서, Control Plane인 multinode-lab 노드에도 파드가 1개 배포되었습니다.\n$ kubectl get node NAME STATUS ROLES AGE VERSION multinode-lab Ready control-plane 12m v1.24.1 multinode-lab-m02 Ready \u0026lt;none\u0026gt; 12m v1.24.1 multinode-lab-m03 Ready \u0026lt;none\u0026gt; 11m v1.24.1 4. service 배포 # 파드에서 실행중인 nginx 웹을 외부에 노출시키려면 service 리소스가 필요합니다.\nservice yaml 작성 # service를 생성하기 위해 매니페스트를 작성합니다.\n$ cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ./nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: type: NodePort selector: app: nginx-app ports: - targetPort: 80 port: 80 # nodePort is Optional field # By default and for convenience, # the Kubernetes control plane will allocate # a port from a range (default: 30000-32767) nodePort: 30080 EOF service 배포 # 작성한 yaml 파일을 사용해서 service를 배포합니다.\n$ kubectl apply -f nginx-service.yaml service/nginx-service created nginx-service가 생성되었습니다.\n$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 23m nginx-service NodePort 10.107.245.50 \u0026lt;none\u0026gt; 80:30080/TCP 2m16s 현재 nginx 파드는 nginx-service를 통해 외부 사용자에게 노출된 상태입니다.\n5. 접속 테스트 # minikube로 접속 가능한 서비스 목록을 확인합니다.\n$ minikube service list \\ --profile multinode-lab |-------------|---------------|--------------|-----| | NAMESPACE | NAME | TARGET PORT | URL | |-------------|---------------|--------------|-----| | default | kubernetes | No node port | | default | nginx-service | 80 | | | kube-system | kube-dns | No node port | |-------------|---------------|--------------|-----| nginx-service는 default 네임스페이스에 있으며 nginx 파드의 80 포트로 연결됩니다.\nminikube의 터널링 기능을 통해 로컬 환경에서 nginx-service로 접속합니다.\nnginx-service는 외부에서 들어온 사용자를 nginx 파드의 80 포트로 연결해줍니다.\n$ minikube service nginx-service \\ --profile multinode-lab 정상 실행된 결과는 다음과 같이 출력됩니다.\n|-----------|---------------|-------------|---------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|---------------|-------------|---------------------------| | default | nginx-service | 80 | http://192.168.58.2:30080 | |-----------|---------------|-------------|---------------------------| 🏃 nginx-service 서비스의 터널을 시작하는 중 |-----------|---------------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |-----------|---------------|-------------|------------------------| | default | nginx-service | | http://127.0.0.1:56757 | |-----------|---------------|-------------|------------------------| 🎉 Opening service default/nginx-service in default browser... ❗ Because you are using a Docker driver on darwin, the terminal needs to be open to run it. 명령어가 실행된 후 자동으로 기본 브라우저가 열리며 nginx 파드에 접속됩니다.\n실습환경 정리 # 방법 1. minikube 종료 # minikube는 실습환경의 CPU, 메모리 리소스를 많이 점유합니다.\nminikube 클러스터를 계속 켜놓는 건 하드웨어에 부담이 많이 가고 배터리 소모도 심하기 때문에 minikube 실습이 끝난 후에는 반드시 종료 또는 삭제해줍니다.\n$ minikube stop -p multinode-lab ✋ Stopping node \u0026#34;multinode-lab\u0026#34; ... 🛑 Powering off \u0026#34;multinode-lab\u0026#34; via SSH ... ✋ Stopping node \u0026#34;multinode-lab-m02\u0026#34; ... 🛑 Powering off \u0026#34;multinode-lab-m02\u0026#34; via SSH ... ✋ Stopping node \u0026#34;multinode-lab-m03\u0026#34; ... 🛑 Powering off \u0026#34;multinode-lab-m03\u0026#34; via SSH ... 🛑 3 nodes stopped. 모든 노드가 종료되었습니다.\n노드상태 확인 # minikube 명령어로 노드 3대의 상태를 확인해봅니다.\n$ minikube status -p multinode-lab multinode-lab type: Control Plane host: Stopped kubelet: Stopped apiserver: Stopped kubeconfig: Stopped multinode-lab-m02 type: Worker host: Stopped kubelet: Stopped multinode-lab-m03 type: Worker host: Stopped kubelet: Stopped multinode-lab, multinode-lab-m02, multinode-lab-m03 노드가 모두 정상 종료(Stopped)되었습니다.\n이전에 생성한 리소스는 남아있기 때문에 다시 클러스터를 시작하면 그대로 실습 환경을 이어서 사용할 수 있습니다.\n방법 2. minikube 삭제 # minikube 클러스터를 더 이상 사용할 필요가 없을 경우, 클러스터의 모든 노드와 리소스를 삭제하면 됩니다.\n$ minikube delete --profile=\u0026#39;multinode-lab\u0026#39; 삭제가 정상적으로 완료되었을 경우 메세지는 다음과 같이 표시됩니다.\n🔥 docker 의 \u0026#34;multinode-lab\u0026#34; 를 삭제하는 중 ... 🔥 /Users/steve/.minikube/machines/multinode-lab 제거 중 ... 🔥 /Users/steve/.minikube/machines/multinode-lab-m02 제거 중 ... 🔥 /Users/steve/.minikube/machines/multinode-lab-m03 제거 중 ... 💀 \u0026#34;multinode-lab\u0026#34; 클러스터 관련 정보가 모두 삭제되었습니다 🔥 모든 프로필이 성공적으로 삭제되었습니다 minikube의 전체 프로파일 리스트를 확인합니다.\n$ minikube profile list 3대의 노드로 구성했던 multinode-lab 프로파일이 삭제된 걸 확인할 수 있습니다.\n🤹 Exiting due to MK_USAGE_NO_PROFILE: No minikube profile was found. 💡 권장: You can create one using \u0026#39;minikube start\u0026#39;. 마치며 # 지금까지 minikube로 멀티노드를 구성하고 서비스를 배포해보는 실습을 진행해보았습니다.\n끝까지 읽어주셔서 감사합니다.\n","date":"Nov 10, 2021","permalink":"/blog/k8s/multinode-in-minikube/","section":"Blogs","summary":"개요 # minikube를 이용해 3대의 노드(1 master node + 2 worker node)를 생성해서 Kubernetes 클러스터 구성하는 방법을 설명합니다.","title":"minikube 멀티노드 구성"},{"content":"개요 # minikube 기반의 쿠버네티스 환경에서 deployment를 생성하고 제어하는 실습을 한다.\n환경 # Hardware : macBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 minikube v1.24.0 + docker desktop v4.1.1 kubectl stable v1.22.3 본문 # 해당 과정은 minikube가 구축된 이후 시점부터 진행된다. 반드시 minikube를 구성한 상태에서 이 실습을 진행해야만 한다.\ndeployment 생성 # deployment는 애플리케이션을 배포하고 업데이트하기 위한 리소스이다. 실제 pod들은 deployment가 아닌 replicaset에 의해 생성, 관리된다. deployment를 생성하면 replicaset 리소스가 그 아래에 자동생성된다. pod를 감시하는 replicaset이 deployment를 지원하는 구조이다. 현업에서 deployment는 배포 업데이트에 주로 쓰인다. (e.g. 블루-그린 업데이트, 롤링 업데이트, 카나리 업데이트) 1. yaml 작성 # nginx pod를 3대 배포하는 Deployment의 yaml 파일을 작성한다.\n$ vi nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment # 타입은 Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 # 3개의 Pod를 유지한다. selector: # Deployment에 속하는 Pod의 조건 matchLabels: # label의 app 속성의 값이 nginx 인 Pod를 찾아라. app: nginx template: metadata: labels: app: nginx # labels 필드를 사용해서 app: nginx 레이블을 붙힘 spec: containers: # container에 대한 정의 - name: nginx # container의 이름 image: nginx:1.7.9 # Docker Hub에 업로드된 nginx:1.7.9 이미지를 사용 ports: - containerPort: 80 해당 yaml 코드를 복사 붙여넣기 해서 작성한다.\n2. 생성 # 작성한 nginx-deploy.yaml 파일을 기반으로 deployment를 생성한다.\n$ kubectl apply -f nginx-deploy.yaml deployment.apps/nginx-deployment created 3. 확인 # 간단한 포맷으로 출력\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deployment-5d59d67564-5jfwj 1/1 Running 0 3m51s pod/nginx-deployment-5d59d67564-djrms 1/1 Running 0 90s pod/nginx-deployment-5d59d67564-vkkqp 1/1 Running 0 2m5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 4h34m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 9m34s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5d59d67564 3 3 3 9m34s deployment는 pod 템플릿의 각 버전마다 하나씩 여러 개의 replicaset을 생성한다. pod 템플릿의 해시값을 사용하면 deployment에서 지정된 버전의 pod 템플릿에 관해 항상 동일한(기존의) replicaset을 사용할 수 있다.\n자세한 포맷으로 출력\n-o wide 옵션을 붙이면 더 자세한 정보를 출력준다.\n$ kubectl get all -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-deployment-5d59d67564-5jfwj 1/1 Running 0 5m37s 172.17.0.6 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-djrms 1/1 Running 0 3m16s 172.17.0.7 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-deployment-5d59d67564-vkkqp 1/1 Running 0 3m51s 172.17.0.5 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 4h36m \u0026lt;none\u0026gt; NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR deployment.apps/nginx-deployment 3/3 3 3 11m nginx nginx:1.7.9 app=nginx NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR replicaset.apps/nginx-deployment-5d59d67564 3 3 3 11m nginx nginx:1.7.9 app=nginx,pod-template-hash=5d59d67564 4. pod 재생성 테스트 # Deployment와 연관된 replicaset은 pod의 개수를 유지하는 역할을 수행한다. pod 1개를 삭제해서 인위적으로 pod 개수를 줄여보자.\nkubectl delete pod nginx-deployment-5d59d67564-djrms pod \u0026#34;nginx-deployment-5d59d67564-djrms\u0026#34; deleted djrms pod를 삭제했다.\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deployment-5d59d67564-5jfwj 1/1 Running 0 8m10s pod/nginx-deployment-5d59d67564-82rvt 1/1 Running 0 88s pod/nginx-deployment-5d59d67564-vkkqp 1/1 Running 0 6m24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 4h38m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 13m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5d59d67564 3 3 3 13m djrms pod를 삭제하면 replicaset은 지정된 pod 개수(3개)를 유지하기 위해 82rvt pod를 자동생성한다.\n$ kubectl describe pod nginx-deployment-5d59d67564-82rvt | grep -i controlled Controlled By: ReplicaSet/nginx-deployment-5d59d67564 82rvt pod를 관리하는 컨트롤러는 nginx-deployment-5d59d67564 replicaset임을 확인할 수 있다.\n롤링 업데이트 # 1. 롤링 업데이트 실행 # 명령어 형식\n$ kubectl set image deployment/\u0026lt;deployment 이름\u0026gt; \u0026lt;container 이름\u0026gt;=\u0026lt;이미지 이름\u0026gt;:\u0026lt;이미지 버전\u0026gt; 실제 명령어 예시\n$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 deployment.apps/nginx-deployment image updated nginx-deployment에 설정된 nginx 1.7.9 버전의 이미지가 1.9.1 버전의 nginx 이미지로 업데이트 되었다.\n2. 결과확인 # $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deployment-69c44dfb78-d97k5 1/1 Running 0 10s pod/nginx-deployment-69c44dfb78-kqqk4 1/1 Running 0 9s pod/nginx-deployment-69c44dfb78-kv6ms 1/1 Running 0 11s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5h11m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 35s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5d59d67564 0 0 0 35s replicaset.apps/nginx-deployment-69c44dfb78 3 3 3 11s 기존의 replicaset은 비활성화되고, 새로운 replicaset인 69c44dfb78이 등장했다. 새 replicaset은 3대의 pod를 유지하고 있다.\n$ kubectl describe deploy Name: nginx-deployment Namespace: default CreationTimestamp: Wed, 10 Nov 2021 00:23:05 +0900 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 2 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.9.1 주요 확인사항\n컨테이너의 이미지(Image:) 값이 nginx:1.9.1로 변경된 것을 확인. 변경 회수(revision) 값이 1에서 2로 증가했다. 즉 1번 deployment의 변경사항이 발생했다는 점이다. 3. 변경기록 확인 # 명령어 형식\n$ kubectl rollout history deployment \u0026lt;deployment 이름\u0026gt; 실제 명령어\n$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; 최초에 revision 1에서 시작한다. 변경사유(CHANGE-CAUSE) 값은 지정해주지 않았기 때문에 \u0026lt;none\u0026gt;으로 출력된다.\n4. 롤백 # 롤링 업데이트 중에 문제가 발생했을 경우, 롤링 업데이트 실행 전으로 즉시 되돌릴 수 있다.\n명령어 형식\n$ kubectl rollout undo deployment \u0026lt;deployment 이름\u0026gt; --to-revision=\u0026lt;숫자\u0026gt; 실제 명령어\ndeployment를 최초 버전(revision 1)으로 돌린다.\n$ kubectl rollout undo deployment nginx-deployment --to-revision=1 deployment.apps/nginx-deployment rolled back 롤백 후 상태확인\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deployment-5d59d67564-24cll 1/1 Running 0 51s pod/nginx-deployment-5d59d67564-9kgp8 1/1 Running 0 52s pod/nginx-deployment-5d59d67564-dvv8n 1/1 Running 0 50s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 5h23m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 12m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5d59d67564 3 3 3 12m replicaset.apps/nginx-deployment-69c44dfb78 0 0 0 12m nginx 이미지 버전을 1.9.1로 업그레이드 하면서 사용하지 않았던 5d59d67564 replicaset이 다시 pod 관리를 넘겨 받았다.\n여기서 가장 중요한 것은 pod 3대도 기존 pod가 아니라 새로 생성(교체)된 pod라는 사실이다.\ndeployment 상세정보 확인\n$ kubectl describe deploy Name: nginx-deployment Namespace: default CreationTimestamp: Wed, 10 Nov 2021 00:23:05 +0900 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 3 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx:1.7.9 Image: 값이 nginx:1.9.1에서 nginx:1.7.9로 rollback 되었다. revision 값도 +1 되어 3이다.\n5. 변경사유 기록 # 방법1. Annotation 수정\nAnnotation은 Label처럼 키:값 쌍으로 이루어져 있으며, 쿠버네티스 시스템이 필요한 정보들을 담고 있다. Annotation은 쿠버네티스 클라이언트나 라이브러리가 활용하는데 사용된다.\n명령어 형식\nchange-cause에 값을 부여해 수정하는 방식이다. annotation을 수정하는 방식이 현업에서 더 권장되고 안전하다.\n$ kubectl annotate deployment.v1.apps/\u0026lt;deployment 이름\u0026gt; kubernetes.io/change-cause=\u0026#34;\u0026lt;변경사유\u0026gt;\u0026#34; 실행 명령어\n$ kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause=\u0026#34;test update V2\u0026#34; deployment.apps/nginx-deployment annotated nginx-deployment의 change-cause 값이 변경되었다.\n변경결과 확인\ndescribe로 nginx-deployment의 상세정보를 확인해본다. 2번째 Annotation인 change-cause: test update V2 생성되어 있다.\n$ kubectl describe deploy nginx-deployment Name: nginx-deployment Namespace: default CreationTimestamp: Wed, 10 Nov 2021 18:54:18 +0900 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 1 kubernetes.io/change-cause: test update V2 롤링 업데이트 기록도 확인해본다. change-cause annotation의 값과 동일하게 적혀있다.\n$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 1 test update V2 방법2. 직접 수정\ndeployment의 설정이 담긴 manifest 파일을 직접 수정한다. 이 방식은 위험하므로 annotation을 수정하는 방식을 추천한다.\n$ kubectl edit deployment nginx-deployment deployment.apps/nginx-deployment edited rollout의 변경사유 기록을 위해 nginx-deployment의 설정파일을 연다.\n# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: apps/v1 kind: Deployment metadata: annotations: deployment.kubernetes.io/revision: \u0026#34;3\u0026#34; kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;nginx-deployment\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;replicas\u0026#34;:3,\u0026#34;selector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;template\u0026#34;:{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;nginx\u0026#34;}},\u0026#34;spec\u0026#34;:{\u0026#34;containers\u0026#34;:[{\u0026#34;image\u0026#34;:\u0026#34;nginx:1.7.9\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;containerPort\u0026#34;:80}]}]}}}} kubernetes.io/change-cause: nginx:1.7.9 creationTimestamp: \u0026#34;2021-11-09T15:23:05Z\u0026#34; generation: 5 annotations 아랫줄에 kubernets.io/change-cause: \u0026lt;사유\u0026gt;를 추가 입력해준다. manifest 파일을 수정 후 저장하면 즉시 적용되니 주의하자.\nrollout 기록을 다시 조회해본다.\n$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 2 \u0026lt;none\u0026gt; 3 nginx:1.7.9 annotations에 입력한 nginx:1.7.9 가 그대로 출력된다.\nrollout 재시작 # deployment에 속한 전체 pod를 재시작하는 방법\n명령어 형식\n$ kubectl rollout restart deployment/\u0026lt;deployment 이름\u0026gt; 실행 명령어\n$ kubectl rollout restart deployment/nginx-deployment deployment.apps/nginx-deployment restarted nginx-deployment가 재시작되었다.\npod, replicaset 상태 확인\n$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-deployment-5468f7bfc5-cpd95 1/1 Running 0 5s pod/nginx-deployment-5468f7bfc5-ds7sm 1/1 Running 0 4s pod/nginx-deployment-5468f7bfc5-x6jjq 1/1 Running 0 3s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6h8m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-deployment 3/3 3 3 57m NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-deployment-5468f7bfc5 3 3 3 5s replicaset.apps/nginx-deployment-5d59d67564 0 0 0 57m replicaset.apps/nginx-deployment-69c44dfb78 0 0 0 56m 새 replicaset인 5468f7bfc5가 생성되고, 새로운 pod 3대도 함께 생성된다.\nrollout 기록 확인\n$ kubectl rollout history deployment nginx-deployment deployment.apps/nginx-deployment REVISION CHANGE-CAUSE 2 \u0026lt;none\u0026gt; 3 nginx:1.7.9 4 nginx:1.7.9 rollout 재시작을 실행하면서 revision 3에서 4로 변경되었다. 새로 생성된 revision 4의 CHANGE-CAUSE 값은 revision 3과 동일하게 가져온다.\n","date":"Nov 10, 2021","permalink":"/blog/k8s/deployment-tutorial/","section":"Blogs","summary":"개요 # minikube 기반의 쿠버네티스 환경에서 deployment를 생성하고 제어하는 실습을 한다.","title":"쿠버네티스 deployment 실습"},{"content":"개요 # M1 CPU를 사용하는 macOS에서 minikube를 설치해 kubernetes 실습 환경을 구축합니다.\n이 방식은 하이퍼바이저로 Virtual Box나 Vagrant를 사용하지 않고, docker에 minikube를 올리는 방식입니다.\n환경 # Hardware : MacBook Pro (16\u0026quot;, M1 Pro, 2021) OS : macOS Monterey 12.4 패키지 관리자 : Homebrew 3.3.2 설치대상 Docker Desktop v4.1.1 minikube v1.25.2 본문 # 1. Docker 설치 # minikube를 사용하기 위해서는 로컬 머신에 docker desktop을 먼저 설치해야합니다.\nmacOS용 패키지 관리자인 Homebrew를 이용해 docker를 쉽게 설치할 수 있습니다.\n$ brew install --cask docker ==\u0026gt; Downloading https://desktop.docker.com/mac/main/arm64/69879/Docker.dmg Already downloaded: /Users/ive/Library/Caches/Homebrew/downloads/b5774f18ca8a6d3936c5174f91b93cb1a1a407daa784fe63d9b6300180c7b1ed--Docker.dmg ==\u0026gt; Installing Cask docker ==\u0026gt; Moving App \u0026#39;Docker.app\u0026#39; to \u0026#39;/Applications/Docker.app\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker-compose.bash-completion\u0026#39; to \u0026#39;/opt/homebrew/etc/bash_c ==\u0026gt; Linking Binary \u0026#39;docker.zsh-completion\u0026#39; to \u0026#39;/opt/homebrew/share/zsh/site-func ==\u0026gt; Linking Binary \u0026#39;docker.fish-completion\u0026#39; to \u0026#39;/opt/homebrew/share/fish/vendor_ ==\u0026gt; Linking Binary \u0026#39;docker-compose.fish-completion\u0026#39; to \u0026#39;/opt/homebrew/share/fish ==\u0026gt; Linking Binary \u0026#39;docker-compose.zsh-completion\u0026#39; to \u0026#39;/opt/homebrew/share/zsh/s ==\u0026gt; Linking Binary \u0026#39;docker.bash-completion\u0026#39; to \u0026#39;/opt/homebrew/etc/bash_completio 🍺 docker was successfully installed! docker 최초 설치시 오래걸리니 인내심을 갖고 기다립니다.\n$ brew list --cask docker iterm2 cask 목록에 docker가 설치된 걸 확인할 수 있습니다.\n런치패드에도 Docker 아이콘이 생성되었습니다.\n2. minikube 설치 # docker desktop 설치가 완료된 후 minikube를 설치합니다.\n$ brew install minikube ... ==\u0026gt; Summary 🍺 /opt/homebrew/Cellar/minikube/1.25.2: 9 files, 70.3MB ==\u0026gt; Running `brew cleanup minikube`... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`). minikube v1.25.2 버전이 설치 완료되었습니다.\n$ minikube version minikube version: v1.25.2 commit: 362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7 버전 확인 명령어가 잘 실행되는지 확인합니다.\n3. minikube 실행 # 로컬에 1대의 노드로 구성된 쿠버네티스 클러스터를 생성합니다.\n$ minikube start \\ --cni=\u0026#39;calico\u0026#39; \\ --driver=\u0026#39;docker\u0026#39; \\ --kubernetes-version=\u0026#39;stable\u0026#39; 명령어 옵션 설명\n--cni : 컨테이너 네트워크 인터페이스를 지정합니다.\n--driver : 쿠버네티스 클러스터를 구동할 하이퍼바이저를 지정합니다.\n--kubernetes-version : 생성되는 노드의 쿠버네티스 버전을 지정합니다.\n$ minikube start \\ --cni=\u0026#39;calico\u0026#39; \\ --driver=\u0026#39;docker\u0026#39; \\ --kubernetes-version=\u0026#39;stable\u0026#39; 😄 Darwin 12.4 (arm64) 의 minikube v1.25.2 ✨ 유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중 👍 minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중 🚜 베이스 이미지를 다운받는 중 ... 🔥 Creating docker container (CPUs=2, Memory=7903MB) ... 🐳 쿠버네티스 v1.23.3 을 Docker 20.10.12 런타임으로 설치하는 중 ▪ kubelet.housekeeping-interval=5m ▪ 인증서 및 키를 생성하는 중 ... ▪ 컨트롤 플레인이 부팅... ▪ RBAC 규칙을 구성하는 중 ... 🔗 Configuring Calico (Container Networking Interface) ... 🔎 Kubernetes 구성 요소를 확인... ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5 🌟 애드온 활성화 : storage-provisioner, default-storageclass 🏄 끝났습니다! kubectl이 \u0026#34;minikube\u0026#34; 클러스터와 \u0026#34;default\u0026#34; 네임스페이스를 기본적으로 사용하도록 구성되었습니다. 쿠버네티스 클러스터가 생성된 걸 확인할 수 있습니다.\n쿠버네티스 노드의 상태를 확인할 수 있습니다.\n$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured Profile로 환경을 나눠서 여러개의 minikube 클러스터를 동시에 사용할 수도 있습니다.\nProfile 이름을 지정하지 않을 경우 기본 Profile인 minikube가 생성됩니다.\n$ minikube profile list |----------|-----------|---------|--------------|------|---------|---------|-------| | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | |----------|-----------|---------|--------------|------|---------|---------|-------| | minikube | docker | docker | 192.168.49.2 | 8443 | v1.23.3 | Running | 1 | |----------|-----------|---------|--------------|------|---------|---------|-------| 4. minikube 상태 확인 # docker 확인\n도커에서 실행되는 minikube 컨테이너를 확인할 수 있습니다.\nminikube dashboard 확인\nminikube를 설치하면 기본적으로 dashboard가 비활성화되어 있습니다.\n$ minikube addons list |-----------------------------|----------|--------------|--------------------------------| | ADDON NAME | PROFILE | STATUS | MAINTAINER | |-----------------------------|----------|--------------|--------------------------------| | ambassador | minikube | disabled | third-party (ambassador) | | auto-pause | minikube | disabled | google | | csi-hostpath-driver | minikube | disabled | kubernetes | | dashboard | minikube | disabled | kubernetes | | default-storageclass | minikube | enabled ✅ | kubernetes | | efk | minikube | disabled | third-party (elastic) | | freshpod | minikube | disabled | google | | gcp-auth | minikube | disabled | google | | gvisor | minikube | disabled | google | | helm-tiller | minikube | disabled | third-party (helm) | | ingress | minikube | disabled | unknown (third-party) | | ingress-dns | minikube | disabled | google | | istio | minikube | disabled | third-party (istio) | | istio-provisioner | minikube | disabled | third-party (istio) | | kong | minikube | disabled | third-party (Kong HQ) | | kubevirt | minikube | disabled | third-party (kubevirt) | | logviewer | minikube | disabled | unknown (third-party) | | metallb | minikube | disabled | third-party (metallb) | | metrics-server | minikube | disabled | kubernetes | | nvidia-driver-installer | minikube | disabled | google | | nvidia-gpu-device-plugin | minikube | disabled | third-party (nvidia) | | olm | minikube | disabled | third-party (operator | | | | | framework) | | pod-security-policy | minikube | disabled | unknown (third-party) | | portainer | minikube | disabled | portainer.io | | registry | minikube | disabled | google | | registry-aliases | minikube | disabled | unknown (third-party) | | registry-creds | minikube | disabled | third-party (upmc enterprises) | | storage-provisioner | minikube | enabled ✅ | google | | storage-provisioner-gluster | minikube | disabled | unknown (third-party) | | volumesnapshots | minikube | disabled | kubernetes | |-----------------------------|----------|--------------|--------------------------------| 기본 비활성화 되어 있는 kubernetes dashboard 애드온을 활성화 합니다.\n$ minikube dashboard 🔌 대시보드를 활성화하는 중 ... ▪ Using image kubernetesui/dashboard:v2.3.1 ▪ Using image kubernetesui/metrics-scraper:v1.0.7 🤔 Verifying dashboard health ... 🚀 프록시를 시작하는 중 ... 🤔 Verifying proxy health ... 🎉 Opening http://127.0.0.1:62073/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser... 자동으로 브라우저 창이 열리면서 Kubernetes dashboard로 이동됩니다.\n아직 아무것도 배포하지 않은 초기화 상태의 클러스터라 표시할 내용이 없습니다.\n$ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME minikube Ready control-plane,master 15m v1.23.3 192.168.49.2 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.10.104-linuxkit docker://20.10.12 minikube 노드 1대가 실행중입니다.\n노드가 1대일 경우 control-plane 역할과 worker node 역할을 같이 수행하게 됩니다.\n5. 테스트 pod 생성 # 파드\n파드Pod는 쿠버네티스에서 가장 최소한의 오브젝트 단위입니다.\n1개의 파드는 최소 1개 이상의 컨테이너Container로 구성됩니다.\nYAML 작성\n현재 경로에 sample-pod.yaml 파일을 생성합니다.\n$ cat \u0026lt;\u0026lt;EOF \u0026gt; ./sample-pod.yaml apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026#39;] EOF sample-pod.yaml은 1개의 파드를 배포하는 매니페스트입니다.\n파드에 들어있는 busybox 컨테이너는 Hello Kubernetes! 메세지를 출력하고 3600초 후 종료되도록 동작합니다.\n작성한 매니페스트를 사용해서 파드를 배포합니다.\n$ kubectl apply -f sample-pod.yaml pod/myapp-pod created 6. 파드 동작 확인 # 파드 상태를 확인합니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myapp-pod 1/1 Running 0 67s 172.17.0.11 minikube \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; myapp-pod가 정상 동작중인 걸 확인할 수 있습니다.\n파드의 로그를 확인합니다.\n$ kubectl logs pod/myapp-pod Hello Kubernetes! yaml 파일에 작성한대로 myapp-pod가 Hello Kubernetes!를 출력했습니다.\n실습환경 정리 # 방법 1. 클러스터 종료 # 쿠버네티스 클러스터를 종료합니다.\n중요: 클러스터 삭제가 아니라 종료이기 때문에 생성한 클러스터와 리소스는 보존됩니다.\n언제든지 minikube start 명령어로 클러스터를 재시작하면 모든 리소스는 다시 복구됩니다.\n리소스 삭제하기 # 모든 pod를 삭제합니다.\n$ kubectl delete pod --all pod \u0026#34;myapp-pod\u0026#34; deleted myapp-pod가 삭제되었습니다.\n$ kubectl get pod -o wide No resources found in default namespace. 파드를 삭제한 후 아무런 pod도 조회되지 않습니다.\n모든 파드가 정상적으로 삭제되었습니다.\n클러스터 종료 # minikube 클러스터를 종료합니다.\n클러스터를 종료한다고 해서 배포했던 리소스들이 삭제되지 않고 다시 시작했을 때 그대로 보존됩니다.\n$ minikube stop ✋ Stopping node \u0026#34;minikube\u0026#34; ... 🛑 Powering off \u0026#34;minikube\u0026#34; via SSH ... 🛑 1 node stopped. 다음에 minikube 클러스터를 다시 시작하고 싶다면 minikube start 명령어를 실행하면 됩니다.\n$ minikube status minikube type: Control Plane host: Stopped kubelet: Stopped apiserver: Stopped kubeconfig: Stopped minikube 클러스터가 중지된 상태Stopped입니다.\n방법 2. 클러스터 삭제 # minikube 클러스터와 그 안에 생성된 모든 리소스를 제거합니다.\n클러스터 삭제 # 삭제하기 전에 프로파일을 확인합니다.\n$ minikube profile list |----------|-----------|---------|--------------|------|---------|---------|-------| | Profile | VM Driver | Runtime | IP | Port | Version | Status | Nodes | |----------|-----------|---------|--------------|------|---------|---------|-------| | minikube | docker | docker | 192.168.49.2 | 8443 | v1.23.3 | Running | 1 | |----------|-----------|---------|--------------|------|---------|---------|-------| 디폴트 프로파일인 minikube의 클러스터를 삭제합니다.\n$ minikube delete 🔥 docker 의 \u0026#34;minikube\u0026#34; 를 삭제하는 중 ... 🔥 Deleting container \u0026#34;minikube\u0026#34; ... 🔥 /Users/steve/.minikube/machines/minikube 제거 중 ... 💀 \u0026#34;minikube\u0026#34; 클러스터 관련 정보가 모두 삭제되었습니다 결과 확인 # 클러스터 삭제가 완료된 후 아래 명령어로 프로파일 목록을 확인합니다.\n$ minikube profile list 🤹 Exiting due to MK_USAGE_NO_PROFILE: No minikube profile was found. 💡 권장: You can create one using \u0026#39;minikube start\u0026#39;. minikube 프로파일이 삭제되어 조회되지 않는 걸 확인할 수 있습니다.\nkubectl 명령어로 쿠버네티스 노드를 확인해도 결과는 마찬가지입니다.\n$ kubectl get node The connection to the server localhost:8080 was refused - did you specify the right host or port? 모든 실습환경이 정리되었습니다.\n더 나아가서 # 1대의 노드가 아닌 여러대의 노드로 실습해보고 싶다면 아래 글도 읽어보는 걸 추천합니다.\nminikube 멀티노드 구성\n","date":"Nov 9, 2021","permalink":"/blog/k8s/installing-minikube/","section":"Blogs","summary":"개요 # M1 CPU를 사용하는 macOS에서 minikube를 설치해 kubernetes 실습 환경을 구축합니다.","title":"minikube 설치"},{"content":"개요 # 패키지 관리자인 Homebrew를 이용해 autojump 플러그인과 neofetch를 설치하고 적용하는 방법을 설명합니다.\n환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 Terminal : iTerm2 + zsh with oh-my-zsh 패키지 관리자 : Homebrew 3.3.2 설치대상 autojump 22.5.3 neofetch 7.1.0 본문 # 1. autojump # autojump는 내가 이전에 이동했던 경로를 기억해놓았다가 해당 경로로 단번에 이동(Jump) 할 수 있게 해주는 기능의 플러그인이다.\n(1) brew 설치 목록 확인 # $ brew list ==\u0026gt; Formulae bat\tlibevent\tneovim bdw-gc\tlibffi\tnettle ca-certificates\tlibidn2\topenssl@1.1 cask\tlibnghttp2\tp11-kit coreutils\tlibtasn1\tpcre emacs\tlibtermkey\tpkg-config fzf\tlibtool\treadline gettext\tlibunistring\ttree-sitter gmp\tlibuv\tunbound gnutls\tluajit-openresty\tunibilium guile\tluv\tzsh hugo\tm4\tzsh-completions jansson\tmsgpack kubernetes-cli\tncurses ==\u0026gt; Casks docker\titerm2 brew에 설치된 소프트웨어 목록을 확인한다. autojump라는 이름의 소프트웨어는 확인되지 않는다.\n(2) 설치 # \u0026gt; brew install autojump [...] ==\u0026gt; /opt/homebrew/Cellar/python@3.10/3.10.0_2/bin/python3 -m pip install -v --no-deps --no-index --upgrade --isolated --target=/opt/homebrew/lib/python3.10/site-packages /opt/hom 🍺 /opt/homebrew/Cellar/python@3.10/3.10.0_2: 3,135 files, 57.6MB ==\u0026gt; Installing autojump ==\u0026gt; Pouring autojump--22.5.3_3.arm64_monterey.bottle.tar.gz ==\u0026gt; Caveats Add the following line to your ~/.bash_profile or ~/.zshrc file: [ -f /opt/homebrew/etc/profile.d/autojump.sh ] \u0026amp;\u0026amp; . /opt/homebrew/etc/profile.d/autojump.sh If you use the Fish shell then add the following line to your ~/.config/fish/config.fish: [ -f /opt/homebrew/share/autojump/autojump.fish ]; and source /opt/homebrew/share/autojump/autojump.fish Restart your terminal for the settings to take effect. zsh completions have been installed to: /opt/homebrew/share/zsh/site-functions ==\u0026gt; Summary 🍺 /opt/homebrew/Cellar/autojump/22.5.3_3: 20 files, 170.7KB ==\u0026gt; Caveats ==\u0026gt; autojump Add the following line to your ~/.bash_profile or ~/.zshrc file: [ -f /opt/homebrew/etc/profile.d/autojump.sh ] \u0026amp;\u0026amp; . /opt/homebrew/etc/profile.d/autojump.sh If you use the Fish shell then add the following line to your ~/.config/fish/config.fish: [ -f /opt/homebrew/share/autojump/autojump.fish ]; and source /opt/homebrew/share/autojump/autojump.fish Restart your terminal for the settings to take effect. zsh completions have been installed to: /opt/homebrew/share/zsh/site-functions autojump 설치가 잘 됐는지 확인한다.\n$ brew list 23:12:50 ==\u0026gt; Formulae autojump\tfzf\tjansson\tlibtermkey\tmpdecimal\tp11-kit\ttree-sitter [...] autojump가 설치목록에 포함된 상태이다.\n(3) 플러그인 추가 # $ cat ~/.zshrc [...] plugins=( git zsh-syntax-highlighting zsh-autosuggestions ) 기존 zsh 설정파일이다. 아랫줄에 autojump 플러그인을 사용하도록 추가한다.\n$ vi ~/.zshrc [...] plugins=( git zsh-syntax-highlighting zsh-autosuggestions autojump ) 변경사항을 저장한다.\n(4) 적용 # $ source ~/.zshrc .zshrc 파일의 변경사항을 즉시 적용한다.\n(5) 동작 테스트 # $ cd /Users/ive/githubrepos/blog/content/blog $ cd / autojump 테스트를 위해 각자 환경에서 깊숙한 경로까지 한번 방문한 후 최상단 디렉토리인 root directory(/)로 이동한다.\nj \u0026lt;점프할 디렉토리명\u0026gt; 명령어를 입력해서 autojump를 사용할 수 있다.\n$ pwd / $ j blog /Users/ive/githubrepos/blog/content/blog $ pwd /Users/ive/githubrepos/blog/content/blog 단번에 root directory(/)에서 블로그 레포지터리로 이동했다.\n2. neofetch # neofetch는 터미널 창에서 컴퓨터와 OS에 대한 유용한 정보를 제공해주는 툴이다.\n(1) brew 설치 목록 확인 # $ brew list ==\u0026gt; Formulae bat\tlibevent\tneovim bdw-gc\tlibffi\tnettle ca-certificates\tlibidn2\topenssl@1.1 cask\tlibnghttp2\tp11-kit coreutils\tlibtasn1\tpcre emacs\tlibtermkey\tpkg-config fzf\tlibtool\treadline gettext\tlibunistring\ttree-sitter gmp\tlibuv\tunbound gnutls\tluajit-openresty\tunibilium guile\tluv\tzsh hugo\tm4\tzsh-completions jansson\tmsgpack kubernetes-cli\tncurses ==\u0026gt; Casks docker\titerm2 brew에 설치된 소프트웨어 목록을 확인한다. neofetch라는 이름의 소프트웨어는 확인되지 않음.\n(2) brew 검색 # $ brew search neofetch ==\u0026gt; Formulae neofetch onefetch 검색 결과에서 neofetch라는 소프트웨어가 있다. neofetch를 설치해본다.\n(3) neofetch 설치 # $ brew install neofetch ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/screenresolution/manifests/1.6 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/screenresolution/blobs/sha256:3 ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/neofetch/manifests/7.1.0-2 ######################################################################## 100.0% ==\u0026gt; Downloading https://ghcr.io/v2/homebrew/core/neofetch/blobs/sha256:78eb3e99d ==\u0026gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh ######################################################################## 100.0% ==\u0026gt; Installing dependencies for neofetch: screenresolution ==\u0026gt; Installing neofetch dependency: screenresolution ==\u0026gt; Pouring screenresolution--1.6.arm64_monterey.bottle.tar.gz 🍺 /opt/homebrew/Cellar/screenresolution/1.6: 5 files, 57.7KB ==\u0026gt; Installing neofetch ==\u0026gt; Pouring neofetch--7.1.0.all.bottle.2.tar.gz 🍺 /opt/homebrew/Cellar/neofetch/7.1.0: 6 files, 350.6KB neofetch가 7.1.0 버전으로 정상 설치되었다.\n(4) 적용 # $ vi ~/.zshrc [...] export PATH=/opt/homebrew/bin:/Library/Frameworks/Python.framework/Versions/3.9/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin neofetch zsh이 실행될 때 마지막에 neofetch 명령어를 실행되도록 마지막 라인에 neofetch를 입력한다.\n이제 iTerm2를 실행시키면 zsh이 실행되고, 마지막 과정에 neofetch가 실행된다.\n(5) 결과확인 # 새 터미널창이 열릴 때마다 neofetch 명령어가 실행되어 디테일한 하드웨어 스펙, OS 정보를 표출해준다.\n좌측에 나오는 ASCII 그림은 원하는 이미지로 설정이 가능하다.\n기본값은 각 운영체제의 로고이다. MacBook의 경우 기본 값으로 Apple 로고가 출력된다.\n마치며 # 댓글로 유용한 zsh 플러그인을 추천해주면 내가 직접 써보고 판단한 후 이 글에 업데이트 하겠다.\n코딩하는거 기왕이면 재미있고 편하게 하길 바라면서 글을 마친다. 끝!\n","date":"Nov 8, 2021","permalink":"/blog/installing-zsh-plugins/","section":"Blogs","summary":"개요 # 패키지 관리자인 Homebrew를 이용해 autojump 플러그인과 neofetch를 설치하고 적용하는 방법을 설명합니다.","title":"zsh 플러그인 설치"},{"content":"개요 # macOS 로컬환경에서 Kubernetes 실습을 할 수 있도록 쿠버네티스 설치 과정을 설명한다.\n환경 # Hardware : MacBook Pro (13\u0026quot;, M1, 2020) OS : macOS Monterey 12.0.1 패키지 관리자 : Homebrew 3.3.2 Docker Desktop 4.1.1 + Kubernetes v1.21.5 절차 # 1. Docker 설치 # macOS용 패키지 관리자인 Homebrew를 이용해 docker를 설치한다. 쿠버네티스를 사용하기 위해서는 docker를 먼저 설치해야한다.\n$ brew install --cask docker ==\u0026gt; Downloading https://desktop.docker.com/mac/main/arm64/69879/Docker.dmg Already downloaded: /Users/ive/Library/Caches/Homebrew/downloads/b5774f18ca8a6d3936c5174f91b93cb1a1a407daa784fe63d9b6300180c7b1ed--Docker.dmg ==\u0026gt; Installing Cask docker ==\u0026gt; Moving App \u0026#39;Docker.app\u0026#39; to \u0026#39;/Applications/Docker.app\u0026#39; ==\u0026gt; Linking Binary \u0026#39;docker-compose.bash-completion\u0026#39; to \u0026#39;/opt/homebrew/etc/bash_c ==\u0026gt; Linking Binary \u0026#39;docker.zsh-completion\u0026#39; to \u0026#39;/opt/homebrew/share/zsh/site-func ==\u0026gt; Linking Binary \u0026#39;docker.fish-completion\u0026#39; to \u0026#39;/opt/homebrew/share/fish/vendor_ ==\u0026gt; Linking Binary \u0026#39;docker-compose.fish-completion\u0026#39; to \u0026#39;/opt/homebrew/share/fish ==\u0026gt; Linking Binary \u0026#39;docker-compose.zsh-completion\u0026#39; to \u0026#39;/opt/homebrew/share/zsh/s ==\u0026gt; Linking Binary \u0026#39;docker.bash-completion\u0026#39; to \u0026#39;/opt/homebrew/etc/bash_completio 🍺 docker was successfully installed! docker 최초 설치시 오래걸리니 인내심을 갖고 기다린다.\n$ brew list --cask docker iterm2 cask 목록에 docker가 설치되었다.\n런치패드에도 Docker 아이콘이 생성됐다.\n2. kubernetes 활성화 # 도커가 정상 설치되었다면 상단바에 Docker Desktop 아이콘이 나타난다.\n상단바 Docker 아이콘 클릭 → 환경설정(Preferences) 클릭 Kubernetes → Enable Kubernetes 체크 → Apply \u0026amp; Restart 3. kubernetes 상태 확인 # 클러스터 정보 확인\n$ kubectl cluster-info Kubernetes control plane is running at https://kubernetes.docker.internal:6443 CoreDNS is running at https://kubernetes.docker.internal:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. kubectl 버전 확인\nkubectl은 쿠버네티스 클러스터를 제어하기 위한 커맨드 라인 도구이다.\n$ kubectl version Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;22\u0026#34;, GitVersion:\u0026#34;v1.22.3\u0026#34;, GitCommit:\u0026#34;c92036820499fedefec0f847e2054d824aea6cd1\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-10-27T18:34:20Z\u0026#34;, GoVersion:\u0026#34;go1.16.10\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;darwin/arm64\u0026#34;} Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;21\u0026#34;, GitVersion:\u0026#34;v1.21.5\u0026#34;, GitCommit:\u0026#34;aea7bbadd2fc0cd689de94a54e5b7b758869d691\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2021-09-15T21:04:16Z\u0026#34;, GoVersion:\u0026#34;go1.16.8\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/arm64\u0026#34;} 4. pod 스펙 작성 # kubernetes에서 오브젝트를 생성하려면 오브젝트에 대한 기본적인 정보와 함께 의도한 상태를 기술한 오브젝트 스펙(spec)을 제시해야한다.\n오브젝트를 생성하기 위한 작성 포맷은 YAML 혹은 JSON을 사용한다. 일반적으로 YAML이 다른 포맷보다 가독성이 좋고 작성도 편해 많이들 사용한다.\n쿠버네티스 구성요소의 최소 단위는 포드(Pod)이다. 1개의 pod는 1개 이상의 컨테이너로 구성된다.\n아래는 pod를 배포하는 간단한 yaml 샘플 코드이다. vi 에디터를 이용해 그대로 작성해 저장한다.\n$ cat sample-pod.yaml apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo Hello Kubernetes! \u0026amp;\u0026amp; sleep 3600\u0026#39;] 5. pod 배포 # 작성한 포드 스펙(sample-pod.yaml)을 적용한다.\n$ kubectl apply -f sample-pod.yaml pod/myapp-pod created 실행 결과로 pod/myapp-pod created 메세지가 출력되면 pod가 정상 생성된 것이다.\n6. pod 상태확인 # 포드 정보를 간단히 출력\n$ kubectl get pods NAME READY STATUS RESTARTS AGE myapp-pod 0/1 ContainerCreating 0 3s pod의 컨테이너가 생성중(ContainerCreating)인 상태다.\n몇 초 후에 pod 상태를 다시 조회해본다.\n$ kubectl get pods NAME READY STATUS RESTARTS AGE myapp-pod 1/1 Running 0 5s 현재는 pod에 컨테이너가 생성된 후 실행중(Running)이다.\n$ kubectl get po pods 키워드의 축약형인 po로 입력해도 된다. (pods 말고 다른 오브젝트들도 모두 축약형이 존재한다. deployments는 deploy, replicasets는 rs이다.)\nCLI 환경에서는 한 글자라도 덜 치는게 고효율로 가는 지름길이다.\n포드 정보 자세히 출력\nkubectl get pod 명령어 뒤에 -o wide 옵션을 붙이면 IP, NODE, NOMINATED NODE, READINESS GATES 4가지 정보를 더 보여준다.\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES myapp-pod 1/1 Running 0 11s 10.1.0.8 docker-desktop \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7. 배포 결과확인 # $ kubectl logs pod/myapp-pod Hello Kubernetes! 특정 pod의 로그를 확인한다. sample-pod.yaml 파일에 작성한대로 pod가 echo Hello Kubernetes!를 실행했다.\n8. pod 접속 # myapp-pod의 myapp-container 안으로 shell 접속을 시도해본다.\n옛날 버전의 kubernetes에서는 pod 접속시 kubectl exec -it [POD] [COMMAND] 명령어를 사용했었다. 하지만 해당 명령어는 신규 명령어로 대체(depreacted) 되어 곧 다음 버전에서 사라질 예정이다. 명령어 사용에 주의한다.\n$ kubectl exec -it myapp-pod /bin/sh -c myapp-container kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. 명령어 형식\nkubernetes 권고사항에 따라 아래의 신규 kubectl exec 명령어를 사용하도록 하자.\n$ kubectl exec \u0026lt;POD_NAME\u0026gt; -c \u0026lt;CONTAINER_NAME\u0026gt; -it -- \u0026lt;COMMAND\u0026gt; 실제 명령어\n$ kubectl exec myapp-pod -c myapp-container -it -- /bin/sh $ kubectl exec myapp-pod -c myapp-container -it -- /bin/sh # --- 여기서부터는 컨테이너의 shell 환경에 진입한 상태이다. --- / # hostname myapp-pod / # ifconfig eth0 Link encap:Ethernet HWaddr 5E:DC:25:DF:28:AA inet addr:10.1.0.9 Bcast:10.1.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:14 errors:0 dropped:0 overruns:0 frame:0 TX packets:1 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:1048 (1.0 KiB) TX bytes:42 (42.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 컨테이너 상태 확인이 끝났다면 Ctrl + p, q 키(Escape Sequence)를 입력해서 container의 shell에서 빠져나온다.\nexec attach failed: error on attach stdin: read escape sequence command terminated with exit code 126 $ 9. 포드 삭제 # 학습이 끝났으니 생성된 pod를 삭제하고 정리하자.\nkubectl delete pod 명령어를 이용해 포드를 지정해 삭제를 할수도 있지만, 깔끔한 삭제를 위해서는 -f 옵션을 이용해 yaml 파일에 선언된 설정을 기반으로 삭제하는 걸 권장한다.\n$ kubectl delete -f sample-pod.yaml pod \u0026#34;myapp-pod\u0026#34; deleted 삭제 소요시간은 약 20초 걸린다. 설정파일에 선언된 myapp-pod 포드가 정상 삭제됐다.\n정말로 포드가 삭제되었는지 확인해보자.\n$ kubectl get po No resources found in default namespace. 아무런 포드도 조회되지 않는다. 잘 삭제되었다. 끝!\n","date":"Nov 7, 2021","permalink":"/blog/k8s/installing-k8s-on-mac/","section":"Blogs","summary":"개요 # macOS 로컬환경에서 Kubernetes 실습을 할 수 있도록 쿠버네티스 설치 과정을 설명한다.","title":"Kubernetes 설치"},{"content":"개요 # 모니터링 대상인 리눅스 서버에 ITSM(IT Service Management) 모니터링 에이전트를 설치할 수 있다.\n환경 # OS : Red Hat Enterprise Linux server release 7.x Architecture : x86_64 에이전트 SW : ITSM 모니터링 에이전트 (zagent-rhes3-x64_254) 해결법 # A) 에이전트 설치 # 반드시 root 계정으로 ITSM 에이전트 설치를 진행한다.\n1. zagent 설치파일 준비 # $ arch x86_64 해당 서버는 64bit 기반 아키텍쳐(x86_64)이다.\n모니터링 대상서버의 아키텍쳐와 동일한 버전의 에이전트 설치파일을 모니터링 대상 서버의 파일시스템에 업로드한다.\n$ chmod 700 zagent-rhes3-x64_254 $ chown root:root zagent-rhes3-x64_254 설치파일의 권한을 700(rwx --- ---), 소유자 root, 그룹 root로 설정한다.\n$ ls -lh [...] -rwx------ 1 root root 1.1M 813 2010 zagent-rhes3-x64_254 $ mv zagent-rh78-i386_254 /usr/sbin/zagent ITSM 에이전트 설치파일의 이름을 zagent로 변경한다. 설치파일을 이동시킬 경로는 반드시 /usr/sbin/ 이어야 한다.\n2. zagent 설치 # $ zagent -install Excute user \u0026lt;default: root\u0026gt; ?: [ENTER] INSTALL: \u0026#39;zagent\u0026#39;, Path : \u0026#39;/sbin\u0026#39;, User : \u0026#39;root\u0026#39; Link /etc/rc.d/init.d/zagent /etc/rc.d/rc2.d/S99zagent Link /etc/rc.d/init.d/zagent /etc/rc.d/rc3.d/S99zagent Link /etc/rc.d/init.d/zagent /etc/rc.d/rc4.d/S99zagent Link /etc/rc.d/init.d/zagent /etc/rc.d/rc5.d/S99zagent Install completed 엔터를 입력해 default 계정(root)으로 설치를 실행한다.\n문제해결\n증상 : zagent -install 명령어로 설치 시도시 /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory 에러가 발생하며 설치가 불가능함 원인 : 설치파일과 서버간 아키텍쳐 호환성 불일치 (64비트 기반 아키텍쳐 서버에 32비트 기반 아키텍쳐용 설치파일 시도) 조치방법 : 64비트 기반 아키텍쳐(x86_64)용 설치파일로 설치를 진행한다. 3. manager 서버 등록 # 명령어 형식\n$ zagent -a admin \u0026lt;ITSM_MANAGER_SERVER1_IP\u0026gt; \u0026lt;ITSM_MANAGER_SERVER2_IP\u0026gt; 실행 명령어\n$ zagent -a admin 50.50.50.101 50.50.50.102 user 0, /var/zagent: created ZENIUS_PROGRAM_PATH= File Path: \u0026#39;/sbin\u0026#39; Current Path: \u0026#39;/var/zagent\u0026#39; 4. manager 서버 연결 테스트 # $ zagent -c ZENIUS_PROGRAM_PATH= File Path: \u0026#39;/sbin\u0026#39; Current Path: \u0026#39;/var/zagent\u0026#39; Try connect to Primary Manager[50.50.50.101:5052] : Success Try connect to Secondary Manager[50.50.50.102:5052] : Success 모니터링 대상서버가 ITSM Manager 서버의 TCP 5052 포트와 통신이 되어야 한다.\n+---------------+ 5052 +---------------+ | target server |---------\u0026gt;| ITSM Manager1 | +---------------+ +---------------+ | | 5052 +---------------+ +-----------------\u0026gt;| ITSM Manager2 | +---------------+ 접속 테스트 결과가 Success 가 아닐 경우, 방화벽과 같은 보안장비나 모니터링 대상 서버의 자체 방화벽(iptables) 정책을 확인한다.\n5. zagent 프로세스 시작 # $ zagent -start ZENIUS_PROGRAM_PATH= File Path: \u0026#39;/sbin\u0026#39; Current Path: \u0026#39;/var/zagent\u0026#39; zagent: started (153799) SetMemLimit: 200 MB (OK) SetFDLimit: 1024 (OK) SetCoreLimit: 0 MB (OK) Current Stack Size: 8388608 bytes Hostname: devserver1 IP: 192.168.0.15 MAC Address: xx:xx:xx:46:15:01 OS: Red Hat Enterprise Linux Server release 7.x 6. 프로세스 상태 확인 # $ ps -ef | grep zagent root 153799 1 0 08:48 ? 00:00:00 zagent -start root 153800 153799 2 08:48 ? 00:00:00 zagent -start root 154157 140259 0 08:48 pts/1 00:00:00 grep --color=auto zagent zagent 프로세스가 정상 동작중이다.\nB) 모니터링 대상의 수집되는 IP 수정 # ITSM 웹페이지에서 모니터링 대상 서버의 IP가 실제 IP와 다르게 수집되는 경우가 있다. 이 경우에만 아래 과정을 수행한다. 수집되는 IP가 실제 IP와 동일하다면 해당 과정을 넘겨도 된다.\n1. 에이전트 중지 # zagent 프로세스가 이미 실행중인 상태라면 설정파일을 수정하기 전에 먼저 zagent -stop 명령어로 ITSM 에이전트 프로세스를 종료한다.\n$ zagent -stop ZENIUS_PROGRAM_PATH= File Path: \u0026#39;/sbin\u0026#39; Current Path: \u0026#39;/var/zagent\u0026#39; Sending signal pid[0][153799], signum[15] Sending signal pid[1][153800], signum[15] zagent is stopped 2. zagent 설정파일 확인 # zagent 설정파일의 기본 경로는 /var/zagent/zagent.ini이다.\nLOCAL_IP 값에 실제 서버 IP를 입력한다.\n$ vi /var/zagent/zagent.ini USE_WATCHDOG = 1 RESTART_LIMIT = 0 USERID = admin MANAGERIP1 = 50.50.50.101 MANAGERPORT1 = 5052 MANAGERIP2 = 50.50.50.102 MANAGERPORT2 = 5052 ZAGENT_CPU_LIMIT = 50 ZAGENT_MEM_LIMIT = 200 ZAGENT_MAX_CPUTIME = 5 ZAGENT_DISK_LIMIT = 100 ZAGENT_GET_CDROM = 0 SMS_GET_THERM = 1 ZAGENT_CORE_LIMIT = 0 ZAGENT_FD_LIMIT = 1024 LOCALIP = PRO_DISKIO = 0 SMS_GET_DISKINV = 1 NETSTAT_SLEEPCOUNT = 500 NETSTAT_SLEEPTIME = 100 AIXMEM_USEDASCOMP = 0 MAX_WORK_THREADS = 5 MAX_CONNECTION = 6 PROACTIVE_SUPPORT = off HOSTID = 46500 SAVEONOFF = 0 SAVEPERIOD = 0 MANAGERTYPE = 1 USER_HOSTNAME = devserver1 3. zagent 설정파일 수정 # LOCAL_IP 값을 실제 서버의 IP로 입력했다.\n$ vi /var/zagent/zagent.ini USE_WATCHDOG = 1 RESTART_LIMIT = 0 USERID = admin MANAGERIP1 = 50.50.50.101 MANAGERPORT1 = 5052 MANAGERIP2 = 50.50.50.102 MANAGERPORT2 = 5052 ZAGENT_CPU_LIMIT = 50 ZAGENT_MEM_LIMIT = 200 ZAGENT_MAX_CPUTIME = 5 ZAGENT_DISK_LIMIT = 100 ZAGENT_GET_CDROM = 0 SMS_GET_THERM = 1 ZAGENT_CORE_LIMIT = 0 ZAGENT_FD_LIMIT = 1024 LOCALIP = 50.50.50.199 PRO_DISKIO = 0 SMS_GET_DISKINV = 1 NETSTAT_SLEEPCOUNT = 500 NETSTAT_SLEEPTIME = 100 AIXMEM_USEDASCOMP = 0 MAX_WORK_THREADS = 5 MAX_CONNECTION = 6 PROACTIVE_SUPPORT = off HOSTID = 46500 SAVEONOFF = 0 SAVEPERIOD = 0 MANAGERTYPE = 1 USER_HOSTNAME = devserver1 4. zagent 프로세스 시작 # $ zagent -start ZENIUS_PROGRAM_PATH= File Path: \u0026#39;/sbin\u0026#39; Current Path: \u0026#39;/var/zagent\u0026#39; zagent: started (158846) SetMemLimit: 200 MB (OK) SetFDLimit: 1024 (OK) SetCoreLimit: 0 MB (OK) Current Stack Size: 8388608 bytes Hostname: devserver1 IP: 50.50.50.199 MAC Address: xx:xx:xx:46:15:01 OS: Red Hat Enterprise Linux Server release 7.x 모니터링 대상으로부터 수집된 IP 정보가 실제 서버의 IP와 동일하게 변경된 상태를 확인했다.\nIP: 50.50.50.199 ","date":"Nov 5, 2021","permalink":"/blog/installing-zagent-on-linux/","section":"Blogs","summary":"개요 # 모니터링 대상인 리눅스 서버에 ITSM(IT Service Management) 모니터링 에이전트를 설치할 수 있다.","title":"zagent 모니터링 에이전트 설치"},{"content":"개요 # 리눅스의 부팅순서를 설명한다.\n서버의 전원 버튼을 누르고 몇 분이 지나면 로그인 프롬프트가 뜬다는 사실은 모두가 알고 있다. 우리가 전원 버튼을 누르고 로그인 프롬프트가 뜨기 전 까지의 과정을 처음부터 따라가보자.\n참고로 리눅스의 부팅순서, RAID의 각 레벨별 설명은 시스템 엔지니어 면접시 단골 질문이니 중요도가 높다.\n리눅스 부팅순서 6단계 # +---+----------+-------------------------------------------------+ | 1 | BIOS | BIOS가 MBR을 실행한다. | +---+----------+-------------------------------------------------+ | 2 | MBR | MBR(Master Boot Record)가 GRUB을 실행한다. | +---+----------+-------------------------------------------------+ | 3 | GRUB | GRUB(Grand Unified Bootloader)가 Kernel을 실행한다.| +---+----------+-------------------------------------------------+ | 4 | Kernel | Kernel이 /sbin/init을 실행한다. | +---+----------+-------------------------------------------------+ | 5 | Init | Init 프로세스가 런레벨 프로그램들을 실행한다. | +---+----------+-------------------------------------------------+ | 6 | Runlevel | 런레벨 프로그램들이 /etc/rc.d/rc*.d 를 실행한다. | +---+----------+-------------------------------------------------+ 본론 # 1. BIOS # BIOS는 기본 입출력 시스템(Basic Input/Output System)의 약자이다. 일부 시스템 무결성 검사를 수행한다. 부트로더 프로그램을 검색, 로드 및 실행한다. BIOS는 플로피, CD-ROM 또는 하드 드라이브에서 부트 로더를 찾는다. BIOS 시작 중에 키(일반적으로 F2 또는 F12 이며 시스템마다 다르다)를 눌러 부팅 순서를 변경할 수 있다. 부트 로더 프로그램이 감지되어 메모리에 로드되면 BIOS가 이를 제어한다. 요약 : BIOS는 MBR 부트 로더를 불러오고 실행하는 역할을 한다. 2. MBR # MBR은 마스터 부트 레코드(Master Boot Record)의 약자이다. MBR은 부팅 디스크의 첫 번째 섹터에 위치해있다. 일반적으로 첫번째 디스크 장치명을 의미하는 /dev/hda 또는 /dev/sda이다. MBR은 크기가 512바이트이다. 여기에는 세 가지 구성 요소가 있다. Bootstrap code : 446bytes 크기의 첫번째 부트 로더 정보 Partition table : 64bytes 크기의 파티션 테이블 정보 Signature : 마지막 2바이트는 MBR 유효성 체크 영역으로 항상 0x55AA 값이 들어간다. (Signature 값이 0x55AA가 아닌 다른 값일 경우 부팅시 Operating System not found 에러와 함께 부팅이 불가능하다.) MBR에는 GRUB(구 시스템의 경우 LILO)에 대한 정보가 들어 있다. 요약 : MBR은 GRUB 부트 로더를 로드하고 실행한다. 3. GRUB # GRUB은 Grand Unified Bootloader의 약자이다.\n시스템에 여러 개의 커널 이미지가 설치된 경우 실행할 이미지를 선택할 수 있다.\nGRUB는 부팅시 시작 화면을 표시하고 사용자가 리눅스 커널 선택을 할 때까지 몇 초 동안 기다린다. 아무 선택도 하지 않으면 grub 설정파일에 지정된 대로 기본 커널 이미지를 로드한다.\nGRUB은 파일 시스템에 대한 정보를 갖고 있다. 구버전의 리눅스 부트로더인 LILO(Linux Loader)는 파일 시스템을 이해하지 못했다.\nGRUB의 설정 파일은 /boot/grub/grub.conf이다. (/etc/grub.conf는 이 파일에 대한 링크파일). 다음은 CentOS의 grub.conf 샘플이다.\n#boot=/dev/sda default=0 timeout=5 splashimage=(hd0,0)/boot/grub/splash.xpm.gz hiddenmenu title CentOS (2.6.18-194.el5PAE) root (hd0,0) kernel /boot/vmlinuz-2.6.18-194.el5PAE ro root=LABEL=/ initrd /boot/initrd-2.6.18-194.el5PAE.img 위의 정보에서 알 수 있듯이 GRUB 설정파일에는 커널과 initrd 이미지가 포함되어 있다.\n요약 : GRUB은 커널 및 initrd 이미지를 불러오고 실행한다.\n4. Kernel # GRUB 설정파일(grub.conf)의 root=에 지정된 대로 루트 파일 시스템을 디스크 장치에 마운트한다. 커널은 /sbin/init 프로세스를 실행합니다. init는 Linux Kernel에서 실행되는 첫 번째 프로그램이기 때문에 프로세스 ID(PID)가 1이다. 시간날 때 ps -ef | grep init 명령어를 입력해서 init 프로세스의 pid를 한 번 확인해보자. initrd는 초기 RAM 디스크(initial ram disk)의 약자로 메모리상의 가상 디스크이며, initrd는 리눅스 커널이 부팅되고 실제 루트 파일 시스템이 마운트될 때까지 커널에 의해 임시 루트 파일 시스템으로 사용된다. initrd는 initramfs이라는 파일시스템을 갖고 있다. initramfs은 리눅스 커널이 초기에 동작할때 필요한 드라이버나 프로그램, 바이너리 파일 등을 가지고 있어 하드 드라이브 파티션 및 기타 하드웨어에 액세스하는 데 도움을 준다. 5. Init # /etc/inittab 설정파일을 참조해 Linux 운영체제의 실행 수준(Run Level)을 결정한다. 다음은 설정 가능한 Run Level 이다. 0 – 종료 (Halt) 1 – 단일 사용자 모드 (Single user mode) 2 – NFS가 없는 다중 사용자 (Multiuser, without NFS) 3 – 전체 다중 사용자 모드 (Full multiuser mode) 4 – 미사용 (unused) 5 – X11 (GUI) 6 – 재부팅 (Reboot) Init 프로세스는 /etc/inittab에서 기본 Run Level을 확인한 후 각 레벨에 맞게 프로그램들을 로드한다. 시스템에서 grep initdefault /etc/inittab을 실행하여 기본 실행 수준을 확인할 수 있다. 스스로 서버 장애를 만들고 싶다면 /etc/inittab 설정파일을 열어 Default run level을 0 또는 6 으로 설정하면 된다. 이제 우리는 0과 6이 무엇을 의미하는지 알기 때문에 아마도 그렇게 하지는 않겠지만. 일반적으로 Runlevel을 3 또는 5로 설정한다. 6. Runlevel programs # Linux 시스템이 부팅될 때 다양한 서비스가 시작되는 것을 볼 수 있다. 예를 들어 starting sendmail … OK와 같은 메세지가 이에 해당한다. 이러한 프로그램들은 Run level 디렉토리(/etc/rc.d/rc*.d/)에 의해 자동실행되는 프로세스들이다. 기본 초기화 레벨(Run Level) 설정에 따라 시스템은 다음 디렉토리 중 하나를 골라 그 안에 나열된 프로그램들을 순차적으로 실행한다. 실행 레벨 0 – /etc/rc.d/rc0.d/ 실행 레벨 1 – /etc/rc.d/rc1.d/ 실행 레벨 2 – /etc/rc.d/rc2.d/ 실행 레벨 3 – /etc/rc.d/rc3.d/ 실행 레벨 4 – /etc/rc.d/rc4.d/ 실행 레벨 5 – /etc/rc.d/rc5.d/ 실행 레벨 6 – /etc/rc.d/rc6.d/ /etc 아래에 직접 이 디렉토리에 사용할 수 있는 심볼릭 링크도 있다. 따라서 /etc/rc0.d는 /etc/rc.d/rc0.d에 연결되어 있다. /etc/rc.d/rc*.d/ 디렉토리에서 첫글자가 S와 K로 시작하는 프로그램들을 볼 수 있다. 첫글자가 S로 시작하는 프로그램은 자동적으로 시작(Startup) 된다. 첫글자가 K로 시작하는 프로그램은 자동적으로 종료(Kill) 된다. 프로그램 이름에서 S와 K 바로 옆에 숫자가 있다. 프로그램이 시작되거나 종료되어야 하는 시퀀스 번호이다. 예를 들어, S12syslog는 시퀀스 번호가 12인 syslog 데몬을 시작한다. S80sendmail은 시퀀스 번호가 80인 sendmail 데몬을 시작한다. 따라서 syslog 프로그램은 sendmail의 시퀀스 번호보다 더 낮기 때문에 sendmail 프로그램보다 먼저 시작된다. ","date":"Nov 2, 2021","permalink":"/blog/linux-boot-sequence/","section":"Blogs","summary":"개요 # 리눅스의 부팅순서를 설명한다.","title":"리눅스 부팅순서"},{"content":"개요 # Hugo-theme-codex 테마를 사용하는 블로그에서 댓글 플러그인인 utterances 기능 추가하는 방법을 설명한다.\n사실 언젠간 해야지 하면서 계속 미루고 있던 블로그 유지보수를 할로윈 기념으로 실시하게 되었다.\nTIL(Today I Learned) 포스트는 꾸준히 작성하고 있는데, 정작 블로그 유지보수는 미루고 있었다.\n환경 # 하드웨어 : Apple / MacBook Pro (13\u0026quot;, M1, 2020) 사이트 정적 생성기 : hugo v0.88.1 $ hugo version hugo v0.88.1+extended darwin/arm64 BuildDate=unknown 테마 : hugo-theme-codex v1.6.0 $ git submodule -f568a96d9b35bfa63c5e5f7feb7e799f7c995cb4 public 9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da themes/hugo-theme-codex (v1.6.0) 댓글 플러그인 : Utterances 목표 # 블로그 레이아웃에 Utterances 플러그인을 추가해 댓글(Comment) 기능을 구현한다.\n해결법 # 1. utterances 설치 # 블로그 레포지터리(repo)에 Utterances 앱을 설치한다.\nUtterances를 사용하면 댓글 기능을 구현하는 데 드는 시간은 10분이면 충분하다. 그만큼 간단하기 때문이다.\n블로그 레포지터리의 공개 상태는 Public으로 설정되어 있어야만 Utterances 플러그인을 사용 가능하다.\nutterances 설치 홈페이지에 접속한 후 본인의 깃허브 계정으로 로그인한다.\n이후 우측에 초록색 Install 버튼 클릭\nOnly select repositories → 블로그 repo 선택 (내 경우는 seyslee/blog)\n2. utternaces 작성 # A) repo # utterances 공식 홈페이지에 접속한다. 중간에 configuration 이후 \u0026lsquo;repo:\u0026rsquo; 값이 있다.\n이 값에 소유자와 레포지터리 이름을 입력한다. 예를 들어 나같은 경우는 다음과 같이 작성했다.\nrepo: seyslee/blog B) Blog Post ↔️ Issue Mapping # 블로그 글과 Issue를 어떻게 Mapping 할 것인지 정하는 설정이다.\n기본값인 Issue title contains page pathname로 선택한다.\nC) Issue Label # blog-comment 로 입력. 아무 입력 없으면 기본값인 Comment로 설정된다.\nIssue Label은 옵션값(Optional)이기 때문에 Default 값인 Comment를 사용해도 블로그 서비스에는 전혀 지장없다.\nD) Theme # 댓글창 테마는 각자 원하는 걸로 선택하면 된다.\n내 블로그 테마인 hugo-theme-codex는 흰색 계열이기 때문에 같은 색상에 맞춰 기본값인 GitHub Light로 선택했다.\nE) Enable Utterances # repo 정보, 블로그 포스트와 이슈간 맵핑 방식, 이슈 라벨, 테마를 선정했다면 최종적으로 Utterances 플러그인의 코드가 자동 완성된다.\n\u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;seyslee/blog\u0026#34; issue-term=\u0026#34;pathname\u0026#34; label=\u0026#34;blog-comment\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; Utterances 생성시 입력한 값이 사용자마다 각각 다르기 때문에, 위 코드는 내 경우에만 해당되니 참고만 하자.\n이제 블로그 포스트 레이아웃 파일(.html)의 적절한 위치에 Utterances 코드를 붙여넣는 작업만 하면 끝난다.\n3. 블로그 레이아웃에 코드 추가 # 블로그 테마마다 레이아웃 파일의 경로나 구조가 다르다. 해당 글은 현재 저자가 사용중인 테마인 hugo-theme-codex 기준으로 설명하고 있기 때문에 만약 다른 블로그 테마를 사용중이라면 도움이 안될 수도 있다.\n작업경로\nblog (/) # seyslee/blog └─ themes └─ hugo-theme-codex └─ layouts └─ _default └─ single.html # 작업대상! 개별 포스트에 대한 레이아웃 파일. hugo-theme-codex 테마의 경우 개별 포스트에 대한 레이아웃은 single.html 파일에 선언되어 있다.\n모든 포스트에 댓글 기능을 일괄 적용하려면 single.html 파일을 열어서 Utterances 코드를 붙여넣으면 된다.\n변경전 single.html\n{{ define \u0026#34;styles\u0026#34; }} {{ $.Scratch.Set \u0026#34;style_opts\u0026#34; (dict \u0026#34;src\u0026#34; \u0026#34;scss/pages/post.scss\u0026#34; \u0026#34;dest\u0026#34; \u0026#34;css/post.css\u0026#34;) }} {{ end }} {{ define \u0026#34;main\u0026#34; }} {{ $dateFormat := .Site.Params.dateFormat | default \u0026#34;Jan 2 2006\u0026#34; }} \u0026lt;div class=\u0026#34;flex-wrapper\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post__container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;post__header\u0026#34;\u0026gt; \u0026lt;h1 id=\u0026#34;post__title\u0026#34;\u0026gt;{{.Title}}\u0026lt;/h1\u0026gt; {{ if .Date }}\u0026lt;time datetime=\u0026#34;{{ .Date }}\u0026#34; class=\u0026#34;post__date\u0026#34;\u0026gt;{{ .Date.Format $dateFormat }}\u0026lt;/time\u0026gt; {{ end }} \u0026lt;/header\u0026gt; \u0026lt;article class=\u0026#34;post__content\u0026#34;\u0026gt; {{ partial \u0026#34;anchored-headings.html\u0026#34; .Content }} {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} \u0026lt;/article\u0026gt; {{ partial \u0026#34;tags.html\u0026#34; .}} {{ partial \u0026#34;post-pagination.html\u0026#34; .}} {{ template \u0026#34;_internal/disqus.html\u0026#34; . }} \u0026lt;footer class=\u0026#34;post__footer\u0026#34;\u0026gt; {{ partial \u0026#34;social-icons.html\u0026#34; .}} \u0026lt;p\u0026gt;{{ replace .Site.Copyright \u0026#34;{year}\u0026#34; now.Year }}\u0026lt;/p\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ if .Params.toc }} \u0026lt;div class=\u0026#34;toc-container\u0026#34;\u0026gt; {{ if .Site.Params.showPageTitleInTOC }} \u0026lt;div class=\u0026#34;toc-post-title\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/div\u0026gt; {{ end }} {{ .TableOfContents }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} {{ define \u0026#34;scripts\u0026#34; }} {{/* Hardcode a specific prismjs version to avoid a redirect on every page load. */}} \u0026lt;script src=\u0026#34;https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{/* Automatically loads the needed languages to highlight the code blocks. */}} \u0026lt;script src=\u0026#34;https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js\u0026#34; data-autoloader-path=\u0026#34;https://unpkg.com/prismjs@1.20.0/components/\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ if .Params.toc }} \u0026lt;script src=\u0026#34;/js/table-of-contents.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} {{ end }} 중간에 \u0026lt;footer\u0026gt; ... \u0026lt;/footer\u0026gt; 부분이 있다. 이 부분 바로 윗줄에 복사한 utterances 스크립트를 복붙한다.\n\u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;seyslee/blog\u0026#34; issue-term=\u0026#34;pathname\u0026#34; label=\u0026#34;blog-comment\u0026#34; theme=\u0026#34;github-light\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; 위 utterances 스크립트는 내 환경에 작성된 코드이기 때문에, 이걸 그대로 본인 블로그에 복붙하면 안된다.\n각자 Utterances 사이트에서 입력한 값을 토대로 자동 생성된 스크립트를 복사해서 붙여넣어야 한다.\n변경후 single.html\n{{ define \u0026#34;styles\u0026#34; }} {{ $.Scratch.Set \u0026#34;style_opts\u0026#34; (dict \u0026#34;src\u0026#34; \u0026#34;scss/pages/post.scss\u0026#34; \u0026#34;dest\u0026#34; \u0026#34;css/post.css\u0026#34;) }} {{ end }} {{ define \u0026#34;main\u0026#34; }} {{ $dateFormat := .Site.Params.dateFormat | default \u0026#34;Jan 2 2006\u0026#34; }} \u0026lt;div class=\u0026#34;flex-wrapper\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post__container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;post\u0026#34;\u0026gt; \u0026lt;header class=\u0026#34;post__header\u0026#34;\u0026gt; \u0026lt;h1 id=\u0026#34;post__title\u0026#34;\u0026gt;{{.Title}}\u0026lt;/h1\u0026gt; {{ if .Date }}\u0026lt;time datetime=\u0026#34;{{ .Date }}\u0026#34; class=\u0026#34;post__date\u0026#34;\u0026gt;{{ .Date.Format $dateFormat }}\u0026lt;/time\u0026gt; {{ end }} \u0026lt;/header\u0026gt; \u0026lt;article class=\u0026#34;post__content\u0026#34;\u0026gt; {{ partial \u0026#34;anchored-headings.html\u0026#34; .Content }} {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} \u0026lt;/article\u0026gt; {{ partial \u0026#34;tags.html\u0026#34; .}} {{ partial \u0026#34;post-pagination.html\u0026#34; .}} {{ template \u0026#34;_internal/disqus.html\u0026#34; . }} \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;seyslee/blog\u0026#34; issue-term=\u0026#34;pathname\u0026#34; label=\u0026#34;blog-comment\u0026#34; theme=\u0026#34;github-dark\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt; \u0026lt;footer class=\u0026#34;post__footer\u0026#34;\u0026gt; {{ partial \u0026#34;social-icons.html\u0026#34; .}} \u0026lt;p\u0026gt;{{ replace .Site.Copyright \u0026#34;{year}\u0026#34; now.Year }}\u0026lt;/p\u0026gt; \u0026lt;/footer\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ if .Params.toc }} \u0026lt;div class=\u0026#34;toc-container\u0026#34;\u0026gt; {{ if .Site.Params.showPageTitleInTOC }} \u0026lt;div class=\u0026#34;toc-post-title\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/div\u0026gt; {{ end }} {{ .TableOfContents }} \u0026lt;/div\u0026gt; {{ end }} \u0026lt;/div\u0026gt; {{ end }} {{ define \u0026#34;scripts\u0026#34; }} {{/* Hardcode a specific prismjs version to avoid a redirect on every page load. */}} \u0026lt;script src=\u0026#34;https://unpkg.com/prismjs@1.20.0/components/prism-core.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{/* Automatically loads the needed languages to highlight the code blocks. */}} \u0026lt;script src=\u0026#34;https://unpkg.com/prismjs@1.20.0/plugins/autoloader/prism-autoloader.min.js\u0026#34; data-autoloader-path=\u0026#34;https://unpkg.com/prismjs@1.20.0/components/\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ if .Params.toc }} \u0026lt;script src=\u0026#34;/js/table-of-contents.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; {{ end }} {{ end }} \u0026lt;script\u0026gt; ... \u0026lt;/script\u0026gt; 부분이 추가됐다.\n4. 댓글창 생성여부 확인 # git commit 해서 실제 블로그 환경에 적용하기 전에 먼저 localhost에서 잘 적용되었는지 확인해보자.\n$ hugo server -D Start building sites … hugo v0.88.1+extended darwin/arm64 BuildDate=unknown | EN -------------------+----- Pages | 65 Paginator pages | 0 Non-page files | 0 Static files | 12 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 37 ms Watching for changes in /Users/ive/githubrepos/blog/{archetypes,content,data,layouts,static,themes} Watching for config changes in /Users/ive/githubrepos/blog/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 이후 블로그 홈페이지( http://localhost:1313/)에 접속해서 확인해본 결과, 포스트 하단에 Utterances 댓글창이 잘 적용된 걸 확인할 수 있다.\n그럼 다들 Hugo 블로그 잘 쓰시길 바란다. Happy Halloween!\n","date":"Oct 31, 2021","permalink":"/blog/installing-utterances-in-hugo/","section":"Blogs","summary":"개요 # Hugo-theme-codex 테마를 사용하는 블로그에서 댓글 플러그인인 utterances 기능 추가하는 방법을 설명한다.","title":"hugo 댓글기능 추가(Utterances)"},{"content":"개요 # 리눅스 서버에서 명령어를 통해 캐시 메모리를 정리해 여유 메모리를 확보할 수 있다.\n환경 # OS : Red Hat Enterprise Linux Server release 5.x Shell : bash 해결법 # 1. 메모리 사용량 요약 확인 # $ free -g total used free shared buffers cached Mem: 62 62 0 0 0 55 -/+ buffers/cache: 6 56 Swap: 62 3 59 전체 메모리(total) 62GB 중 55GB가 캐시 메모리(cached)로 잡혀있다.\n2. 메모리 사용량 상세 확인 # 기본 단위(kB)로 보기 # $ cat /proc/meminfo | column -t MemTotal: 66004284 kB MemFree: 211724 kB Buffers: 759392 kB Cached: 58053888 kB SwapCached: 2245728 kB Active: 11509948 kB Inactive: 53444952 kB HighTotal: 0 kB HighFree: 0 kB LowTotal: 66004284 kB LowFree: 211724 kB SwapTotal: 65537156 kB SwapFree: 62276916 kB Dirty: 1056 kB Writeback: 0 kB AnonPages: 6139572 kB Mapped: 42920 kB Slab: 668696 kB PageTables: 36240 kB NFS_Unstable: 0 kB Bounce: 0 kB CommitLimit: 98539296 kB Committed_AS: 11802812 kB VmallocTotal: 34359738367 kB VmallocUsed: 295112 kB VmallocChunk: 34359442931 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 Hugepagesize: 2048 kB column -t : 출력결과를 칸에 맞게 정렬해서 가독성을 높여주는 명령어\n확인결과 전체 메모리(MemTotal)의 87%가 캐시 메모리(Cached)로 잡혀있다.\nGB 단위로 보기 # kB 단위로 보는게 불편하면 GB 단위로 변환시켜 볼 수도 있다.\n$ awk \u0026#39;$3==\u0026#34;kB\u0026#34;{$2=$2/1024/1024;$3=\u0026#34;GB\u0026#34;} 1\u0026#39; /proc/meminfo | column -t MemTotal: 62.9466 GB MemFree: 0.191048 GB Buffers: 0.724033 GB Cached: 55.3696 GB SwapCached: 2.14169 GB Active: 10.6456 GB Inactive: 51.3047 GB HighTotal: 0 GB HighFree: 0 GB LowTotal: 62.9466 GB LowFree: 0.191048 GB SwapTotal: 62.5011 GB SwapFree: 59.3919 GB Dirty: 0.00088501 GB Writeback: 0 GB AnonPages: 5.8541 GB Mapped: 0.0409317 GB Slab: 0.636795 GB PageTables: 0.0345459 GB NFS_Unstable: 0 GB Bounce: 0 GB CommitLimit: 93.9744 GB Committed_AS: 11.3072 GB VmallocTotal: 32768 GB VmallocUsed: 0.281441 GB VmallocChunk: 32767.7 GB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 Hugepagesize: 0.00195312 GB 전체 메모리(MemTotal) 64GB 중에서 캐시 메모리(Cached)를 55GB 쓰고, 버퍼 메모리(Buffers)는 0.7GB 사용중.\n여유 메모리(MemFree) 값이 0.1GB로 너무 낮다.\n3. 캐시 메모리 정리 # 메모리 사용량 모니터링 # 캐시 메모리 정리 전에 메모리 사용량 변화를 모니터링 하기 위해 세션을 하나 더 띄워서 watch -d -n1 \u0026quot;free -g\u0026quot; 명령어를 실행한다.\n$ watch -d -n1 \u0026#34;free -g\u0026#34; 1초 간격으로 free -g 명령어를 실행하는 명령어.\n-d 옵션은 변경된 값을 하이라이팅 해서 가독성을 높여준다.\nsync # $ sync 캐시 메모리에 있는 변동사항을 하드디스크에 옮겨서 쓰기(Write)\n캐시 메모리 정리 # $ echo 3 \u0026gt; /proc/sys/vm/drop_caches 캐시 메모리를 정리한다. 정리대상은 pagecache, dentry, inode 이다.\n캐시 메모리 정리 소요시간은 서버 환경과 캐시 메모리 용량에 따라 다른데 보통 10초 안으로 정리가 완료된다.\n용어설명\npagecache : 파일의 입출력(I/O)의 속도와 퍼포먼스를 높이기 위해 시스템이 할당한 메모리 영역(임시 메모리 저장소). 예를 들어 어떤 경로의 파일을 한 번 읽어들이면 시스템이 해당 파일 내용을 임시메모리에 저장시키는데 이후에 해당 파일을 다시 읽을 때 이를 새로 읽어들이지 않고 이 메모리에서 바로 불러오면 디스크의 읽기/쓰기 속도가 빨라지므로 효율이 높아짐. 윈도우 OS의 페이지 파일 같은 역할. dentry : directory entry의 줄임말로 예를 들어 /usr/share 같은 경로에서 usr과 share를 지칭. inode : 파일과 디렉토리에 관한 정보를 담고 있는 자료구조. 예를 들어 파일의 퍼미션 정보, 디스크 상의 파일의 물리적 위치, 크기, 생성된 일시 정보 등을 저장. drop_caches 의미\n$ echo 1 \u0026gt; /proc/sys/vm/drop_caches # To free pagecache $ echo 2 \u0026gt; /proc/sys/vm/drop_caches # To free dentries and inodes $ echo 3 \u0026gt; /proc/sys/vm/drop_caches # To free pagecache, dentries and inodes 4. 메모리 반환 결과 확인 # 메모리 사용량 요약 확인 # $ free -g total used free shared buffers cached Mem: 62 12 50 0 0 5 -/+ buffers/cache: 6 56 Swap: 62 3 59 -g : GB 단위로 보여준다.\n메모리 사용량 상세 확인 # $ awk \u0026#39;$3==\u0026#34;kB\u0026#34;{$2=$2/1024/1024;$3=\u0026#34;GB\u0026#34;} 1\u0026#39; /proc/meminfo | column -t MemTotal: 62.9466 GB MemFree: 48.9179 GB Buffers: 0.0405312 GB Cached: 7.78165 GB SwapCached: 2.14169 GB Active: 8.87233 GB Inactive: 4.87525 GB HighTotal: 0 GB HighFree: 0 GB LowTotal: 62.9466 GB LowFree: 48.9179 GB SwapTotal: 62.5011 GB SwapFree: 59.3919 GB Dirty: 0.00118637 GB Writeback: 0 GB AnonPages: 5.92254 GB Mapped: 0.0409355 GB Slab: 0.11797 GB PageTables: 0.0352592 GB NFS_Unstable: 0 GB Bounce: 0 GB CommitLimit: 93.9744 GB Committed_AS: 11.2496 GB VmallocTotal: 32768 GB VmallocUsed: 0.281441 GB VmallocChunk: 32767.7 GB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 Hugepagesize: 0.00195312 GB 메모리 전후 비교표 # 버퍼 메모리는 작업전부터 크기가 작아 효과가 그리 체감되지는 않지만 94% 줄였고, 캐시 메모리는 87% 줄였다.\n구분 버퍼 메모리 캐시 메모리 Before 0.7 [GB] 55 [GB] After 0.04 [GB] 7 [GB] 5. 캐시 메모리 스케줄링 등록 # 시스템 엔지니어가 매번 수동으로 캐시 메모리를 정리하는 건 번거롭고 불가능하니, 서버가 알아서 자동 정리할 수 있도록 crontab에 캐시 메모리 정리 스케줄을 등록해준다.\n반드시 root 계정에서 crontab을 등록해야한다.\n서버에 부하가 많은 시점에 drop_cache 값에 3을 부여하는 행위는 서버 멈춤 현상을 유발할 수 있다. 따라서 캐시 메모리 정리 스케줄링 등록 시 실행시간은 부하(Load)가 적은 새벽 시간대로 설정해야만 한다.\nCron 커닝페이퍼(Cheatsheet) # # * * * * * command to be executed # - - - - - # | | | | | # | | | | +----\u0026gt; Day of Week (0 - 6) (0 = Sunday ... 6 = Saturday) # | | | +------\u0026gt; Month (1 - 12) # | | +--------\u0026gt; Day of Month (1 - 31) # | +----------\u0026gt; Hour (0 - 23) # +------------\u0026gt; Minute (0 - 59) crontab을 작성하는 일이 그리 빈번히 발생하는 편이 아니기 때문에 가끔 헷갈릴 때가 있다.\ncrontab 윗줄에 주석처리된 커닝페이퍼를 달아놓으면 작업시 눈 앞에 바로 참고자료가 있기 때문에 편리한 서버 관리가 가능하다.\n스케줄링 등록 # $ crontab -e 0 3 * * * sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches -e : crontab을 편집한다.\n매일 새벽 3시마다 캐시 메모리를 정리하는 명령어를 실행한다.\n스케줄링 확인 # $ crontab -l 0 3 * * * sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches -l : crontab의 설정 상태를 확인한다.\n참고자료 # https://kangwoo.github.io/devops/linux/linux-buffer-cache/\nhttps://linux-mm.org/Drop_Caches\n","date":"Oct 28, 2021","permalink":"/blog/cleaning-cache-memory-in-linux/","section":"Blogs","summary":"개요 # 리눅스 서버에서 명령어를 통해 캐시 메모리를 정리해 여유 메모리를 확보할 수 있다.","title":"리눅스 캐시 메모리 비우기"},{"content":"환경 # Model : MacBook Pro (13inch, M1, 2020)\nOS : macOS Monterey 12.0.1\n$ sw_vers ProductName:\tmacOS ProductVersion:\t12.0.1 BuildVersion:\t21A559 Shell : zsh\n터미널 : macOS 기본 터미널\n증상 # macOS Catalina 11.6에서 macOS Monterey 12.0.1로 소프트웨어 업데이트를 한 이후부터 git 명령어를 실행하면 에러 메세지가 반환되며 실행할 수 없다.\n$ git commit -m \u0026#39;rebuilding site 2021년 10월 27일 수요일 00시 34분 04초 KST\u0026#39; xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun git commit 명령어 뿐만 아니라 git push 또한 동일한 에러 메세지이다.\n$ git push origin master xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun 원인 # macOS에서 OS 업데이트가 진행될때마다 자주 발생되는 xcode cli 관련 호환성 이슈이다.\n대표적인 개발툴인 git, make, gcc가 영향을 받는다.\n해결법 # 1. Xcode CLI 설치 # Xcode CLI만 따로 설치해서 문제를 해결할 수 있다.\n$ xcode-select --install xcode-select: note: install requested for command line developer tools 이후 GUI 환경에서 [설치] 버튼을 눌러 명령어 라인 개발자 도구(Command line developer tools)를 설치하면 된다.\n설치후에 이전에 안됐던 명령어가 잘 실행되는 지 테스트해본다.\n2. Xcode CLI 초기화 # Xcode CLI를 설치해도 증상이 동일하다고 하면 xcode CLI를 초기화한다. 초기화시에는 root 권한이 필요하다.\n$ sudo xcode-select --reset Password: [패스워드 입력] 3. git 명령어 테스트 # $ git commit -m \u0026#39;rebuilding site 2021년 10월 27일 수요일 00시 45분 23초 KST\u0026#39; [...] 2 files changed, 28 insertions(+), 3 deletions(-) $ git push origin master Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 8 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 1.33 KiB | 1.33 MiB/s, done. [...] 5936bbc..902284d master -\u0026gt; master Xcode CLI를 설치완료 후 에러 메세지 없이 git 명령어가 실행된다.\n","date":"Oct 27, 2021","permalink":"/blog/fixing-xcrun-error/","section":"Blogs","summary":"환경 # Model : MacBook Pro (13inch, M1, 2020)","title":"macOS xcrun: error 해결법"},{"content":"","date":"Oct 26, 2021","permalink":"/tags/aix/","section":"Tags","summary":"","title":"aix"},{"content":"개요 # IBM AIX의 클라이언트 서버에서 NTP 데몬을 동작후 NTP 서버와 시간 동기화하는 방법을 설명한다.\n어느날 내가 관리하는 IBM AIX 서버가 NTP 동기화가 안되고 있길래 점검하면서 공부할 겸 정리한 메뉴얼이다.\n증상 # IBM AIX 클라이언트에서 NTP 서버와 시간 동기화가 불가능하다.\n$ ntpq -p ntpq: read: A remote host refused an attempted connect operation. 원인 # NTP 데몬인 xntpd가 실행중이지 않았다.\n환경 # OS : IBM AIX 7.2.0.0 Shell : ksh 조치방법 # 1. NTP 데몬 상태확인 # IBM AIX에서 NTP 데몬의 이름은 xntpd이다. 리눅스의 경우는 ntpd이다.\n가장 먼저 xntpd가 실행중인지 확인한다.\n$ ps -ef | grep xntpd root 16449928 8848040 0 08:25:49 pts/0 0:00 grep xntpd xntpd 가 실행중이지 않다.\n2. NTP 설정파일 확인 # IBM AIX의 NTP 설정파일의 절대경로는 /etc/ntp.conf로 리눅스와 동일하다.\nxntpd는 시작될 때 /etc/ntp.conf 설정 정보를 읽는다.\n해당 설정파일을 수정하자.\n$ cat /etc/ntp.conf | grep -v ^# ... broadcastclient driftfile /etc/ntp.drift tracefile /etc/ntp.trace server 10.10.10.123 prefer prefer\nNTP 서버 IP 뒤에 prefer 키워드가 붙을 경우, 해당 NTP 서버와 우선적으로 연결한다.\n명령어 설명\ngrep -v ^#는 주석을 제외한 내용만 출력하겠다는 의미이다.\nNTP 설정파일인 ntp.conf 상단에는 제조사인 IBM에서 초기 작성한 주석 라인이 약 30줄 정도로 많다.\ngrep -v ^# 명령어를 붙이면 주석이 아닌 실제 설정 내용만 필터링해서 출력하기 때문에 보기 편하다.\n3. NTP 서비스 기동 # $ startsrc -s xntpd -a \u0026#34;-x\u0026#34; 0513-059 The xntpd Subsystem has been started. Subsystem PID is 19268504. xntpd 서비스가 시작되었다.\nstartsrc 명령어 설명 # -s : 서비스 이름 -a : 서비스 옵션 파라미터 \u0026quot;-x\u0026quot; : 클라이언트의 시간이 뒤로 돌아가는 것을 방지하는 옵션 시간롤백 방지 \u0026ldquo;-x\u0026rdquo; # 만약 참조하는 NTP 서버(time server)가 현재 시간보다 과거시점으로 시간이 변경되었을 때, 해당 NTP 서버를 참조하는 클라이언트 측에서 시간이 뒤로 돌아가는 현상을 방지하기 위해서 xntpd 시작시, -x 옵션을 준다. Oracle DB 서버의 경우 시간 롤백이 발생할 경우 충돌될 수 있으므로 반드시 xntpd 시작시 -x 옵션을 넣어서 시간이 뒤로 돌아가는 것(time backward)을 방지한다. 4. NTP 데몬 동작상태 확인 # $ ps -ef | grep xntpd root 11010996 8848040 0 08:55:57 pts/0 0:00 grep xntpd root 19268504 3146712 0 08:55:54 - 0:00 /usr/sbin/xntpd -x 19268504 PID를 가진 xntpd 데몬이 -x 옵션이 붙은 상태로 잘 동작중이다.\n$ lssrc -s xntpd Subsystem Group PID Status xntpd tcpip 19268504 active xntpd의 PID는 19268504, 상태는 동작중(active)이다.\n5. NTP 수동 동기화 실행 # $ ntpdate -d 10.10.10.123 26 Oct 09:51:07 ntpdate[16777548]: 3.4y transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) server 10.10.10.123, port 123 stratum 1, precision -19, leap 00, trust 000 refid [GPS], delay 0.02525, dispersion 0.00034 transmitted 4, in filter 4 reference time: e521cf7a.00000000 Tue, Oct 26 2021 9:51:06.000 originate timestamp: e521cf7b.6dd2f12b Tue, Oct 26 2021 9:51:07.429 transmit timestamp: e521cf7b.6dd32000 Tue, Oct 26 2021 9:51:07.429 filter delay: 0.02649 0.02769 0.02525 0.02640 0.00000 0.00000 0.00000 0.00000 filter offset: 0.000229 -0.00127 -0.00016 -0.00039 0.000000 0.000000 0.000000 0.000000 delay 0.02525, dispersion 0.00034 offset -0.000160 26 Oct 09:51:07 ntpdate[16777548]: adjust time server 10.10.10.123 offset -0.00016 만약 ntpdate -d 실행후 no server suitable for synchronization found 에러 메세지가 발생할 경우, 아래 2가지 사항을 점검해보자.\nNTP 서버에서 NTP 데몬(리눅스의 경우 ntpd, AIX의 경우 xntpd)이 정상 동작중인가? 클라이언트와 NTP 서버 사이에 위치한 방화벽에서 NTP용 포트인 123번 포트가 통과 허용되어 있는가? transmit과 receive가 반복되면 NTP 서버와 클라이언트간 시간 데이터를 잘 주고 받고 있다는 의미이다.\n... transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) receive(10.10.10.123) transmit(10.10.10.123) ... 시간 오차값(offset)은 -0.000160 이다.\n... offset -0.000160 ... 6. NTP 동기화 여부 확인 # 등록된 NTP 서버와 동기화된 상태인지 확인한다.\n$ ntpq -p remote refid st t when poll reach delay offset disp ============================================================================== *10.10.10.123 .GPS. 1 u 20 64 377 0.76 0.183 0.20 ntpq 설명 # remote : IP 앞에 * 표시가 붙으면 해당 NTP 서버와 클라이언트가 정상적으로 동기화된 상태를 의미한다.\nwhen : NTP 서버로부터 데이터를 수신한 후 경과된 시간으로 데이터를 받으면 0으로 초기화된다. 단위는 초(second).\npoll : 클라이언트가 NTP 서버에 동기화 요청하는 주기. 단위는 초(second).\nreach : 최근 8번 Poll 요청에 대한 NTP 서버의 응답 여부. reach는 8진수로 표기된 값이다. 시간이 지날수록 오르면서 377에 도달하면 동기화가 완료된다.\nreach 10진수 2진수 해석 1 1 0 0 0 0 0 0 0 1 최근 8번 시도중 1번 성공 3 3 0 0 0 0 0 0 1 1 최근 8번 시도중 2번 성공 7 7 0 0 0 0 0 1 1 1 최근 8번 시도중 3번 성공 17 15 0 0 0 0 1 1 1 1 최근 8번 시도중 4번 성공 37 31 0 0 0 1 1 1 1 1 최근 8번 시도중 5번 성공 77 63 0 0 1 1 1 1 1 1 최근 8번 시도중 6번 성공 177 127 0 1 1 1 1 1 1 1 최근 8번 시도중 7번 성공 377 255 1 1 1 1 1 1 1 1 최근 8번 시도중 8번 성공 delay : 네트워크 지연시간. 단위는 밀리초(millisecond). delay 값이 5보다 작아야 한다.\n7. NTP 자동시작 설정 # /etc/rc.tcpip는 부팅시 TCP/IP 데몬들과 Subsystem을 호출하는 설정 파일이다.\n서버가 리부팅 되더라도 NTP 데몬을 자동시작 하도록 설정파일을 수정한다.\n변경 전 설정파일 # $ vi /etc/rc.tcpip ... # Start up Network Time Protocol (NTP) daemon #start /usr/sbin/xntpd \u0026#34;$src_running\u0026#34; \u0026#34;-x\u0026#34; start /usr/sbin/xntpd ... 라인의 주석을 제거해서 서버 재부팅 후에도 NTP Daemon이 자동 기동되도록 설정한다.\n시간롤백을 방지하는 -x 옵션도 선언되어 있는지 잘 확인한다.\n변경 후 설정파일 # $ vi /etc/rc.tcpip ... # Start up Network Time Protocol (NTP) daemon start /usr/sbin/xntpd \u0026#34;$src_running\u0026#34; \u0026#34;-x\u0026#34; 참고자료 # AIX 공식문서: xntpd 데몬\n","date":"Oct 26, 2021","permalink":"/blog/starting-ntpd-in-aix/","section":"Blogs","summary":"개요 # IBM AIX의 클라이언트 서버에서 NTP 데몬을 동작후 NTP 서버와 시간 동기화하는 방법을 설명한다.","title":"AIX NTP 데몬 설정"},{"content":"","date":"Oct 26, 2021","permalink":"/tags/unix/","section":"Tags","summary":"","title":"unix"},{"content":"개요 # VMware ESXi 호스트 서버를 종료할 수 있다.\n배경지식 # ESXi 호스트의 Shutdown # VMware ESXi 호스트 서버의 경우 일반 x86 리눅스 서버처럼 shutdown -h now 명령어로 서버를 끌 수 없다.\nshutdown 명령어 자체가 존재하지 않기 때문이다.\n# shutdown -h now -sh: shutdown: not found 반면 reboot 명령어는 ESXi에서도 존재하고 잘 실행되기 때문에 테스트 차원으로 실행해서는 안되니 주의!\n환경 # OS : VMware ESXi 5.1.0 Shell : sh (ESXi shell) ID : root 해결법 # 호스트 서버의 종료를 위해서 해당 호스트를 유지보수 모드(Maintenance Mode)로 전환한 후, esxcli 명령어를 실행해 종료한다.\n1. root 로그인 # $ login root Password: \u0026lt;root 패스워드 입력\u0026gt; The time and date of this login have been sent to the system logs. VMware offers supported, powerful system administration tools. Please see www.vmware.com/go/sysadmintools for details. The ESXi Shell can be disabled by an administrative user. See the vSphere Security documentation for more information. # 유저 계정일 경우 login root 명령어로 root 로그인을 한다.\n서버 종료(Shutdown)나 서버 재시작(reboot)은 반드시 root 권한에서 진행되어야 한다.\n2. 구동중인 VM 리스트 확인 # # esxcli vm process list # 결과가 출력되지 않는다면 해당 호스트에서 구동중인 가상머신(VM, Virutal Machine)이 없는 상태이다.\n해당 호스트 위에서 구동중인 가상머신이 있을 경우 vSphere Client를 이용해 미리 다른 호스트로 옮겨놓자.\n3. 유지보수 모드 확인 # # esxcli system maintenanceMode get Disabled Disabled : 유지보수 모드가 비활성화되어 있음\n4. 유지보수 모드 활성화 # # esxcli system maintenanceMode set --enable true 유지보수 모드 값을 true로 설정한다. 유지보수 모드가 설정되면 vSphere Client에서도 호스트 아이콘이 바리게이트가 쳐진 호스트 아이콘으로 변경된다. 반대로 유지보수 모드를 끄는 방법은 esxcli system maintenanceMode set --enable false 를 실행한다.\n# esxcli system maintenanceMode get Enabled Enabled : 유지보수 모드가 활성화되었다.\n5. 서버 종료 # 실행 명령어 예시 # # esxcli system shutdown poweroff --reason=\u0026#34;Memory replacement.\u0026#34; 작업사유(--reason)에는 반드시 서버 종료조치(shutdown)하는 사유를 상세히 적어준다.\n애초에 작업사유를 입력하지 않으면 명령어가 실행되지 않는다.\npoweroff 명령어 사용법 확인 # # esxcli system shutdown poweroff Error: Missing required parameter -r|--reason Usage: esxcli system shutdown poweroff [cmd options] Description: poweroff Power off the system. The host must be in maintenance mode. Cmd options: -d|--delay=\u0026lt;long\u0026gt; Delay interval in seconds -r|--reason=\u0026lt;str\u0026gt; Reason for performing the operation (required) 호스트 서버의 poweroff 실행을 위해서는 작업 사유(--reason) 값을 필수적으로 입력해야 서버가 꺼진다.\n사유를 입력하지 않을 경우 작업사유를 입력하라는 에러 메세지가 나오면서, shutdown 명령어가 실행되지 않는다.\n# esxcli system shutdown poweroff Error: Missing required parameter -r|--reason [...] ","date":"Oct 25, 2021","permalink":"/blog/shutdown-esxi-host/","section":"Blogs","summary":"개요 # VMware ESXi 호스트 서버를 종료할 수 있다.","title":"ESXi 호스트 서버 끄기"},{"content":"개요 # Oracle DB에서 아카이브 로그(Archive Log)를 사용하도록 모드를 변경한다.\n환경 # OS : HP-UX B.11.31 Shell : sh (POSIX shell) DB : Oracle Database 10g 해결법 # 1. DB 로그인 # $ sqlplus / as sysdba oracle 계정에서 sysdba 권한으로 Oracle Database에 로그인합니다.\n2. 아카이브 로그 설정 확인 # SQL\u0026gt; archive log list Database log mode No Archive Mode Automatic archival Disabled Archive destination /oracle/arch Oldest online log sequence 7989 Current log sequence 7991 현재 Database log mode는 No Archive Mode로 아카이브 로그를 사용하지 않는 상태를 의미한다.\nNo Archive Mode를 Archive Mode로 활성화 해주기 위해서는 SQL alter문으로 설정을 변경한 후 DB 인스턴스의 재기동이 필요하다.\nNo Archive Mode vs Archive Mode # No Archive Log Mode는 Oracle Database 설치시 기본값이며, 몇 개의 redo 로그 파일을 돌려서 변경사항을 기록해두는 방식이다. 따라서, 일정 갯수의 redo 로그 파일 이전의 변경기록은 보관하지 않고 밀려나 사라지게 된다.\nArchive Mode는 이와 달리 redo 파일에 변경사항을 다 쓴후 파일에 덮어쓰기 전에 이전 파일을 다른 곳에 복사해 보관해두는 방식이다. 따라서 용량만 여유가 있다면 모든 변경사항이 덮어쓰여지지 않고 보관된다.\n3. DB 종료 후 mount # SQL\u0026gt; shutdown immediate; DB를 내려준다.\nSQL\u0026gt; startup mount; ORACLE instance started. [...] Database mounted. Database altered. 메세지가 떨어지면 정상적으로 실행된 것이다.\n실제 운영서버에서 startup mount 실행시 소요시간이 5~10분 걸릴 수 있다.\n4. 아카이브 로그 설정 변경 # SQL\u0026gt; alter database archivelog; Database altered. 아카이브 로그를 사용하도록 설정하는 alter문이다. Database altered. 메세지가 떨어지면 정상적으로 실행된 것이다.\n5. DB open # SQL\u0026gt; alter database open; Database altered. 6. 작업 결과확인 # Database log mode 확인\nSQL\u0026gt; archive log list Database log mode Archive Mode [...] Database log mode 값이 No Archive Mode에서 Archive Mode 로 변경되었다.\nInstance 상태 확인\nSQL\u0026gt; SELECT instance_name, status FROM v$instance; INSTANCE_NAME STATUS ------------------ ------------ DEV OPEN DB 인스턴스의 상태값(STATUS)이 OPEN 일 경우 정상이다.\n","date":"Oct 25, 2021","permalink":"/blog/enabling-archive-log-in-oracle/","section":"Blogs","summary":"개요 # Oracle DB에서 아카이브 로그(Archive Log)를 사용하도록 모드를 변경한다.","title":"오라클 아카이브 로그 활성화 설정"},{"content":"개요 # IBM AIX의 ksh에서 명령어 실행기록(history)에 실행시간을 표시하도록 설정합니다.\n환경 # OS : IBM AIX 7.2.0.0 Shell : ksh 문제점 # 명령어 실행기록에 실행시간(Timestamp)가 표기되지 않는다.\n$ fc -t [...] 641 ? :: su - oracle 642 ? :: vi /etc/environments 643 ? :: fc -t 명령어 실행시간(time field)이 물음표로 표시되어 정확한 명령어 실행시간을 확인할 수 없다.\n명령어 실행 시간의 중요성 # 시스템 엔지니어가 장애처리(Troubleshooting) 시 명령어 실행시간을 통해 장애 원인을 발견하는 경우가 종종 있다.\n서버 환경에서는 사람에 의해 유발되는 장애(Human Fault)가 생각보다 많이 발생하기 때문이다.\n무엇보다 명령어 실행 시간을 체크하는 일은 장애처리시 간단하면서도 관리자에게 많은 정보를 제공해주기 때문에,\n실행한 명령어에 실행시간까지 기록하도록 설정해놓는다면 장기적인 관점에서 서버 관리가 더 편해진다.\n해결법 # IBM AIX 5.3 버전 이후부터 ksh에서 timestamp 기능을 지원한다. 환경설정(/etc/environment) 파일에 새 값을 추가해 명령어 기록에 시간을 표기할 수 있다.\n1. 작업전 environment 파일 확인 # $ cat /etc/environment [...] PATH=/usr/bin:/etc:/usr/sbin:/usr/ucb:/usr/bin/X11:/sbin:/usr/java7_64/jre/bin:/usr/java7_64/bin TZ=Asia/Seoul LANG=en_US LOCPATH=/usr/lib/nls/loc 기존에는 EXTENDED_HISTORY 파라미터가 없다.\n2. environment 파일 수정 # $ cat /etc/environment [...] PATH=/usr/bin:/etc:/usr/sbin:/usr/ucb:/usr/bin/X11:/sbin:/usr/java7_64/jre/bin:/usr/java7_64/bin TZ=Asia/Seoul LANG=en_US LOCPATH=/usr/lib/nls/loc EXTENDED_HISTORY=ON vi 편집기로 /etc/environment 파일을 열고 맨 아랫줄에 EXTENDED_HISTORY 파라미터에 ON 값을 추가한다.\n/etc/environment 파일에 EXTENDED_HISTORY 파라미터를 추가하면 전체 계정에 해당 설정이 적용된다.\n3. 로그아웃 후 재로그인 # $ exit 4. 명령어 실행기록 확인 # $ fc -t 662 2021/10/23 10:16:58 :: cat .sh_history | more 663 2021/10/23 10:17:07 :: ls 664 2021/10/23 10:17:16 :: vi .sh_history 665 2021/10/23 10:17:36 :: vi .sh_history 666 2021/10/23 10:17:38 :: ls 667 2021/10/23 10:17:39 :: fc -t 668 2021/10/23 10:17:44 :: vi .sh_history 669 2021/10/23 10:17:53 :: fc -t 670 2021/10/23 10:24:09 :: fc -t 671 2021/10/23 10:24:12 :: id 672 2021/10/23 10:24:13 :: clear 673 2021/10/23 10:24:15 :: fc -t 674 2021/10/23 10:24:18 :: clear 675 2021/10/23 10:24:21 :: ping 127.0.0.1 676 2021/10/23 10:24:24 :: clear 677 2021/10/23 10:24:28 :: fc -t 이제 명령어 실행시간이 표시된다. history -t 명령어도 fc -t 명령어와 완전 동일한 기능을 하니 기호에 맞춰 사용하면 된다.\n$ history -t 690 2021/10/23 10:32:03 :: id 691 2021/10/23 10:32:03 :: ls 692 2021/10/23 10:32:04 :: pwd 693 2021/10/23 10:32:05 :: pwd 694 2021/10/23 10:32:06 :: ls 695 2021/10/23 10:32:07 :: clear 696 2021/10/23 10:32:08 :: clear 697 2021/10/23 10:32:09 :: id 698 2021/10/23 10:32:11 :: date 699 2021/10/23 10:32:15 :: cat /etc/passwd 700 2021/10/23 10:32:16 :: clear 701 2021/10/23 10:32:17 :: clear 702 2021/10/23 10:32:18 :: id 703 2021/10/23 10:32:19 :: id 704 2021/10/23 10:32:20 :: pwd 705 2021/10/23 10:32:24 :: history -t ","date":"Oct 23, 2021","permalink":"/blog/enabling-history-timestamp-in-aix/","section":"Blogs","summary":"개요 # IBM AIX의 ksh에서 명령어 실행기록(history)에 실행시간을 표시하도록 설정합니다.","title":"AIX History 시간 설정"},{"content":"개요 # IBM AIX에서 계정이 잠긴 여부를 확인하고 잠긴 계정을 해제할 수 있다.\n증상 # OS 계정이 잠겨서 원격 로그인(SSH)이 불가능한 상황\n환경 # OS : IBM AIX 7.2.0.0 Shell : ksh 해결방안 # 쉘에서 명령어를 실행해 계정의 잠금상태를 확인하고 잠금을 해제하면 된다.\n계정 패스워드 제어와 관련된 작업이기 때문에 반드시 root 권한을 얻어 실행해야만 한다.\n1. 로그인 실패회수 확인 # 명령어 형식\n$ lsuser -a \u0026lt;속성명\u0026gt; \u0026lt;계정명\u0026gt; 명령어 예시\n$ lsuser -a unsuccessful_login_count devuser1 devuser1 unsuccessful_login_count=6 devuser1 계정에서 로그인을 6번 실패한 이력이 있다.\n2. 계정상태 확인 # 명령어 형식\n$ lsuser -a \u0026lt;속성명\u0026gt; \u0026lt;계정명\u0026gt; 명령어 예시\n$ lsuser -a account_locked devuser1 devuser1 account_locked=true devuser1 계정의 account_locked 값이 true이므로 계정이 잠긴 상태이다.\n3. 로그인 실패회수 초기화 # devuser1 계정의 로그인 실패회수(unsuccessful_login_count) 값을 0으로 설정하여 초기화한다.\n$ chsec -f /etc/security/lastlog -a unsuccessful_login_count=0 -s devuser1 명령어 옵션\n-f : 파일명(File) 옵션\n-a : 속성 옵션(Attribute)\n-s : 계정명(Stanza)\n4. 계정잠금 해제 # 명령어 형식\n$ chuser \u0026lt;속성명\u0026gt;=\u0026lt;값\u0026gt; \u0026lt;계정명\u0026gt; 명령어 예시\n$ chuser account_locked=false devuser1 devuser1 account_locked=false account_locked 속성값을 true에서 false로 변경해서 devuser1 계정 잠금을 해제한다.\n추가정보 # 이 부분부터는 본문인 \u0026lsquo;잠긴 OS 계정 풀기\u0026rsquo; 메뉴얼과는 무관하나 알고 있으면 유용한 명령어이다.\nA. 계정 잠그는 방법 # 종종 서버의 보안강화를 목적으로 관리자가 장기간 사용하지 않는 운휴 계정을 잠금처리할 때 사용한다.\n1. 계정 잠금 실행 # $ chuser account_locked=true devuser1 2. 계정상태 확인 # $ lsuser -a account_locked devuser1 devuser1 account_locked=true devuser1 계정의 account_locked 값이 true이므로 현재 계정이 잠긴 상태이다.\nB. 계정 잠금 임계값 설정 # 무차별 대입 공격(brute-force attack)은 특정 암호를 풀기 위해 가능한 모든 값을 대입해 암호를 뚫는 공격 기법이다.\n무작위 대입 공격을 방어하는 대표적인 방법은 서버 보안설정에서 계정 잠금 임계값을 설정하는 것이다.\n아래는 AIX 서버에서 여러번 로그인 실패시 계정이 잠기는 설정방법이다.\n1. user 설정파일 확인 # $ cat /etc/security/user [...] default: loginretries = 0 loginretries : 계정 잠금 임계값 (계정 잠그기 전 로그인 시도 횟수) loginretries = 0은 로그인 시도 가능 횟수가 무제한이며, 계정 잠금 임계값이 설정되지 않은 상태를 의미한다.\n2. 계정 잠금 임계값 수정 # $ vi /etc/security/user [...] default: loginretries = 5 vi 편집기를 사용해서 loginretries 값을 0에서 5로 수정한다. 기본적default으로 모든 계정이 5번 로그인이 실패하면, 계정이 잠기게 된다.\n작업 끝.\n","date":"Oct 20, 2021","permalink":"/blog/unlocking-account-in-aix/","section":"Blogs","summary":"개요 # IBM AIX에서 계정이 잠긴 여부를 확인하고 잠긴 계정을 해제할 수 있다.","title":"AIX 계정잠금 해제"},{"content":" Summary # Younsung Steve Lee\nCloud Engineer @Watcha\nKubernetes와 Cloud Infrastructure에 관심이 많습니다. 아아를 달고 삽니다 Up to 3잔 / 1 day 전자책밀리의 서재 덕후 가장 좋아하는 소설은 위대한 개츠비 가장 좋아하는 언어는 Python. 아름다움이 추한 것보다 낫다 Beautiful is better than ugly.\n단순함이 복잡한 것보다 낫다 Simple is better than complex.\n- The Zen of Python\n미니멀리스트입니다. 오픈소스 커뮤니티에 기여하고, 테크와 관련된 글쓰는 일을 좋아합니다. 애자일하게 생활하며 좋은 사람들과 함께 클라우드 엔지니어로 행복하게 일하고 있습니다. Work Experience # 2022/02 ― Present\nCloud Engineer @Watcha\n업무 병목 지점인 클라우드 권한 관리를 해결하고 사내 개발자 경험DX, Developer Experience 향상을 위해 클라우드 권한 신청/승인 플랫폼인 ConsoleMe를 구축하고 운영 2013/12 ― 2022/02\nSystem Engineer @KDN\n260대의 VMVirtual Machine 및 물리서버 유지보수 및 운영 SAN 스토리지 및 SAN 스위치 유지보수 및 운영 DNS 사내 강사 2년 연속 출강 쉘스크립트 작성을 통한 OS 보안조치 자동화 Skills # Certification # AWS Certified Solutions Architect - Associate\nCCNP Data Center\nCCNP Enterprise\nContact # seyslee cysl@kakao.com\n","date":"Oct 18, 2021","permalink":"/about/","section":"","summary":"Summary # Younsung Steve Lee","title":"About"},{"content":"개요 # IBM AIX 운영체제에서 CPU 관련 상세정보를 확인할 수 있다.\n확인 가능한 주요정보\nCPU 제원정보 (모델명, Clock Speed 등) 물리 CPU 수 전체 물리코어 수 전체 논리코어 수 환경 # OS : IBM AIX 7.2.0.0 Shell : ksh 전제조건 # 없음\nTL;DR # Too long; didn\u0026rsquo;t read\n시간이 없는 분들을 위해 명령어만 요약한다.\n# 1. CPU 소켓 확인 $ lscfg -vp | grep WAY # 2. CPU 모델명 및 클럭 확인 $ prtconf | grep Processor # 3. 물리코어, 논리코어 수 확인 $ smtctl # 4. 물리코어 수 확인 \u0008$ lsdev -Cc processor # 5. 논리코어 수 확인 $ bindprocessor -q # 6. 전체 CPU 수 확인 $ lparstat -i 확인방법 # 1. CPU 소켓 확인 # $ lscfg -vp | grep WAY 06-WAY PROC CUOD: 06-WAY PROC CUOD: 06-WAY PROC CUOD: : 각 1줄은 물리적 CPU 칩 1개를 의미한다. 결과값이 2줄이므로 해당 서버는 2개의 물리적 CPU 칩이 장착된 상태이다. 06-WAY : CPU 1개당 코어수가 6개임을 의미한다. 전체 물리코어 수 계산 : 6 Core x 2 CPU = 12 Core 2. CPU 모델명 및 클럭 확인 # $ prtconf | grep Processor Processor Type: PowerPC_POWER8 Processor Implementation Mode: POWER 8 Processor Version: PV_8_Compat Number Of Processors: 2 Processor Clock Speed: 3891 MHz Model Implementation: Multiple Processor, PCI bus + proc0 Processor + proc8 Processor 주요정보\nProcessor Type : CPU 모델명 Number Of Processors : 물리적인 CPU 소켓 수 Processor Clock Speed : 해당 CPU의 Clock 속도 3. 물리코어, 논리코어 수 확인 # SMT 설정 확인 # SMTSimultaneous Multi-Threading는 물리코어를 여러개의 Thread로 나누는 기술을 의미한다.\nSMT는 Intel의 HTTHyper-Threading Technology와 비슷한 멀티스레딩 기술이다.\n$ smtctl This system is SMT capable. This system supports up to 8 SMT threads per processor. SMT is currently enabled. SMT boot mode is not set. SMT threads are bound to the same physical processor. proc0 has 4 SMT threads. Bind processor 0 is bound with proc0 Bind processor 1 is bound with proc0 Bind processor 2 is bound with proc0 Bind processor 3 is bound with proc0 proc8 has 4 SMT threads. Bind processor 4 is bound with proc8 Bind processor 5 is bound with proc8 Bind processor 6 is bound with proc8 Bind processor 7 is bound with proc8 서버에서 SMT 기능을 사용중이다. (SMT is currently enabled) 물리코어 수 2개, 논리코어 수 8개 물리코어 수 확인 # $ lsdev -Cc processor proc0 Available 00-00 Processor proc8 Available 00-08 Processor 물리코어는 총 2개이다. (proc0, proc8)\n이 물리코어들이 SMTSimultaneous Multi-Threading로 인하여 다중의 논리코어Thread로 나뉜다.\n논리코어 수 확인 # $ bindprocessor -q The available processors are: 0 1 2 3 4 5 6 7 논리코어는 총 8개이다. (Bind processor 0, 1, 2, 3, 4, 5, 6, 7) 4. 전체 CPU 수 확인 # lparstat 명령어는 LPARLogical Partition 관련된 CPU 정보를 출력한다.\n$ lparstat -i Node Name : devserver Partition Name : dev1 Partition Number : 1 Type : Dedicated-SMT-4 Mode : Capped Entitled Capacity : 2.00 Partition Group-ID : 32769 Shared Pool ID : - Online Virtual CPUs : 2 Maximum Virtual CPUs : 12 Minimum Virtual CPUs : 1 Online Memory : 61696 MB Maximum Memory : 61696 MB Minimum Memory : 30720 MB Variable Capacity Weight : - Minimum Capacity : 1.00 Maximum Capacity : 12.00 Capacity Increment : 1.00 Maximum Physical CPUs in system : 12 Active Physical CPUs in system : 12 Active CPUs in Pool : - Shared Physical CPUs in system : 0 Maximum Capacity of Pool : 0 Entitled Capacity of Pool : 0 Unallocated Capacity : - Physical CPU Percentage : 100.00% Unallocated Weight : - Memory Mode : Dedicated Total I/O Memory Entitlement : - Variable Memory Capacity Weight : - Memory Pool ID : - Physical Memory in the Pool : - Hypervisor Page Size : - Unallocated Variable Memory Capacity Weight: - Unallocated I/O Memory entitlement : - Memory Group ID of LPAR : - Desired Virtual CPUs : 2 Desired Memory : 61696 MB Desired Variable Capacity Weight : - Desired Capacity : 2.00 Target Memory Expansion Factor : - Target Memory Expansion Size : - Power Saving Mode : Disabled Sub Processor Mode : - Online Virtual CPUs 값을 주목해보자.\n$ lparstat -i [...] Online Virtual CPUs : 2 Maximum Virtual CPUs : 12 IBM AIX 전용 HMCHardware Management Console을 이용해 전체 12코어(Maximum Virtual CPUs)중 2코어(Online Virtual CPUs)만 활성화했기 때문에, Online Virtual CPUs 값이 2이다.\n","date":"Oct 15, 2021","permalink":"/blog/checking-cpu-info-in-aix/","section":"Blogs","summary":"개요 # IBM AIX 운영체제에서 CPU 관련 상세정보를 확인할 수 있다.","title":"AIX CPU 정보 및 전체 코어수 확인"},{"content":"개요 # CLI 환경에서 ipmitool 명령어로 HP iLO IP 주소를 확인할 수 있다.\nOS에 ipmitool 패키지만 설치되어 있다면 iLO IP를 원격으로도 확인 가능하다. 즉, 번거롭게 데이터센터에 들어갈 필요 없다.\n환경 # Vendor : HP Model : ProLiant DL380 Gen9 OS : Red Hat Enterprise Linux Server release 6.5 (Santiago) Shell : bash Package : ipmitool-1.8.11 배경지식 # IPMI (Intelligent Platform Management Interface) # 서버 관리를 위한 관리 인터페이스로 원격지나 로컬서버의 상태를 파악하고 제어할 수 있는 기능을 제공한다. 따라서 많은 수의 서버를 관리하는 경우에 아주 유용하게 사용이 될 수 있다.\n요즘 나오는 대부분의 서버용 메인보드에서는 지원하는 기능이다.\n해결법 # 1. ipmi 패키지 확인 # $ rpm -qa | grep ipmitool ipmitool-1.8.11-16.el6.x86_64 ipmitool 1.8.11 버전의 패키지가 설치되어 있다.\n2. ipmi 서비스 시작 # $ service ipmi start Starting ipmi drivers: [ OK ] ipmi 서비스를 시작한다.\n3. iLO IP 확인 # $ ipmitool lan print Set in Progress : Set Complete Auth Type Support : Auth Type Enable : Callback : : User : : Operator : : Admin : : OEM : IP Address Source : Static Address IP Address : 192.168.0.120 Subnet Mask : 255.255.255.0 MAC Address : 98:f2:b3:3b:fa:3e SNMP Community String : BMC ARP Control : ARP Responses Enabled, Gratuitous ARP Disabled Default Gateway IP : 0.0.0.0 802.1q VLAN ID : Disabled 802.1q VLAN Priority : 0 RMCP+ Cipher Suites : 0,1,2,3 Cipher Suite Priv Max : XuuaXXXXXXXXXXX : X=Cipher Suite Unused : c=CALLBACK : u=USER : o=OPERATOR : a=ADMIN : O=OEM 서버 하드웨어 제조사가 HP일 경우, ipmitool lan print 명령어 결과에 나오는 IP Address 는 iLO IP와 동일한 의미이다.\nipmi 서비스가 실행되어 있지 않을 경우, 제대로 결과를 반환하지 않으므로 결과가 잘 보이지 않는다면 ipmi 서비스 기동 상태를 확인한다.\n4. iLO IP 설정 # ## (1) 고정(Static) IP 설정 $ ipmitool lan set 1 ipsrc static ## (2) IP 설정 $ ipmitool lan set 1 ipaddr 10.10.10.10 ## (3) Subnet Mask 설정 $ ipmitool lan set 1 netmask 255.255.255.0 ## (4) Default Gateway IP 설정 $ ipmitool lan set 1 defgw ipaddr 10.10.10.1 서버 하드웨어 환경에 따라 channel 번호는 반드시 1이 아닌 다른 수일수도 있으므로 주의하자.\nipmitool lan set 명령어 형식\nipmitool lan set 명령어만 치면 설정 관련 명령어 전체 메뉴얼이 나오므로 참고할 수 있다.\n$ ipmitool lan set usage: lan set \u0026lt;channel\u0026gt; \u0026lt;command\u0026gt; \u0026lt;parameter\u0026gt; LAN set command/parameter options: ipaddr \u0026lt;x.x.x.x\u0026gt; Set channel IP address netmask \u0026lt;x.x.x.x\u0026gt; Set channel IP netmask macaddr \u0026lt;x❌x❌x:x\u0026gt; Set channel MAC address defgw ipaddr \u0026lt;x.x.x.x\u0026gt; Set default gateway IP address defgw macaddr \u0026lt;x❌x❌x:x\u0026gt; Set default gateway MAC address bakgw ipaddr \u0026lt;x.x.x.x\u0026gt; Set backup gateway IP address bakgw macaddr \u0026lt;x❌x❌x:x\u0026gt; Set backup gateway MAC address password \u0026lt;password\u0026gt; Set session password for this channel snmp \u0026lt;community string\u0026gt; Set SNMP public community string user Enable default user for this channel access \u0026lt;on|off\u0026gt; Enable or disable access to this channel alert \u0026lt;on|off\u0026gt; Enable or disable PEF alerting for this channel arp respond \u0026lt;on|off\u0026gt; Enable or disable BMC ARP responding arp generate \u0026lt;on|off\u0026gt; Enable or disable BMC gratuitous ARP generation arp interval \u0026lt;seconds\u0026gt; Set gratuitous ARP generation interval vlan id \u0026lt;off|\u0026lt;id\u0026gt;\u0026gt; Disable or enable VLAN and set ID (1-4094) vlan priority \u0026lt;priority\u0026gt; Set vlan priority (0-7) auth \u0026lt;level\u0026gt; \u0026lt;type,..\u0026gt; Set channel authentication types level = CALLBACK, USER, OPERATOR, ADMIN type = NONE, MD2, MD5, PASSWORD, OEM ipsrc \u0026lt;source\u0026gt; Set IP Address source none = unspecified source static = address manually configured to be static dhcp = address obtained by BMC running DHCP bios = address loaded by BIOS or system software cipher_privs XXXXXXXXXXXXXXX Set RMCP+ cipher suite privilege levels X = Cipher Suite Unused c = CALLBACK u = USER o = OPERATOR a = ADMIN O = OEM 5. iLO 설정 변경사항 적용 # $ ipmitool mc reset cold IP 설정 적용을 위해 BMCBaseboard Management Controller를 리부팅해준다.\nBMCBaseboard Management Controller\nIPMIIntelligent Platform Management Interface 유틸리티의 핵심 하드웨어 칩으로, 관리자들이 서버와 데스크톱에 대한 원격으로 긴급 모니터링하는데 활용한다.\nBMC 칩의 핵심기능은 CPU, Fan, 전원부(Power Supply), VGAVideo Graphic Array 카드 등 각 하드웨어 부품(Component)에 설치된 센서들과 통신을 통해 상태를 모니터링하고, 발생하는 이벤트를 기록하며 원격으로 복구, 제어 등의 기능을 수행할 수도 있다.\n이것으로 설정작업은 끝난다.\n","date":"Oct 13, 2021","permalink":"/blog/checking-ilo-ip-on-cli/","section":"Blogs","summary":"개요 # CLI 환경에서 ipmitool 명령어로 HP iLO IP 주소를 확인할 수 있다.","title":"ipmitool 명령어로 HP iLO IP 확인"},{"content":"","date":"Oct 8, 2021","permalink":"/tags/webtob/","section":"Tags","summary":"","title":"webtob"},{"content":"개요 # WebtoB 라이센스를 교체하는 절차이다.\n어느날 개발팀으로부터 WebtoB 라이센스 교체 요청이 들어온 것이 발단이다.\n배경지식 # 웹투비(WebtoB)\n한국의 티맥스소프트에서 개발한 웹서버 소프트웨어.\n기존 웹서버가 가지고 있는 구조적인 문제를 혁신적으로 개선해 성능 및 안정성에 탁월한 기능을 제공하는 차세대 웹서버 제품이다.\n주로 WAS 프로그램인 Jeus와 함께 사용한다.\n작업절차 # 1. 라이센스 다운로드 # WebtoB 라이센스는 티맥스소프트에서 운영하는 기술지원 페이지인 Technet에서 다운로드 받을 수 있다.\n2. 기존 라이센스 상태 확인 # 유저변경\n$ su - tmax $ 이번 작업은 tmax 계정으로 진행해야한다.\nWebtoB 홈 디렉토리로 이동\nWebtoB 홈 디렉토리는 서버 환경마다 다를 수 있다.\nWebtoB 홈 디렉토리 환경변수($WEBTOBDIR) 또한 설정되어 있지 않을 수도 있다.\n$ cd $WEBTOBDIR $ pwd /home/tmax/webtob WebToB 설치 경로로 이동한 후, license 디렉토리로 들어간다.\n$ cd license 기존 라이센스 상태 확인\nlicense 디렉토리 안에서 wsadmin 명령어를 실행해서 라이센스 상태를 확인할 수 있다.\n$ wsadmin -i license.dat ############################################## License Information (file: ./license.dat) ############################################## License seqno: WDE-xxxx-xxx-xxxx License issue date: 2021/5/14 License type: DEMO Expiration date: 2021/7/14 Edition: Enterprise License check by hostname: testserver Unlimited license 라이센스 만료일(Expiration date)을 보니 임시로 발급받은 데모 라이센스가 만료하기 직전이다.\n3. 라이센스 파일 업로드 # 기존 라이센스 파일명 변경\n$ cd /home/tmax/webtob/license $ mv license.dat license.dat.old 기존 임시 라이센스 파일명은 license.dat이다.\nlicense.dat을 license.dat.old 로 파일명 변경한다.\n신규 라이센스 파일명 변경\n신규 정식 라이센스 파일을 사용하기 위해서 파일명을 license.dat 으로 변경한다.\n$ mv license_standard_new.dat license.dat 4. 라이센스 파일 권한설정 # 기존 WebtoB 라이센스 파일(license.dat)을 참고하여 소유자와 그룹명을 동일하게 맞춰준다.\n$ chown tmax:web license.dat 라이센스 파일의 소유자는 tmax, 그룹은 web으로 설정한다.\n$ ls -l �� 66 [...] -rw-r--r-- 1 tmax web 80 6�� 23�� 13:44 license.dat -rw-r--r-- 1 tmax web 80 6�� 1�� 14:03 license.dat.old 기존 라이센스 파일(license.dat.old)의 소유자, 그룹과 동일하게 설정되었다.\n5. 웹서버 환경설정 컴파일 # $ wscfl -i http.m Current configuration: Number of client handler(HTH) = 1 Supported maximum user per node = 8163 Supported maximum user per handler = 8163 Successfully created the configuration file (/home/tmax/webtob/config/wsconfig) for node testserver. The host name of the running machine is testserver. 6. WebtoB 재시작 # 변경된 라이센스 환경설정을 적용하기 위해서는 WebtoB 서버를 내렸다가 올려야 한다.\nWebtoB 중지\nwsdown 명령어를 실행후 y 키를 눌러서 WebtoB 서버를 내린다.\n$ wsdown Do you really want to shut down WebtoB? (y : n): y WSDOWN for node(testserver) is starting: WSDOWN: SERVER(html:0) downed: Wed Jun 23 13:50:45 2021 WSDOWN: SERVER(html:1) downed: Wed Jun 23 13:50:45 2021 WSDOWN: HTL downed: Wed Jun 23 13:50:45 2021 WSDOWN: HTH downed: Wed Jun 23 13:50:45 2021 WSDOWN: WSM downed: Wed Jun 23 13:50:45 2021 WSDOWN: WebtoB is down WebtoB 시작\nWebtoB 서버를 다시 올린다.\n$ wsboot Booting WebtoB on node (testserver) Starting WSM at Wed Jun 23 13:50:48 2021 Starting HTL at Wed Jun 23 13:50:48 2021 Starting HTH at Wed Jun 23 13:50:48 2021 Current WebtoB Configuration: Number of client handlers (HTH) = 1 Supported maximum user per node = 8163 Supported maximum user per handler = 8163 Starting SVR(htmls) at Wed Jun 23 13:50:48 2021 Starting SVR(htmls) at Wed Jun 23 13:50:48 2021 WebtoB의 주요 프로세스인 wsm, htl, hth, htmls 가 정상적으로 구동된 걸 확인할 수 있다.\nWebtoB 주요 프로세스별 역할\nWSM(WebtoB System Manager)\n전체적인 WebtoB 시스템의 운용 프로세스로써 시스템의 운영 정보를 관리하고, HTL/HTH 프로세스 및 모든 서버 프로세스들을 관리하는 프로세스이다. WebtoB 시스템을 기동할 때 WSM은 가장 먼저 메모리에 로드되고 시스템이 종료될 때에는 가장 나중에 종료된다.\nHTL(HTTP Listener)\nHTL은 클라이언트와 WebtoB 간의 연결을 관리하는 Listener 프로세스이다. 클라이언트가 처음 WebtoB에 접속할 때에는 HTL과 연결을 맺어 통신이 이루어진다. 하지만 서비스 요청이 있을 경우 내부적으로 HTH와 연결이 되어 모든 서비스 처리가 이루어진다.\nHTH(HTTP Handler)\n클라이언트 핸들러라고도 하며 실질적으로 클라이언트와 서버의 업무 처리 프로세스 사이를 중계하는 프로세스이다. HTH는 서버 프로세스들과의 통신을 통해 모든 실제적인 데이터의 흐름을 관리한다. 즉 클라이언트의 서비스 요청을 받아 그에 해당하는 업무를 처리하며 그 결과를 수신하여 다시 클라이언트에게 되돌려준다.\nHTMLS(HTML Server)\nHTML 요청을 처리하는 HTML 서버 프로세스이다.\n7. 라이센스 \u0026amp; 프로세스 상태 확인 # 라이센스 적용 상태 확인\n$ wsadmin -i license.dat ############################################### License Information (file: license.dat) ############################################### License seqno: WRS-xxxx-xxx-xxxx License issue date: 2021/6/14 License type: REAL Edition: Standard License check by hostname: testserver 32 CPU license Standard Edition, 32 CPU License 로 정상 적용 확인.\nWebtoB 프로세스 상태 확인\n$ ps -ef | egrep \u0026#39;htm|htl|hth|htmls\u0026#39; tmax 1582 1 0 13:50:49 pts/5 0:00 htmls -l 0x2 -I webtob1_1577 -b 1577 -s html tmax 1578 1 0 13:50:49 pts/5 0:00 wsm -l 0x2 -I webtob1_1577 -b 1577 tmax 1581 1 0 13:50:49 pts/5 0:00 htmls -l 0x2 -I webtob1_1577 -b 1577 -s html tmax 1580 1 0 13:50:49 pts/5 0:00 hth -l 0x2 -I webtob1_1577 -b 1577 root 1579 1 0 13:50:49 pts/5 0:00 htl -l 0x2 -I webtob1_1577 -b 1577 wsm, hth, htl, htmls 프로세스가 구동중인 걸 확인했다.\n작업 끝.\n","date":"Oct 8, 2021","permalink":"/blog/changing-webtob-license/","section":"Blogs","summary":"개요 # WebtoB 라이센스를 교체하는 절차이다.","title":"WebtoB 라이센스 교체"},{"content":"개요 # ORA-12516 에러를 해결하기 위해 Oracle DB 리소스 파라미터의 Processes 값을 변경하고 적용할 수 있다.\n환경 # OS : Red Hat Enterprise Linux Server release 5.5 (Tikanga) Shell : bash DB : Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production 증상 # SQL\u0026gt; select count(*) from adm.share_table@link_datanet * ERROR at line 1: ORA-12516: TNS:listener could not find available handler with matching protocol stack 다른 서버에서 문제가 발생한 서버에 DB Link 조회시 에러 메세지를 반환해 조회할 수 없는 문제이다.\n원인 # ORA-12516: TNS:listener could not find available handler with matching protocol stack 에러가 발생하는 가장 대표적인 원인은 오라클 DB에 붙을 수 있는 프로세스 혹은 세션의 개수가 최대치에 도달했기 때문이다.\n이미 접속되어 있는 세션은 잘 동작하지만 새로운 프로그램이 DB에 접속할 때 DB는 이미 자신이 허용할 수 있는 연결의 최대치에 도달했기 때문에 에러를 반환하고 연결 실패가 발생한다.\n조치방법 # 1. sqlplus 접속 # $ sqlplus / as sysdba SQL*Plus: Release 10.2.0.4.0 - Production on Wed Oct 6 11:00:47 2021 Copyright (c) 1982, 2007, Oracle. All Rights Reserved. Connected to: Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - Production With the Partitioning, OLAP, Data Mining and Real Application Testing options SQL\u0026gt; 2. 리소스 파라미터 확인 # SQL\u0026gt; SET LINESIZE 200; SQL\u0026gt; SELECT * FROM V$RESOURCE_LIMIT; RESOURCE_NAME CURRENT_UTILIZATION MAX_UTILIZATION INITIAL_ALLOCATION LIMIT_VALUE ------------------------------ ------------------- --------------- -------------------- -------------------- processes 247 250 250 250 sessions 250 255 280 280 enqueue_locks 20 37 3840 3840 enqueue_resources 21 81 1452 UNLIMITED ges_procs 0 0 0 0 ges_ress 0 0 0 UNLIMITED ges_locks 0 0 0 UNLIMITED ges_cache_ress 0 0 0 UNLIMITED ges_reg_msgs 0 0 0 UNLIMITED ges_big_msgs 0 0 0 UNLIMITED ges_rsv_msgs 0 0 0 0 RESOURCE_NAME CURRENT_UTILIZATION MAX_UTILIZATION INITIAL_ALLOCATION LIMIT_VALUE ------------------------------ ------------------- --------------- -------------------- -------------------- gcs_resources 0 0 0 0 gcs_shadows 0 0 0 0 dml_locks 0 79 1232 UNLIMITED temporary_table_locks 0 3 UNLIMITED UNLIMITED transactions 3 21 308 UNLIMITED branches 2 18 308 UNLIMITED cmtcallbk 0 2 308 UNLIMITED sort_segment_locks 0 5 UNLIMITED UNLIMITED max_rollback_segments 12 15 308 65535 max_shared_servers 1 1 UNLIMITED UNLIMITED parallel_max_servers 0 0 160 3600 processes 값이 최대 250개 중 현재 247을 사용중이다.\nprocesses, sessions 값의 의미\nprocesses는 동시에 Oracle에 연결할 수 있는 OS 사용자 프로세스의 최대 수를 지정하는 값이다.\nprocesses 값에는 백그라운드 프로세스, Job 프로세스, 병렬 실행 프로세스 등의 수가 포함된다.\nprocesses 값을 변경하면 sessions 값도 자동으로 (processes x 1.1) + 5 로 설정된다.\n3. spfile 존재유무 확인 # SQL\u0026gt; SHOW PARAMETER SPFILE; NAME TYPE VALUE ------------------------------------ ----------- ------------------------------ spfile string /oracle/10g/dbs/spfiledba.ora spfile\nspfile은 Server Parameter File의 약자로 데이터베이스 관련 파라미터 설정 파일이다. spfile은 Oracle 9i 버전부터 지원되는 기능이므로 존재하지 않는다면 DB 버전을 확인한다. spfile은 ALTER SYSTEM 명령어를 통해 운영중에 파라미터를 수정 할 수 있다. spfile의 장점은 서버를 재시작하지 않고 운영중에 변경사항을 반영 가능하다는 점이다. spfile은 기본적으로 binary 파일이기 때문에 텍스트 편집기(vi editor, nano 등)를 이용해 수정하면 다시 사용할 수 없다. 4. spfile 설정 확인 # $ strings /oracle/10g/dbs/spfiledba.ora [...] *.open_cursors=300 *.pga_aggregate_target=241172480 *.processes=250 *.remote_login_passwordfile=\u0026#39;EXCLUSIVE\u0026#39; *.resource_limit=TRUE *.resource_manager_plan=\u0026#39;\u0026#39; *.service_names=\u0026#39;dba\u0026#39;,\u0026#39;DBA.REGRESS.RDBMS.DEV.US.ORACLE.COM\u0026#39; *.sga_target=608174080 *.undo_management=\u0026#39;AUTO\u0026#39; *.undo_retention=900 *.undo_tablespace=\u0026#39;UNDOTBS1\u0026#39; *.user_dump_dest=\u0026#39;/oracle/admin/dba/udump\u0026#39; *.utl_file_dir=\u0026#39;/BACKUP/logminer\u0026#39; spfile은 binary 형식의 파일이므로 cat 명령어가 아닌 strings 명령어를 사용해 읽어야한다.\nspfile을 확인해보니 현재 processes 값이 250이다.\n5. 리소스 파라미터 수정 # 명령어 형식\nSQL\u0026gt; alter system set processes=\u0026lt;INT\u0026gt; scope=spfile; 실제 명령어\nSQL\u0026gt; alter system set processes=400 scope=spfile; System altered. processes 값을 400으로 변경한다. System altered 메세지가 출력되면 정상 적용된 것이다.\n6. spfile 변경 확인 # $ strings /oracle/10g/dbs/spfiledba.ora [...] *.open_cursors=300 *.pga_aggregate_target=241172480 *.processes=400 *.remote_login_passwordfile=\u0026#39;EXCLUSIVE\u0026#39; *.resource_limit=TRUE *.resource_manager_plan=\u0026#39;\u0026#39; *.service_names=\u0026#39;dba\u0026#39;,\u0026#39;DBA.REGRESS.RDBMS.DEV.US.ORACLE.COM\u0026#39; *.sga_target=608174080 *.undo_management=\u0026#39;AUTO\u0026#39; *.undo_retention=900 *.undo_tablespace=\u0026#39;UNDOTBS1\u0026#39; *.user_dump_dest=\u0026#39;/oracle/admin/dba/udump\u0026#39; *.utl_file_dir=\u0026#39;/BACKUP/logminer\u0026#39; processes 값이 400으로 변경되었다.\n7. DB 재기동 # 중지 명령어\nSQL\u0026gt; shutdown immediate; Database closed. Database dismounted. ORACLE instance shut down. 기동 명령어\nSQL\u0026gt; startup; ORACLE instance started. Total System Global Area 608174080 bytes Fixed Size 1268896 bytes Variable Size 373293920 bytes Database Buffers 226492416 bytes Redo Buffers 7118848 bytes Database mounted. Database opened. 8. 리소스 파라미터 변경 확인 # SQL\u0026gt; SET LINESIZE 200; SQL\u0026gt; SELECT * FROM V$RESOURCE_LIMIT; RESOURCE_NAME CURRENT_UTILIZATION MAX_UTILIZATION INITIAL_ALLOCATION LIMIT_VALUE ------------------------------ ------------------- --------------- -------------------- -------------------- processes 77 86 400 400 sessions 80 89 445 445 enqueue_locks 19 30 5790 5790 enqueue_resources 19 48 2176 UNLIMITED ges_procs 0 0 0 0 ges_ress 0 0 0 UNLIMITED ges_locks 0 0 0 UNLIMITED ges_cache_ress 0 0 0 UNLIMITED ges_reg_msgs 0 0 0 UNLIMITED ges_big_msgs 0 0 0 UNLIMITED ges_rsv_msgs 0 0 0 0 RESOURCE_NAME CURRENT_UTILIZATION MAX_UTILIZATION INITIAL_ALLOCATION LIMIT_VALUE ------------------------------ ------------------- --------------- -------------------- -------------------- gcs_resources 0 0 0 0 gcs_shadows 0 0 0 0 dml_locks 0 51 1956 UNLIMITED temporary_table_locks 0 3 UNLIMITED UNLIMITED transactions 0 11 489 UNLIMITED branches 0 8 489 UNLIMITED cmtcallbk 0 1 489 UNLIMITED sort_segment_locks 0 3 UNLIMITED UNLIMITED max_rollback_segments 13 13 489 65535 max_shared_servers 1 1 UNLIMITED UNLIMITED parallel_max_servers 0 0 160 3600 22 rows selected. processes 의 LIMIT_VALUE 값이 250에서 400으로 변경되었다.\nprocesses 파라미터의 영향을 받는 sessions LIMIT_VALUE 값도 (processes x 1.1) + 5 의 결과인 445로 변경되었다.\n","date":"Oct 6, 2021","permalink":"/blog/oracle-processes-parameter/","section":"Blogs","summary":"개요 # ORA-12516 에러를 해결하기 위해 Oracle DB 리소스 파라미터의 Processes 값을 변경하고 적용할 수 있다.","title":"Oracle processes 파라미터 변경"},{"content":"개요 # 리눅스 서버에서 명령어Command를 이용해 메인보드의 메모리 슬롯 정보를 확인할 수 있다.\n확인 가능한 핵심 정보\n전체 메모리 슬롯 개수 사용중인 메모리 슬롯 개수 증설 가능한 최대 메모리 용량 메모리 제원정보 (Part Number, 규격, 사이즈 등) 환경 # OS : CentOS Linux release 7.6.1810 (Core) Shell : bash Package : dmidecode 3.1 작업절차 # 1. dmidecode 패키지 확인 # $ rpm -qa | grep dmidecode dmidecode-3.1-2.el7.x86_64 dmidecode 패키지가 설치되어 있는지 확인한다.\n2. 서버 메모리 확인 # $ free -m total used free shared buff/cache available Mem: 31670 2020 237 310 29412 28841 Swap: 32767 2 32765 -m은 메모리 용량을 MBMegabytes 단위로 출력한다.\n서버의 전체 메모리 용량은 32GB이다.\n3. 메인보드 메모리 슬롯 확인 (간단히) # $ dmidecode -t 17 | egrep \u0026#39;Memory|Size\u0026#39; Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: 16384 MB Memory Device Size: No Module Installed Memory Device Size: 16384 MB Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed Memory Device Size: No Module Installed 전체 24개 메모리 슬롯중 2개 메모리 슬롯을 사용중이다. 16GB 용량의 메모리 2개가 장착된 상태이다.\nNo Module Installed 는 빈 메모리 슬롯을 의미한다.\nTip. dmidecode -t 뒤의 숫자 의미 # -t 옵션 뒤의 숫자는 Type Number 를 의미한다.\nType Number 값만 다르게 주면 메모리 뿐만 아니라 다양한 정보를 얻을 수 있다.\ndmidecode의 Type Number 목록은 man dmidecode 명령어로 확인할 수 있다.\n$ man dmidecode [...] DMI TYPES The SMBIOS specification defines the following DMI types: Type Information ──────────────────────────────────────────── 0 BIOS 1 System 2 Baseboard 3 Chassis 4 Processor 5 Memory Controller 6 Memory Module 7 Cache 8 Port Connector 9 System Slots 10 On Board Devices 11 OEM Strings 12 System Configuration Options 13 BIOS Language 14 Group Associations 15 System Event Log 16 Physical Memory Array 17 Memory Device 18 32-bit Memory Error 19 Memory Array Mapped Address 20 Memory Device Mapped Address 21 Built-in Pointing Device 22 Portable Battery 23 System Reset 24 Hardware Security 25 System Power Controls 26 Voltage Probe 27 Cooling Device 28 Temperature Probe 29 Electrical Current Probe 30 Out-of-band Remote Access 31 Boot Integrity Services 32 System Boot 33 64-bit Memory Error 34 Management Device 35 Management Device Component 36 Management Device Threshold Data 37 Memory Channel 38 IPMI Device 39 Power Supply 40 Additional Information 41 Onboard Devices Extended Information 42 Management Controller Host Interface 메모리 슬롯 정보 확인시 자주 사용하는 Type Number\n16 : Physical Memory Array 17 : Memory Device 4. 메인보드 메모리 슬롯 확인 (자세히) # 메모리의 Part Number, 용량(Size), 폼팩터, 규격, 설정된 Clock 등의 유용한 정보를 얻을 수 있다.\n$ dmidecode -t 17 # dmidecode 3.1 Getting SMBIOS data from sysfs. SMBIOS 3.2.1 present. # SMBIOS implementations newer than version 3.1.1 are not # fully supported by this version of dmidecode. Handle 0x001B, DMI type 17, 84 bytes Memory Device Array Handle: 0x000F Error Information Handle: Not Provided Total Width: 72 bits Data Width: 64 bits Size: No Module Installed Form Factor: DIMM Set: None Locator: PROC 1 DIMM 1 Bank Locator: Not Specified Type: Other Type Detail: Synchronous Speed: Unknown Manufacturer: UNKNOWN Serial Number: Not Specified Asset Tag: Not Specified Part Number: NOT AVAILABLE Rank: Unknown Configured Clock Speed: Unknown Minimum Voltage: Unknown Maximum Voltage: Unknown Configured Voltage: Unknown [...] Handle 0x0022, DMI type 17, 84 bytes Memory Device Array Handle: 0x000F Error Information Handle: Not Provided Total Width: 72 bits Data Width: 64 bits Size: 16384 MB Form Factor: DIMM Set: 7 Locator: PROC 1 DIMM 8 Bank Locator: Not Specified Type: DDR4 Type Detail: Synchronous Registered (Buffered) Speed: 2933 MT/s Manufacturer: HPE Serial Number: Not Specified Asset Tag: Not Specified Part Number: P03050-091 Rank: 2 Configured Clock Speed: 2400 MT/s Minimum Voltage: 1.2 V Maximum Voltage: 1.2 V Configured Voltage: 1.2 V [...] Handle 0x0024, DMI type 17, 84 bytes Memory Device Array Handle: 0x000F Error Information Handle: Not Provided Total Width: 72 bits Data Width: 64 bits Size: 16384 MB Form Factor: DIMM Set: 9 Locator: PROC 1 DIMM 10 Bank Locator: Not Specified Type: DDR4 Type Detail: Synchronous Registered (Buffered) Speed: 2933 MT/s Manufacturer: HPE Serial Number: Not Specified Asset Tag: Not Specified Part Number: P03051-091 Rank: 1 Configured Clock Speed: 2400 MT/s Minimum Voltage: 1.2 V Maximum Voltage: 1.2 V Configured Voltage: 1.2 V [...] 5. 메인보드가 최대 지원하는 메모리 용량 확인 # $ dmidecode -t 16 # dmidecode 3.1 Getting SMBIOS data from sysfs. SMBIOS 3.2.1 present. # SMBIOS implementations newer than version 3.1.1 are not # fully supported by this version of dmidecode. Handle 0x000F, DMI type 16, 23 bytes Physical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: Multi-bit ECC Maximum Capacity: 3 TB Error Information Handle: Not Provided Number Of Devices: 12 Handle 0x0010, DMI type 16, 23 bytes Physical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: Multi-bit ECC Maximum Capacity: 3 TB Error Information Handle: Not Provided Number Of Devices: 12 해당 서버는 총 2개의 Memory Array로 구성되어 있다.\n각 Memory Array 당 지원하는 최대 메모리 용량(Maximum Capacity)은 3TB이다.\n각 Memory Array 당 전체 메모리 슬롯(Number of Devices)은 12개이다.\nMemory Array가 총 2개이므로, 서버 전체에 꽂을 수 있는 메모리 용량은 6TB, 메모리 슬롯은 총 24개이다.\n결론 # 서버 관리자에게 가장 중요한 업무는 서버의 네트워크 구성과 서비스 흐름 정보, 제원Spec을 제대로 파악하는 일이다.\ndmidecode는 다양한 하드웨어 정보를 수집할 수 있는 유용한 명령어이다. 실무에서 잘 활용만 한다면 보다 더 많은 하드웨어 정보를 수집할 수 있다는 사실을 명심하도록 하자.\n","date":"Sep 29, 2021","permalink":"/blog/checking-ram-slots-in-linux/","section":"Blogs","summary":"개요 # 리눅스 서버에서 명령어Command를 이용해 메인보드의 메모리 슬롯 정보를 확인할 수 있다.","title":"리눅스 메인보드 메모리 슬롯 확인"},{"content":"개요 # 오라클 DB에서 세션 상태를 확인하고 Lock이 발생한 비정상적인 DB 세션을 강제종료(Kill)할 수 있다.\n작성배경 # 가끔씩 운영중인 DB서버에서 Application 에러로 Query가 비정상적으로 반복실행되면서 시스템 부하가 급증하는 상황이 발생한다.\n어느날 개발자로부터 세션 강제종료(Kill) 조치 요청이 들어와 조치한 기록이다.\n환경 # OS : HP-UX B.11.31 ID : oracle Shell : sh Database : Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production 작업절차 # 1. oracle 계정 접속 # oracle 계정으로 먼저 로그인해야 한다.\n$ su - oracle 2. DB 접속 # sysdba 권한으로 Oracle DB에 접속한다.\n$ sqlplus / as sysdba SQL*Plus: Release 11.2.0.4.0 Production on Wed Sep 8 15:46:26 2021 Copyright (c) 1982, 2013, Oracle. All rights reserved. Connected to: Oracle Database 11g Enterprise Edition Release 11.2.0.4.0 - 64bit Production SQL\u0026gt; 3. DB 세션 상태확인 # Oracle Session\nClient Device Database Server ┌───────────────────┐ ┌────────────────────────────────────────┐ │ │ Acess │ │ │ ┌───────────────┐ │ Request │ ┌───────────────┐ ┌───────────────┐ │ │ │ Client ├─┼───(1)───────┼──► Oracle │ │ Client │ │ │ │ Application │ │ │ │ Listener │ │ Application │ │ │ └─────────────▲─┘ │ │ └───────┬───────┘ └───────▲───────┘ │ │ │ │ │ │ │ Session │ └───────────────┼───┘ │ (2) Create (1) established │ Session │ │ Process │ Local Connection │ established │ ┌───────▼───────┐ ┌───────▼───────┐ │ └───────(3)───────┼──► Server │ │ Server │ │ Remote │ │ Process │..│ Process │ │ Connection │ └───────────────┘ └───────────────┘ │ │ │ └────────────────────────────────────────┘ Oracle 데이터베이스는 사용자와 데이터베이스 접속이 이루어지면 세션을 생성한다. 세션은 사용자가 데이터베이스에 연결되어 있는 동안 계속 유지되고, 각 세션에는 Session ID(SID)와 Serial 번호(Serial#)가 부여된다. Session ID(SID)와 Serial 번호(Serial#)가 같이 부여되는 이유는 Session이 종료되었으나, 다른 세션이 동일한 SID를 갖고 시작되었을 때 세션 명령들이 정확한 세션에 적용될 수 있도록 하기 위해서이다. 세션이 사용자에 의해 작업중이라면 ACTIVE 상태로 작업을 하게된다.\nINACTIVE 상태는 세션은 연결되어있지만 작업을 하고있지 않는 상태를 의미한다. 명령어 형식\nSELECT SID, SERIAL#, USERNAME, PROGRAM, STATUS FROM V$SESSION WHERE SID=\u0026#39;\u0026lt;SID\u0026gt;\u0026#39;; 실행 명령어\nSQL\u0026gt; SELECT SID, SERIAL#, USERNAME, PROGRAM, STATUS 2 FROM V$SESSION 3 WHERE SID=\u0026#39;657\u0026#39;; SID SERIAL# USERNAME PROGRAM STATUS ---------- ---------- ------------------------------ ------------------------------------------------ -------- 657 13469 DEV SQL Developer ACTIVE Elapsed: 00:00:00.01 4. DB 세션 강제종료(Kill) # 세션을 강제로 끊어버리기 위해서는 Session ID(SID)와 Serial 번호(SERIAL#) 정보가 필요하다.\n명령어 형식\nALTER SYSTEM KILL SESSION \u0026#39;\u0026lt;SID\u0026gt;, \u0026lt;SERIAL#\u0026gt;\u0026#39;; 실행 명령어\nSQL\u0026gt; ALTER SYSTEM KILL SESSION \u0026#39;657, 13469\u0026#39;; System altered. Elapsed: 00:00:01.01 세션 강제종료(Kill)가 정상적으로 실행되었다.\n5. DB 세션 재확인 # 13469번 Serial Number(SERIAL#)를 가진 세션이 사라진 걸 확인할 수 있다.\nSQL\u0026gt; SELECT SID, SERIAL#, USERNAME, PROGRAM, STATUS 2 FROM V$SESSION 3 WHERE SID=\u0026#39;657\u0026#39;; SID SERIAL# USERNAME PROGRAM STATUS ---------- ---------- ------------------------------ ------------------------------------------------ -------- 657 13478 DEV oracle@devdb1 (TNS V1-V3) INACTIVE Elapsed: 00:00:00.00 조치 완료.\n","date":"Sep 8, 2021","permalink":"/blog/killing-locked-session-in-oracle-database/","section":"Blogs","summary":"개요 # 오라클 DB에서 세션 상태를 확인하고 Lock이 발생한 비정상적인 DB 세션을 강제종료(Kill)할 수 있다.","title":"오라클 세션 확인하고 Kill"},{"content":"개요 # 파일시스템 용량이 부족한 상황을 해결하기 위해 테이블스페이스의 데이터 파일을 다른 파일시스템 경로로 이동시켜 Filesystem Full로 인한 서비스 장애를 방지할 수 있다.\n문제점 # /vol1 파일시스템의 사용률이 90% 초과 Filesystem Full로 인한 서비스 장애 발생 가능성이 존재함 $ df -h Filesystem Size Used Avail Use% Mounted on /dev/sda5 63G 30G 30G 50% / /dev/sda2 39G 14G 23G 38% /oracle /dev/sda1 99M 12M 82M 13% /boot tmpfs 32G 0 32G 0% /dev/shm /dev/emcpowera 600G 538G 63G 90% /vol1 /dev/emcpowerb 600G 12G 589G 4% /vol2 해결방안 # 테이블스페이스의 데이터 파일을 여유공간이 있는 파일시스템으로 이동시킨 후 데이터 파일의 경로 설정을 변경한다.\n환경 # OS : Red Hat Enterprise Linux Server release 5.3 (Tikanga) Kernel : 2.6.18-128.el5 ID : oracle Shell : bash Database : Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - 64bit Production [DB] DB 이중화 : Oracle RAC(Real Application Clusters) 사용중 [DB] DB 인스턴스 : DEV1, DEV2 총 2개 사용중 조치방법 # 1. DB 접속 # # su - oracle $ sqlplus / as sysdba SQL*Plus: Release 10.2.0.4.0 - Production on Tue Sep 7 11:00:04 2021 Copyright (c) 1982, 2007, Oracle. All Rights Reserved. Connected to: Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - 64bit Production With the Partitioning, Real Application Clusters, Data Mining and Real Application Testing options 2. 데이터파일 정보 확인 # SQL 출력결과 사이즈 최적화\nSQL\u0026gt; SET LINESIZE 200; SQL\u0026gt; SET PAGESIZE 200; SQL\u0026gt; COL FILE_NAME FOR A40; SQL\u0026gt; COL TABLESPACE_NAME FOR A20; 테이블스페이스 데이터 파일 조회\nDBA_DATA_FILES 데이터 사전에서 테이블스페이스의 데이터 파일 크기와 테이블스페이스 사이즈를 확인할 수 있습니다.\nSQL\u0026gt; SELECT TABLESPACE_NAME, ROUND(BYTES/1024/1024/1024) AS GB, FILE_NAME FROM DBA_DATA_FILES WHERE FILE_NAME LIKE \u0026#39;%/vol1/DBF/%\u0026#39;; TABLESPACE_NAME GB FILE_NAME -------------------- ---------- ---------------------------------------- TEST_TS 32 /vol1/DBF/TESTINFO3.DBF TEST_TS 32 /vol1/DBF/TESTINFO4.DBF TEST_TS 32 /vol1/DBF/TESTINFO5.DBF TEST_TS 32 /vol1/DBF/TESTINFO6.DBF 데이터 파일 정보를 확인 후 exit 명령어로 sqlplus에서 빠져나온다.\nSQL\u0026gt; exit Disconnected from Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - 64bit Production With the Partitioning, Real Application Clusters, Data Mining and Real Application Testing options 3. 데이터 파일 실제경로 확인 # 파일시스템 경로에 데이터 파일이 실제로 존재하는 지 확인한다.\n# ls -lh /vol1/DBF/ total 128G -rw-r----- 1 oracle dba 32G Aug 9 2010 TESTINFO3.DBF -rw-r----- 1 oracle dba 32G Aug 9 2010 TESTINFO4.DBF -rw-r----- 1 oracle dba 32G Aug 9 2010 TESTINFO5.DBF -rw-r----- 1 oracle dba 32G Aug 9 2010 TESTINFO6.DBF 32GB 용량의 데이터 파일 4개가 존재한다. /vol1 에 있는 4개의 데이터 파일을 /vol2 로 이동시키자.\n4. 서비스 중지 # 데이터 파일을 이동시키기 전에 데이터베이스 서비스를 종료한다.\nsrvctl : Cluster의 자원(Resource)인 Listener, Instance, Disk Group, Network 등과 같은 오라클을 구성하는 오브젝트를 관리할 때 사용하는 명령어. 명령어 형식\n$ srvctl stop database -d \u0026lt;db_unique_name\u0026gt; 실행 명령어\n$ srvctl stop database -d DEV 만약 위 명령어가 실행되지 않을 경우 각 DB Instance 개별로 종료한다.\n명령어 형식\n$ srvctl stop instance -d \u0026lt;db_unique_name\u0026gt; -i \u0026lt;instance_name\u0026gt; 실행 명령어\n$ srvctl stop instance -d DEV -i DEV1 $ srvctl stop instance -d DEV -i DEV2 5. 데이터파일 복사 # /vol1에 있는 데이터 파일을 /vol2로 이동시킨다.\n$ cp -rp /vol1/DBF/TESTINFO3.DBF /vol2/DBF/ $ cp -rp /vol1/DBF/TESTINFO4.DBF /vol2/DBF/ $ cp -rp /vol1/DBF/TESTINFO5.DBF /vol2/DBF/ $ cp -rp /vol1/DBF/TESTINFO6.DBF /vol2/DBF/ cp 명령어 옵션 설명\n-r (\u0026ndash;recursive) : 하위 디렉토리 및 파일까지 모두 복사 -p : 원본파일의 소유주, 그룹, 권한, 시간정보를 보존하여 복사 6. 서비스 시작 # DB 접속\nsysdba 권한으로 DB에 접속한다.\n$ sqlplus / as sysdba DB 파일시스템 기동\nSQL\u0026gt; startup mount 데이터 파일 경로 변경설정(Rename)\nSQL\u0026gt; alter database rename file \u0026#39;/vol1/DBF/TESTINFO3.DBF\u0026#39; to \u0026#39;/vol2/DBF/TESTINFO3.DBF\u0026#39;; SQL\u0026gt; alter database rename file \u0026#39;/vol1/DBF/TESTINFO4.DBF\u0026#39; to \u0026#39;/vol2/DBF/TESTINFO4.DBF\u0026#39;; SQL\u0026gt; alter database rename file \u0026#39;/vol1/DBF/TESTINFO5.DBF\u0026#39; to \u0026#39;/vol2/DBF/TESTINFO5.DBF\u0026#39;; SQL\u0026gt; alter database rename file \u0026#39;/vol1/DBF/TESTINFO6.DBF\u0026#39; to \u0026#39;/vol2/DBF/TESTINFO6.DBF\u0026#39;; 테이블스페이스 데이터 파일 조회\nSQL\u0026gt; SELECT TABLESPACE_NAME, ROUND(BYTES/1024/1024/1024) AS GB, FILE_NAME FROM DBA_DATA_FILES WHERE FILE_NAME LIKE \u0026#39;%/vol2/DBF/%\u0026#39;; TABLESPACE_NAME GB FILE_NAME -------------------- ---------- ---------------------------------------- TEST_TS 32 /vol2/DBF/TESTINFO3.DBF TEST_TS 32 /vol2/DBF/TESTINFO4.DBF TEST_TS 32 /vol2/DBF/TESTINFO5.DBF TEST_TS 32 /vol2/DBF/TESTINFO6.DBF 데이터 파일의 경로FILE_NAME가 /vol1에서 /vol2로 정상 변경된 걸 확인할 수 있다.\nDB 서비스 시작\nSQL\u0026gt; alter database open; open의 의미\n온라인 데이터 파일과 온라인 Redo 로그 파일의 존재 및 정합성을 확인한 후, 해당 파일들을 열어 실제 데이터베이스를 사용할 수 있는 상태로 만든다.\n7. 데이터 파일 사용여부 확인 # $ fuser /vol1/DBF/TESTINFO3.DBF /vol1/DBF/TESTINFO4.DBF /vol1/DBF/TESTINFO5.DBF /vol1/DBF/TESTINFO6.DBF $ fuser로 이전 데이터 파일을 확인시 PID 값이 출력되지 않아야 정상이다.\nPID 값이 출력되지 않는다는 의미는 해당 데이터 파일에 접근하는 프로세스가 없다는 뜻이다.\n8. 기존 데이터 파일 삭제 # 삭제 작업은 인적 실수Human Fault를 유발할 수 있는 위험한 작업이다.\n기존 데이터 파일을 삭제하기 전에 현재 내가 작업하려는 데이터 파일의 번호, 절대경로를 잘 확인하고 삭제하도록 한다.\n$ rm -f /vol1/DBF/TESTINFO3.DBF $ rm -f /vol1/DBF/TESTINFO4.DBF $ rm -f /vol1/DBF/TESTINFO5.DBF $ rm -f /vol1/DBF/TESTINFO6.DBF 9. DB 서비스 종료 # 기존 데이터 파일을 삭제한 후 이상 여부를 확인하기 위해 DB를 다시 재기동해준다.\nSQL\u0026gt; shutdown immediate 10. DB 서비스 시작 # 명령어 형식\n$ srvctl start database -d \u0026lt;db_unique_name\u0026gt; 실행 명령어\n$ srvctl start database -d DEV 11. 사용률 확인 # 쉘 환경으로 빠져나온 다음, 파일시스템 사용률을 확인한다.\n$ df -h Filesystem Size Used Avail Use% Mounted on /dev/sda5 63G 30G 30G 50% / /dev/sda2 39G 16G 22G 41% /oracle /dev/sda1 99M 12M 82M 13% /boot tmpfs 32G 0 32G 0% /dev/shm /dev/emcpowera 600G 408G 193G 68% /vol1 /dev/emcpowerb 600G 143G 458G 24% /vol2 /vol1 파일시스템의 사용률이 90%에서 68%로 감소된 걸 확인할 수 있다.\n작업결과\n/vol1 : 작업 전 사용률 90% → 작업 후 사용률 68% /vol2 : 작업 전 사용률 4% → 작업 후 사용률 24% 결론 # 테이블스페이스의 데이터 파일을 옮기는 작업을 통해 파일시스템 Full로 인한 서비스 장애를 예방할 수 있다.\n인적 실수Human Fault가 발생하기 쉬운 작업이기 때문에 데이터 파일 이동, 삭제 과정에서는 재차 확인 후 명령어를 실행해야한다.\n","date":"Sep 8, 2021","permalink":"/blog/moving-datafile-in-oracle-database/","section":"Blogs","summary":"개요 # 파일시스템 용량이 부족한 상황을 해결하기 위해 테이블스페이스의 데이터 파일을 다른 파일시스템 경로로 이동시켜 Filesystem Full로 인한 서비스 장애를 방지할 수 있다.","title":"오라클 데이터 파일 옮기기"},{"content":"증상 # DB 계정의 패스워드를 갱신하는 상황에서 ORA-28007: the password cannot be reused 에러 메세지가 출력되면서 패스워드를 갱신할 수 없는 문제를 해결한다.\n에러 메세지\nSQL\u0026gt; ALTER USER MAXGAUGE IDENTIFIED BY \u0026#34;...\u0026#34;; ALTER USER MAXGAUGE IDENTIFIED BY \u0026#34;...\u0026#34;; * ERROR at line 1: ORA-28007: the password cannot be reused 원인 # ORA-28007: the password cannot be reused는 동일한 패스워드를 Limit 개수만큼 변경했을 때 발생하는 오류이다.\n환경 # OS : Red Hat Enterprise Linux Server release 5.10 (Tikanga) ID : oracle Shell : bash Database : Oracle Database 11g Release 11.2.0.3.0 - Production 해결법 # 1. Oracle 접속 # oracle 계정으로 접속한다.\n$ id uid=501(oracle) gid=501(dba) groups=501(dba) sysdba 권한을 얻어 sys 계정 접속한다.\n$ sqlplus sys as sysdba SQL*Plus: Release 11.2.0.3.0 Production on Thu Sep 2 13:28:01 2021 Copyright (c) 1982, 2011, Oracle. All rights reserved. Enter password: \u0026lt;Password 입력\u0026gt; Connected to: Oracle Database 11g Release 11.2.0.3.0 - Production SQL\u0026gt; Connected to: 메세지와 함께 Oracle DB에 정상 접속된 걸 확인할 수 있다.\n2. 계정의 프로필 확인 # SQL\u0026gt; set linesize 200; SQL\u0026gt; SELECT USERNAME, ACCOUNT_STATUS, PROFILE FROM DBA_USERS WHERE USERNAME=\u0026#39;MAXGAUGE\u0026#39;; USERNAME ACCOUNT_STATUS PROFILE ------------------------------ -------------------------------- ------------------------------ MAXGAUGE EXPIRED DEFAULT MAXGAUGE 계정이 만료된(EXPIRED) 상태이며, Default Profile 을 사용하는 걸 확인할 수 있다.\n3. 프로필 설정확인 # SQL\u0026gt; SELECT * FROM DBA_PROFILES WHERE PROFILE=\u0026#39;DEFAULT\u0026#39;; PROFILE RESOURCE_NAME RESOURCE LIMIT ------------------------------ -------------------------------- -------- ---------------------------------------- DEFAULT COMPOSITE_LIMIT KERNEL UNLIMITED DEFAULT SESSIONS_PER_USER KERNEL UNLIMITED DEFAULT CPU_PER_SESSION KERNEL UNLIMITED DEFAULT CPU_PER_CALL KERNEL UNLIMITED DEFAULT LOGICAL_READS_PER_SESSION KERNEL UNLIMITED DEFAULT LOGICAL_READS_PER_CALL KERNEL UNLIMITED DEFAULT IDLE_TIME KERNEL UNLIMITED DEFAULT CONNECT_TIME KERNEL UNLIMITED DEFAULT PRIVATE_SGA KERNEL UNLIMITED DEFAULT FAILED_LOGIN_ATTEMPTS PASSWORD 3 DEFAULT PASSWORD_LIFE_TIME PASSWORD 90 DEFAULT PASSWORD_REUSE_TIME PASSWORD 356 DEFAULT PASSWORD_REUSE_MAX PASSWORD 10 DEFAULT PASSWORD_VERIFY_FUNCTION PASSWORD VERIFY_FUNCTION DEFAULT PASSWORD_LOCK_TIME PASSWORD 3 DEFAULT PASSWORD_GRACE_TIME PASSWORD 5 암호 재사용 방지와 관련된 설정값\nDatabase SQL Reference - CREATE PROFILE\nParameter 이름 설명 PASSWORD_REUSE_TIME 주어진 날 수 동안 암호를 재사용할 수 없도록 지정. (PASSWORD_REUSE_TIME 값이 5일 경우, 5일 안에는 똑같은 암호를 사용할 수 없다.) PASSWORD_REUSE_MAX 사용했던 암호를 기억하는 횟수. 사용했던 암호를 재사용하는 걸 방지하는 목적의 설정값. 위 두 설정값은 사용자가 지정된 기간 동안 암호를 재사용하지 못하게 한다. 이 두 파라미터는 반드시 같이 설정해야 동작한다. 4. 보안설정 해제 # 불가피하게 패스워드 재사용이 필요할 경우, 암호 재사용 방지와 관련된 설정값을 해제한다.\nPASSWORD_REUSE_TIME과 PASSWORD_REUSE_MAX 파라미터를 모두 UNLIMITED로 설정하면 데이터베이스가 두 파라미터를 모두 무시한다. SQL\u0026gt; ALTER PROFILE DEFAULT LIMIT PASSWORD_REUSE_TIME UNLIMITED; Profile altered. PASSWORD_REUSE_TIME 파라미터를 비활성화한다.\nSQL\u0026gt; ALTER PROFILE DEFAULT LIMIT PASSWORD_REUSE_MAX UNLIMITED; Profile altered. PASSWORD_REUSE_MAX 파라미터를 비활성화한다.\n5. 패스워드 갱신 # 명령어 형식\nSQL\u0026gt; ALTER USER \u0026lt;계정 이름\u0026gt; IDENTIFIED BY \u0026#34;\u0026lt;갱신할 패스워드\u0026gt;\u0026#34;; 명령어 예시\nSQL\u0026gt; ALTER USER MAXGAUGE IDENTIFIED BY \u0026#34;change!me!please\u0026#34;; User altered. 새 패스워드로 갱신 완료되었다.\n6. 계정 상태 확인 # SQL\u0026gt; SELECT USERNAME, ACCOUNT_STATUS, PROFILE FROM DBA_USERS WHERE USERNAME=\u0026#39;MAXGAUGE\u0026#39;; USERNAME ACCOUNT_STATUS PROFILE ------------------------------ -------------------------------- ------------------------------ MAXGAUGE OPEN DEFAULT MAXGAUGE 계정이 잠금해제OPEN 상태로 변경되었다.\n7. 보안설정 원복하기 # 패스워드 갱신을 완료했으면 해제했던 암호 재사용 방지 관련 설정값을 다시 돌려놓는다.\nSQL\u0026gt; ALTER PROFILE DEFAULT LIMIT PASSWORD_REUSE_TIME 356; Profile altered. SQL\u0026gt; ALTER PROFILE DEFAULT LIMIT PASSWORD_REUSE_MAX 10; Profile altered. 결론 # 불가피하게 패스워드 재사용이 필요할 경우 위 방법을 사용한다.\n그러나 무분별한 패스워드 재사용은 보안 취약점이다.\n번거롭더라도 완전히 새로운 패스워드로 갱신하는 것이 안전하며 정상적인 조치 방법이다.\n","date":"Sep 4, 2021","permalink":"/blog/tip-for-oracle-28007-the-password-cannot-be-reused/","section":"Blogs","summary":"증상 # DB 계정의 패스워드를 갱신하는 상황에서 ORA-28007: the password cannot be reused 에러 메세지가 출력되면서 패스워드를 갱신할 수 없는 문제를 해결한다.","title":"ORA-28007 조치방법"},{"content":"개요 # 가상머신에 VMware Tools 를 설치할 수 있다.\n배경지식 # VMware Tools # 가상 머신에 설치된 게스트(Guest) 운영체제의 성능과 가상 머신의 관리 기능을 향상시키기 위한 소프트웨어 집합이다.\nVMware Tools 데몬은 가상머신의 IP, Hostname 등의 정보를 수집해 가상화 관리 프로그램(vSphere Client)에 표출해준다.\n원활한 가상화 시스템 운영을 위해서 모든 가상머신에 VMware Tools를 설치가 필요하다.\n환경 # ESXi : ESXi 5.x vSphere Client 관리자 계정에 로그인되어 있어야 함 가상머신의 ID : root 가상머신의 Shell : bash 작업절차 # 1. root 로그인 # VMware Tools 설치 작업은 root 계정으로만 가능하다. 가상머신에 접속 후 root 계정에 로그인한다.\n$ su - Password: [패스워드 입력] # 2. VMware Tools 설치 / 업그레이드 버튼 클릭 # vSphere Client 접속 → \u0026lsquo;가상머신\u0026rsquo; 오른쪽 마우스 클릭 → \u0026lsquo;VMware Tools 설치 / 업그레이드\u0026rsquo; 버튼 클릭\n3. 리눅스 서버에 CD-ROM mount # # mount /dev/cdrom /mnt mount: block device /dev/sr0 is write-protected, mounting read-only 4. VMware Tools 설치파일 복사 # CD-ROM 을 mount한 경로인 /mnt 로 이동한다.\n# cd /mnt # ls VMwareTools-9.0.10-1481436.tar.gz manifest.txt VMwareTools-9.0.10-1481436.tar.gz 파일을 /tmp 로 복사한다.\n# cp VMwareTools-9.0.10-1481436.tar.gz /tmp 5. VMware Tools 설치파일 압축해제 # # tar -zxvf VMwareTools-9.0.10-1481436.tar.gz [...] vmware-tools-distrib/etc/scripts/vmware/ vmware-tools-distrib/etc/scripts/vmware/network 6. VMware Tools 설치 # VMware Tools 설치시 방법은 2가지가 있다.\n기본 설정값(Default) 설치 커스텀 설치 각자 환경에 맞게 설치한다. 특별한 경우가 아니라면, 일반적으로 기본 설정값 설치를 해도 모니터링 및 정보수집 기능에 전혀 문제없다.\n방법1. 기본 설정값(Default) 설치\n# cd /tmp/vmware-tools-distrib/ # ls FILES INSTALL bin doc etc installer lib vmware-install.pi # ./vmware-install.pi -default VMware Tools를 기본 설정값(-default)으로 설치한다.\n방법2. 커스텀 설치\n# cd /tmp/vmware-tools-distrib/ # ls FILES INSTALL bin doc etc installer lib vmware-install.pi # ./vmware-install.pi 설치 과정에서 여러가지 설치환경 값들을 물어보는데, 모두 엔터를 눌러 Default 값으로 설정한다.\n[...] Enjoy, --the VMware team Found VMware Tools CDROM mounted at /mnt. Ejecting device /dev/sr0 ... VMware Tools가 정상적으로 설치됐을 때 마지막 문구.\n7. VMware Tools 동작여부 확인 # # ps -ef | grep vmtoolsd root 19777 1 0 13:17 ? 00:00:00 /usr/sbin/vmtoolsd VMware Tools 데몬(vmtoolsd)이 정상 실행중인 걸 확인한다.\n# vmtoolsd -v VMware Tools daemon, version 9.0.10.29005 (build-1481436) VMware Tools 데몬이 버전 정보를 정상 반환하는 거 확인.\n작업 끝!\n참고자료 # VMware Tools 설치\n위 링크는 VMware 공식문서로 한국어로 친절하게 작성된 가이드다.\n위 내용도 같이 참조하면 문제 해결에 도움이 된다.\n","date":"Aug 30, 2021","permalink":"/blog/installing-vmware-tools/","section":"Blogs","summary":"개요 # 가상머신에 VMware Tools 를 설치할 수 있다.","title":"VMware Tools 설치"},{"content":"환경 # OS : CentOS Linux release 8.3.2011 Shell : Bash Package : cockpit 증상 # 서버에 로그인할 때마다 세션에 불필요한 Welcome Message 출력된다. Cockpit 을 사용하지 않으니 메세지를 표출되지 않도록 영구 삭제한다.\nActivate the web console with: systemctl enable --now cockpit.socket [testserver ~]# 배경지식 # Cockpit # Cockpit의 사전적 의미는 비행기 조종석을 의미한다.\n리눅스 환경에서 Cockpit 은 Fedora Project 에서 나온 웹 UI 기반의 모니터링 및 관리 툴이다.\nCockpit을 이용하면 별도 프로그램 설치 없이도 웹 브라우저를 통해 리눅스 서버에 원격접속하고 관리할 수 있다. 참고로 Cockpit Web Console의 기본포트는 TCP 9090이다.\n서비스 최소화의 원칙 # 모든 서버는 최소한의 서비스로만 운영되어야 한다. 아무리 기능이 유용해도 사용하지 않는 서비스라면 반드시 비활성화(Disable) 조치를 한다. 서비스 최소화의 목적에는 크게 2가지가 있다. 보안 향상과 불필요한 자원 낭비를 방지하기 위함이다.\n내가 근무하는 데이터센터는 이 원칙을 지키기 위해 Cockpit 서비스를 사용하지 않는다.\n해결법 # 1. motd 디렉토리 이동 # motd란 message of the day의 줄임말로 유저가 로그인시 특정 메시지를 뛰우는걸 말한다. CentOS에서는 /etc/motd 파일에 전달할 메시지를 적어 놓는다.\nmotd.d 는 어플리케이션 개별로 motd 메세지를 모아놓은 디렉토리이다.\n[testserver ~]# cd /etc/motd.d 2. motd 파일 확인 # motd.d 디렉토리 안에 cockpit 링크 파일이 들어있는 걸 확인할 수 있다.\n[testserver motd.d]# ls lrwxrwxrwx. 1 root root 17. 8월 25일 2020 cockpit -\u0026gt; /run/cockpit/motd 3. motd 파일 삭제 # [testserver motd.d]# rm -f cockpit [testserver motd.d]# rm -f : 삭제 여부를 묻지 않고 강제(force)로 파일을 삭제한다.\n4. 로그인 테스트 # 다시 로그인해보면 Cockpit 관련 motd 메세지가 출력되지 않는다.\nLast login: Tue May 11 20:09:04 2021 from 10.10.10.10 [testserver ~]$ 참고자료 # https://www.programmersought.com/article/14417225140/\n","date":"Aug 23, 2021","permalink":"/blog/disabling-motd-of-cockpit/","section":"Blogs","summary":"환경 # OS : CentOS Linux release 8.","title":"Cockpit 로그인 메세지 비활성화"},{"content":"개요 # Brocade 제조사의 SAN Switch 장비에서 Zoning 설정 후 적용할 수 있다.\n환경 # Model : Brocade DS6510 Kernel : 2.6.14.2 Fabric OS : v8.0.2d 절차 # 1. 장비 접속 # Serial Cable 을 이용한 연결 또는 SSH 기본포트(TCP/22)로 접속한다.\n2. Alias 생성 # Alias 생성 작업은 필수는 아니지만, 관리자 메뉴얼 기준으로 매우 권장되는 절차다.\n사람이 이해하기 쉽게 Zone에 속한 구성 장비들의 이름을 적어놓는 절차이기 때문이다.\nAlias : Switch 의 Port 번호 혹은 WWN 을 알기쉽게 별칭(Alias)으로 지정하여, Zone Member의 확인을 쉽게 합니다. \u0026gt; alicreate \u0026#34;WEBWAS1_P2\u0026#34;, \u0026#34;21:00:00:24;ff:33:a3:4c\u0026#34; 3. Zone 생성 # \u0026gt; zonecreate \u0026#34;WEBWAS1_P2_SN87118_S5_P1_V_SN87119_S5_P1_V\u0026#34;, \u0026#34;WEBWAS1_P2; SN87118_S5_P1_V; SN87119_S5_P1_V\u0026#34; 4. cfg 추가 # 명령어 형식\n\u0026gt; cfgadd \u0026#34;\u0026lt;config_name\u0026gt;\u0026#34;, \u0026#34;\u0026lt;zone_name\u0026gt;; \u0026lt;zone_name\u0026gt;; ...; \u0026lt;zone_name\u0026gt;\u0026#34; 기존에 존재하는 config에 새로 만든 zone을 추가한다. 여기서 cfg는 설정(Configuration)을 의미한다.\n명령어 예시\n\u0026gt; cfgadd \u0026#34;ds6510_top_cfg\u0026#34;, \u0026#34;WEBWAS1_P2_SN87118_S5_P1_V_SN87119_S5_P1_V\u0026#34; 5. 설정 저장 # 설정을 저장한다.\n\u0026gt; cfgsave [...] Do you want to save the Defined zoning configuration only? (yes, y, no, n): [no] cfgsave 명령어를 수행하지 않을 경우, 휘발성 영역에 Config가 존재하므로 Switch 리부팅 시 설정한 Zone 정보가 사라지므로 주의한다.\n6. cfg 적용 # \u0026gt; cfgenable \u0026#34;ds6510_top_cfg\u0026#34; ","date":"Aug 23, 2021","permalink":"/blog/configuring-zoning-on-brocade-fc-switch-using-ds6510/","section":"Blogs","summary":"개요 # Brocade 제조사의 SAN Switch 장비에서 Zoning 설정 후 적용할 수 있다.","title":"Brocade DS6510에서 Zoning 설정하기"},{"content":"","date":"Aug 23, 2021","permalink":"/tags/san/","section":"Tags","summary":"","title":"san"},{"content":"개요 # CentOS 7 + HP ProLiant 서버 조합에 ssacli(Smart Storage Administrator CLI) 패키지를 설치한다. ssacli(Smart Storage Administrator CLI) 명령어를 실행해 RAID 구성정보와 디스크 용량, RPM, Serial Number 등의 디스크 상세정보를 확인할 수 있다. 환경 # OS : CentOS Linux release 7.6.1810 (Core) Kernel : 3.10.0 Shell : bash 절차 # 1. 패키지 설치파일 다운로드 # hpssacli\nHPE SSA CLI(HPE Smart Storage Administrator CLI)는 HPE Smart Arrays 컨트롤러 및 HPE SAS HBA를 신속하게 설치, 구성 및 관리하는 단일 인터페이스를 제공하는 Storage 관리 프로그램이다.\n줄여서 hpssacli 라고 부른다.\n설치파일 다운로드 링크\nHPE Software Delivery Repository\n내 경우는 https://downloads.linux.hpe.com/SDR/repo/mcp/centos/8/x86_64/current/ 에 위치한 ssacli-4.17-6.0.x86_64.rpm 15MB 용량의 패키지 설치파일을 다운로드 받았다.\n각 서버의 운영체제, CPU 아키텍쳐가 다양하기 때문에 본인의 환경에 맞게 패키지를 찾아서 다운로드 받는다.\n이 글에서 운영체제와 CPU 아키텍쳐의 종류 및 확인방법까지는 다루지 않는다.\n2. 패키지 설치파일 업로드 # FTP 또는 SFTP를 이용해 서버에 패키지 설치파일을 업로드한다.\n$ pwd /root/rpms 내 경우 패키지 설치파일의 이름은 ssacli-4.17-6.0.x86_64.rpm 이다. ssacli v4.17 패키지 파일의 용량은 약 15MB 이다.\n$ ls -lh /root/rpms -rw-rw-r-- 1 testuser testuser 15M 8월 3 10:41 ssacli-4.17-6.0.x86_64.rpm 3. ssacli 설치 # 패키지 설치\nRed Hat Enterprise Linux와 CentOS의 경우 패키지 관리자로 rpm을 사용한다.\n패키지 설치 과정에서 발생할 수 있는 권한 문제를 방지하기 위해, 반드시 root 계정으로 전환한 후 패키지 설치 명령어 rpm -ivh \u0026lt;패키지 이름\u0026gt;.rpm 를 실행한다.\n$ rpm -ivh ssacli-4.17-6.0.x86_64.rpm 준비 중... ################################# [100%] Updating / installing... 패키지 설치 확인\n설치된 ssacli RPM 패키지 정보를 확인한다.\n$ rpm -qi ssacli Name : ssacli Version : 4.17 Release : 6.0 Architecture: x86_64 Install Date: 2021년 08월 03일 (화) 오전 10시 47분 35초 Group : Applications/System Size : 42277683 License : See ssacli.license Signature : RSA/SHA256, 2020년 01월 14일 (화) 오전 04시 23분 30초, Key ID cxxxaddexxcxbxxx Source RPM : ssacli-4.17-6.0.src.rpm Build Date : 2020년 01월 14일 (화) 오전 04시 26분 18초 Build Host : abls12ex6404.sde.rdlabs.hpecorp.net Relocations : (not relocatable) Packager : Hewlett Packard Enterprise Development LP Vendor : Hewlett Packard Enterprise Development LP URL : http://www.hpe.com Summary : Command Line Smart Storage Administrator Description : The Command Line Smart Storage Administrator is the storage management application suite for Proliant Servers. Description 란에는 해당 패키지에 대한 간략할 설명이 적혀있다. HP ProLiant 모델에 제공되는 스토리지 관리 프로그램이라고 한다.\n[...] Description : The Command Line Smart Storage Administrator is the storage management application suite for Proliant Servers. Smart Storage Administrator 모드 접속\n$ ssacli Smart Storage Administrator CLI 4.17.6.0 Detecting Controllers...Done. Type \u0026#34;help\u0026#34; for a list of supported commands. Type \u0026#34;exit\u0026#34; to close the console. =\u0026gt; SSA(Smart Storage Administrator) 모드에 접속하면 프롬프트가 =\u0026gt; 모양으로 바뀐다.\n4. RAID 컨트롤러 구성 상태 확인 # 명령어 형식\n=\u0026gt; ctrl all show config Shell에서 ssacli 모드에 접속하지 않고 ssacli 명령어를 한 번에 실행하는 방법은 아래와 같다.\n$ ssacli ctrl all show config 명령어 결과\n=\u0026gt; ctrl all show config HPE Smart Array P408i-a SR Gen10 in Slot 0 (Embedded) (sn: PWXKVxxxHDKCQx) Internal Drive Cage at Port 1I, Box 1, OK Internal Drive Cage at Port 2I, Box 0, OK Port Name: 1I (Mixed) Port Name: 2I (Mixed) Array A (SAS, Unused Space: 0 MB) logicaldrive 1 (558.88 GB, RAID 1, OK) physicaldrive 1I:1:1 (port 1I:box 1:bay 1, SAS HDD, 600 GB, OK) physicaldrive 1I:1:2 (port 1I:box 1:bay 2, SAS HDD, 600 GB, OK) SEP (Vendor ID HPE, Model Smart Adapter) 379 (WWID: xxxxxEC0xxCxBAxx, Port: Unknown) 600GB 물리 디스크 2개가 RAID 1으로 구성된 걸 확인할 수 있다. 모든 디스크 상태는 정상(OK)이다.\n5. 특정 PD(Physical Drive) 상태 확인 # 명령어 형식\n=\u0026gt; ctrl slot=\u0026lt;slot_number\u0026gt; pd \u0026lt;PortNumber\u0026gt;:\u0026lt;BoxNumber\u0026gt;:\u0026lt;BayNumber\u0026gt; show ctrl all show config 명령어에서 확인한 Port, Box, Bay 번호를 토대로 특정 디스크의 정보를 확인할 수 있다.\n예를 들어 1I:1:2 의 경우 1I Port, 1번 Box, 2번 bay에 장착한 물리 디스크이다.\n명령어 결과\n=\u0026gt; ctrl slot=0 pd 1I:1:1 show HPE Smart Array P408i-a SR Gen10 in Slot 0 (Embedded) Array A physicaldrive 1I:1:1 Port: 1I Box: 1 Bay: 1 Status: OK Drive Type: Data Drive Interface Type: SAS Size: 600 GB Drive exposed to OS: False Logical/Physical Block Size: 512/512 Rotational Speed: 10000 Firmware Revision: HPD2 Serial Number: xxUxAxELFFxF WWID: 00000000E000E000 Model: HP EG000600JWJNH Current Temperature (C): 37 Maximum Temperature (C): 42 PHY Count: 2 PHY Transfer Rate: 12.0Gbps, Unknown PHY Physical Link Rate: 12.0Gbps, Unknown PHY Maximum Link Rate: 12.0Gbps, 12.0Gbps Drive Authentication Status: OK Carrier Application Version: 11 Carrier Bootloader Version: 6 Sanitize Erase Supported: True Sanitize Estimated Max Erase Time: 4 hour(s), 1 minute(s) Unrestricted Sanitize Supported: True Shingled Magnetic Recording Support: None Drive Unique ID: 00000000E000E000 명령어 결과에서 인터페이스 종류(Interace Type), 디스크 용량(Size), RPM(Rotational Speed), Serial Number, 전송속도(PHY Transfer Rate) 등 상세한 정보까지 확인이 가능하다. 심지어 디스크의 현재 온도(Current Temperature)까지 확인 가능하다.\n6. SSA 모드 나가기 # exit 명령어를 실행해서 SSA 모드를 빠져나올 수 있다.\n=\u0026gt; exit $ 이걸로 SSA 설치 및 간단 사용법은 익혔다.\n결론 # 서버 관리 업무 중 중요 테스크로 뽑히는 것 중 하나는 서버의 물리 디스크 및 RAID 구성 관리이다.\nHPE에서 만든 서버의 경우, SSA CLI를 통해 디스크 제원과 상태, RAID 상태를 자세히 확인할 수 있다.\n무엇보다 ssacli 명령어를 잘 활용하면 서버에 꽂힌 디스크 수량이나 모델명을 조사하러 데이터센터에 들어갈 일이 없다.\n이 글을 읽는 분들은 대부분 시스템 엔지니어나 어드민일 것 같은데, 여러 대의 서버를 관리하는거 기왕이면 HPE 서버에는 모두 SSA CLI를 깔아놓도록 하자.\n처음엔 패키지 설치 과정이 귀찮겠지만, 장기적 관점에서 훨씬 서버 관리가 편해질 것이다.\n","date":"Aug 3, 2021","permalink":"/blog/installing-hpssacli-on-centos7/","section":"Blogs","summary":"개요 # CentOS 7 + HP ProLiant 서버 조합에 ssacli(Smart Storage Administrator CLI) 패키지를 설치한다.","title":"CentOS 7에서 hpssacli 설치 및 사용법"},{"content":"개요 # EMC Networker 솔루션을 통해 백업을 받기 위해 백업 에이전트를 설치할 수 있다.\n환경 # OS : Red Hat Enterprise Linux release 6.5 Shell : bash 설치절차 # 1. rpm 설치파일 업로드 # [root@testserver1 rpm]# ls lgtoclnt-18.2.0.5-1.x86_64.rpm lgtoxtdclnt-18.2.0.5-1.x86_64.rpm 2. 패키지 설치 # 반드시 lgtoclnt 를 먼저 설치후, lgtoxtdclnt 를 설치해야 정상적으로 완료된다.\nlgtoclnt 패키지 설치\n[root@testserver1 rpm]# rpm -ivh lgtoclnt-18.2.0.5-1.x86_64.rpm warning: lgtoclnt-18.2.0.5-1.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c5dfe03d: NOKEY Preparing... ########################################### [100%] 1:lgtoclnt ########################################### [100%] lgtoclnt 패키지 설치가 정상적으로 완료되었다.\nlgtoxtdclnt 패키지 설치\n[root@testserver1 rpm]# rpm -iv lgtoxtdclnt-18.2.0.5-1.x86_64.rpm warning: lgtoxtdclnt-18.2.0.5-1.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c5dfe03d: NOKEY Preparing... ########################################### [100%] 1:lgtoxtdclnt ########################################### [100%] [root@testserver1 rpm]# lgtoxtdclnt 패키지 설치가 정상적으로 완료되었다.\n3. 호스트 파일 수정 # /etc/hosts 파일에 추가할 내용\n백업 관리서버의 IP 주소, Hostname 자기 자신의 IP 주소, Hostname [root@testserver1 rpm]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 [...] ### EMC NETWORKER ### 1.1.1.1 testserver1 2.2.2.2 BACKUP_MANAGEMENT_SERVER_HOSTNAME 4. NetWorker 서비스 시작 # [root@testserver1 ~]# service networker start [root@testserver1 ~]# 5. NetWorker 프로세스 확인 # [root@testserver1 ~]# ps -ef | grep nsr root 53433 1 0 13:14 ? 00:00:00 /usr/sbin/nsrexecd NetWorker 데몬인 nsrexecd 프로세스가 구동중이다.\n","date":"Jul 31, 2021","permalink":"/blog/installing-emc-network-on-linux/","section":"Blogs","summary":"개요 # EMC Networker 솔루션을 통해 백업을 받기 위해 백업 에이전트를 설치할 수 있다.","title":"리눅스에서 EMC Networker 에이전트 설치"},{"content":"개요 # HP-UX 운영체제에서 SSH root 로그인 금지 설정을 할 수 있다.\n환경 # OS : HP-UX B.11.31 Shell : sh (POSIX Shell) 설정방법 # 1. 설정파일 확인 # SSH 설정파일의 기본 경로는 /opt/ssh/etc/sshd_config 이다.\n$ vi /opt/ssh/etc/sshd_config [...] PermitRootLogin yes PermitRootLogin yes 를 no 로 변경\n$ vi /opt/ssh/etc/sshd_config [...] PermitRootLogin no 2. SSH 서비스 중지 # 변경된 SSH 설정을 적용하기 위해서 SSH 서비스(secsh)를 중지한 후, 다시 기동시킨다.\nSSH를 중지하더라도 이미 로그인 상태인 원격 세션은 끊기지 않는다.\n$ /sbin/init.d/secsh stop HP-UX Secure Shell stopped 3. SSH 서비스 시작 # SSH 서비스(secsh)를 다시 시작한다.\n$ /sbin/init.d/secsh start HP-UX Secure Shell started 4. SSH 서비스 확인 # $ ps -ef | grep ssh 참고자료 # How to Disable Root SSH Login in HP-UX?\n","date":"Jul 31, 2021","permalink":"/blog/disabling-direct-root-login-in-hp-ux/","section":"Blogs","summary":"개요 # HP-UX 운영체제에서 SSH root 로그인 금지 설정을 할 수 있다.","title":"HP-UX root 직접 로그인 금지"},{"content":"","date":"Jul 31, 2021","permalink":"/tags/hpux/","section":"Tags","summary":"","title":"hpux"},{"content":"개요 # history 명령어 실행시 날짜와 시간을 표시해주도록 설정한다.\nhistory 명령어에 시간이 표시될 때 장점\n특정 서버를 유지 관리하는 사람이 1명이 아닌 여러 명인 경우 각 계정마다 명령어가 언제 수행되엇는지 볼 수 있다.\n서버를 혼자 관리하더라도 시스템 관리자 스스로가 언제 무엇을 하거나 변경했는지 정확히 기억할 수 없는 경우에 특히 유용하다.\n환경 # OS : Red Hat Enterprise Linux Server release 6.2 (Santiago) Architecture : x86_64 Shell : bash 조치방법 # 1. 설정파일 수정 # $ vi /etc/profile [...] HISTTIMEFORMAT=\u0026#34;[%F %T] \u0026#34; export HISTTIMEFORMAT 설정값 해석\nHISTTIMEFORMAT : history 명령어의 시간값에 대한 미리 지정된 환경변수 이름 %F : 날짜 (년-월-일) %T : 시간 (시:분:초) /etc/profile\n시스템 사용자 전체에 적용할 Shell 환경 변수를 담고 있는 설정파일. /etc/profile의 적용 대상은 모든 사용자이며 시스템에 로그인할 때마다 /etc/profile을 읽어들이며 수행하게 된다.\n2. 설정파일 적용 # HISTTIMEFORMAT 변수를 추가한 내용을 /etc/profile에 바로 적용시킨다.\n$ source /etc/profile 3. 로그아웃 # 변경된 설정을 적용하기 위해서 사용 중인 계정을 로그아웃한다.\n$ logout 4. 테스트 # $ history [...] 504 [2015-01-07 15:17:58] ls 505 [2015-01-07 15:18:07] source /etc/profile 506 [2015-01-07 15:18:10] history history 명령어 실행시 왼쪽에 명령어가 실행된 날짜와 시간이 표시되는 것을 확인할 수 있다.\n참고자료 # https://www.hahwul.com/2015/01/07/history-view-date-and-time-in-history/\nhttps://www.geeksforgeeks.org/histtimeformat-variable-in-linux-with-example/\n","date":"Jul 31, 2021","permalink":"/blog/enabling-history-timestamp-in-linux/","section":"Blogs","summary":"개요 # history 명령어 실행시 날짜와 시간을 표시해주도록 설정한다.","title":"리눅스 history에 시간 표시"},{"content":"개요 # hugo 기반의 깃허브 블로그를 구축할 수 있다.\n환경 # Hardware : MacBook Pro (13-inch, M1, 2020) OS : macOS Big Sur 11.5 Shell : zsh Hugo : hugo v0.86.0+extended darwin/arm64 (brew를 이용해서 설치) brew : Homebrew 3.2.5 전제조건 # 패키지 관리자인 Homebrew가 설치되어 있는 macOS 환경\nGithub 계정\n블로그 만드는 방법 # 1. 레포지터리 생성 # 블로그를 생성하기 위해 레포지터리 2개를 새로 생성한다.\n왜 블로그를 운영하는 데에 레포지터리가 2개나 필요할까. 개발용 레포지터리는 말 그대로 소스 원본 저장 및 관리용이며, 서비스용 레포지터리는 개발용 레포지터리를 빌드한 결과물이 담긴 레포지터리이다.\n사실 개발용 레포지터리를 꼭 등록하지 않아도 블로그는 운영이 가능하지만, 둘 다 일괄적으로 Github 레포지터리로 관리하면 편하기 때문이다. 대표적인 장점으로 개발용 레포지터리의 Commit log를 보면서 블로그 작업이나 글 작성, 변경에 대한 이력도 추적 가능하다.\n개발용(Development) 레포지터리 이름 : blog\n서비스용(Production) 레포지터리 이름 : iiivvveee.github.io\n(본인의 Github username을 레포지터리 이름 맨 앞에 넣어준다. Github 유저네임이 seyslee일 경우, 레포지터리 이름은 seyslee.github.io가 된다.)\n2. Hugo 설치 # Hugo 블로그를 생성하려면 hugo 를 먼저 설치해야한다.\nmacOS용 패키지 관리자인 Homebrew를 사용중이라면 아래 명령어 한 줄로 쉽게 설치가 가능하다.\n만약 Homebrew가 설치되어 있지 않다면 brew 공식홈페이지를 방문해서 설치한다.\nbrew를 이용한 hugo 설치\n$ brew install hugo hugo 버전 확인\n정상적으로 hugo가 설치가 되었다면 hugo의 버전이 출력되어야 한다.\n$ hugo version hugo v0.86.0+extended darwin/arm64 BuildDate=unknown 3. 사이트 생성 # 프로젝트 폴더를 만들 경로로 이동한다.\n내 경우는 github_repos 라는 폴더 안에 모든 레포지터리를 관리하는 구성이기 때문에 github_repos 폴더 안에 blog 폴더를 따로 생성하기로 결정했다.\n현재 작업 구성은 아래와 같다.\n$ pwd /Users/ive/github_repos ## 트리 구조로 본 프로젝트 경로 / └── Users └── ive └── github_repos └── blog \u0026lt;== 내가 새로 만들 블로그 폴더 사이트 생성 명령어 형식\n$ hugo new site \u0026lt;프로젝트 이름(폴더명)\u0026gt; 내가 실행한 명령어\n사이트 생성 전에 현재 작업경로를 미리 확인한다.\n$ pwd /Users/ive/github_repos 사이트 생성\nblog라는 이름의 디렉토리를 현재 경로에 생성하고, 해당 디렉토리 안에 블로그 환경을 자동 구성한다.\n$ hugo new site blog Congratulations! Your new Hugo site is created in /Users/ive/github_repos/blog. Just a few more steps and you\u0026#39;re ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; command. 2. Perhaps you want to add some content. You can add single files with \u0026#34;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 3. Start the built-in live server via \u0026#34;hugo server\u0026#34;. Visit https://gohugo.io/ for quickstart guide and full documentation. Congratulations!메세지가 나오고 정상적으로 blog 폴더가 생성된 걸 확인할 수 있다.\n$ pwd /Users/ive/github_repos $ ls PycharmProjects nodejs-with-docker-toyproj blog ps gs-gradle seyslee jenkins-pipeline-toyproj 현재 경로에 blog 디렉토리가 새로 생성되었다!\n4. 테마 다운로드 # 현재 블로그 테마는 hugo-theme-codex를 사용 중이다.\n블로그 프로젝트 폴더로 이동한 후 git init 명령어를 실행해 블로그 폴더에 git 환경을 구성한다.\n$ pwd /Users/ive/github_repos/blog $ git init 서브모듈(Submodule)의 개념\nGit의 서브모듈(Submodule)이란 하나의 저장소(Repository) 안에 있는 또 다른 별개의 저장소이다. 보통 다른 원격 저장소를 가져와(Pull) 서브모듈로 사용하게 된다.\n테마 깃허브 저장소를 서브모듈로 가져오기\n아래 명령어는 hugo-theme-codex 테마 깃허브 저장소를 내 저장소 내 서브모듈로 가져오는 명령어이다.\n$ git submodule add https://github.com/jakewies/hugo-theme-codex.git themes/hugo-theme-codex . ├── archetypes ├── content ├── data ├── layouts ├── resources ├── static └── themes └── hugo-theme-codex \u0026lt;===[submodule]=== https://github.com/jakewies/hugo-theme-codex git submodule 명령어를 실행해 submodule 목록이 생성된 걸 확인한다.\n$ git submodule 9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da themes/hugo-theme-codex (v1.6.0) 설정파일(config.toml) 생성\n그 다음 themes/hugo-theme-codex/exampleSite/config.toml 파일을 복사해 초기에 생성된 config.toml 에 덮어쓴다.\n. ├── archetypes ├── config.toml \u0026lt;== 초기에 생성된 config.toml [...] └── themes └── hugo-theme-codex └── exampleSite └── config.toml \u0026lt;== 복사할 config.toml 교체한 config.toml 설정파일을 열어보면 맨 윗줄에 themesDir 값이 있다.\n이 라인과 주석을 삭제하고 저장한다.\n# REMOVE THIS themesDir = \u0026#34;../../\u0026#34; 추가적인 블로그 설정 변경은 hugo-theme-codex 공식문서를 참고한다.\nindex 페이지 설정\ncontent/_index.md 파일을 생성하여 사이트의 초기화면(index) 페이지를 구성할 수 있다.\n.md 확장자는 markdown 파일을 의미한다.\n_index.md파일 위치는 다음과 같다.\n. ├── archetypes ├── config.toml [...] └── content └── _index.md \u0026lt;== 메인 페이지 설정파일 _index.md 파일 내용은 다음과 같이 작성한다.\n--- heading: \u0026#34;Hi, I\u0026#39;m Codex\u0026#34; subheading: \u0026#34;A minimal blog theme for hugo.\u0026#34; handle: \u0026#34;hugo-theme-codex\u0026#34; --- 이후 본인의 환경에 맞게 수정한다.\n--- heading: \u0026#34;Younsung Lee\u0026#34; subheading: \u0026#34;시스템 엔지니어\u0026#34; handle: \u0026#34;seyslee\u0026#34; --- 테스트\n$ hugo server -D hugo server : 생성한 페이지를 빌드하기 전, localhost 에서 미리 확인해보기 위한 웹서버 실행 명령어.\n-D : draft 속성이 true 인 글도 보이도록 하는 옵션 Start building sites … hugo v0.86.0+extended darwin/arm64 BuildDate=unknown | EN -------------------+----- Pages | 23 Paginator pages | 0 Non-page files | 0 Static files | 12 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 19 ms Watching for changes in /Users/ive/github_repos/blog/{archetypes,content,data,layouts,static,themes} Watching for config changes in /Users/ive/github_repos/blog/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop 마지막에 Web Server is available at http://localhost:1313/ 메세지가 출력되면 정상적으로 완료된 것이다.\n인터넷 브라우저를 열어 http://localhost:1313/ 으로 접속한다. Index 페이지가 잘 수정되었는지 확인한다.\n페이지 확인을 마친 후에 Ctrl + C 키를 눌러 개발환경의 웹 서버를 종료할 수 있다.\n5. 리모트 추가 # 명령어 형식\n$ git remote add origin \u0026lt;블로그 작성용 레포지터리의 URL\u0026gt; git remote : 현재 프로젝트에 등록된 리모트 저장소를 확인하는 명령어. 이 명령어는 리모트 저장소의 단축 이름을 보여준다. 레포지터리를 복제(Clone)하면 origin이라는 리모트 저장소가 자동으로 등록되기 때문에 origin이라는 이름을 볼 수 있다.\norigin : 축약형 이름을 말한다.\n블로그 작성용 레포지터리의 URL : https://github.com/\u0026lt;Github 유저네임\u0026gt;/\u0026lt;블로그용 레포지터리 이름\u0026gt;\n내가 실행한 명령어\n필자의 경우 Github 유저네임는 iiivvveee, 블로그 작성용 레포지터리의 이름은 blog이다.\n그 결과 아래 명령어를 실행해 리모트를 추가했다.\n$ git remote add origin https://github.com/iiivvveee/blog ^-------^ ^--^ | | | +--\u0026gt; 블로그용 레포지터리 이름 +---------\u0026gt; Github 유저네임 리모트 저장소가 정상적으로 추가되었는지 git remote -v 명령어로 확인한다.\n-v 옵션을 주면 등록된 리모트 저장소의 이름과 URL을 같이 볼 수 있다.\n$ git remote -v origin https://github.com/iiivvveee/blog (fetch) origin https://github.com/iiivvveee/blog (push) 6. 서브모듈 추가 # $ git submodule add -b master https://github.com/iiivvveee/iiivvveee.github.io.git public 서브모듈을 추가후 정상적으로 적용되었는지 git submodule 명령어로 확인한다.\n$ git submodule -a3a5abf8f39e7b6dcaf49431c90f4e963591e35d public 9e911e331c90fcd56ae5d01ae5ecb2fa06ba55da themes/hugo-theme-codex (v1.6.0) public 폴더가 submodule 로 추가된 걸 확인할 수 있다.\n7. 테스트 # hugo server : 생성한 페이지를 빌드하기 전, localhost 에서 미리 확인해보기 위한 웹서버 실행 명령어.\n-D : draft 속성이 true 인 글도 보이도록 하는 옵션 $ hugo server -D Start building sites … hugo v0.86.0+extended darwin/arm64 BuildDate=unknown | EN -------------------+----- Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 12 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Built in 27 ms Watching for changes in /Users/ive/github_repos/blog/{archetypes,content,data,layouts,static,themes} Watching for config changes in /Users/ive/github_repos/blog/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Hugo 의 기본포트는 1313 이다. 이 때, Hugo의 강점인 빠른 속도가 체감된다. Build 하는 시간이 겨우 27ms 밖에 안걸린다.\nChange detected, rebuilding site. 2021-07-31 01:36:42.676 +0900 Source changed \u0026#34;/Users/ive/github_repos/blog/content/blog/installing-hugo-github-blog.md\u0026#34;: CREATE Total in 7 ms 개발환경에서 테스트하는 동안 Hugo는 실시간으로 변경사항을 체크해 반영하는 기능도 있다.\nhugo server는 기본적으로 사용자가 변경한 사항이 있는지 파일을 항상 감시하고 사이트를 자동으로 재구축하는 기능을 갖고 있다.\n즉, 블로그의 게시글을 한 글자라도 수정하면 그 즉시 hugo server가 수정사항을 반영해서 변경된 웹사이트로 다시 보여준다.\n글을 작성하는 사람의 입장에서 내가 입력한 변경사항을 바로 확인할 수 있어서 매우 편리하다.\n브라우저를 키고 주소창에 http://localhost:1313/ 을 입력해 접속한다. 내가 만든 블로그 화면이 잘 보일 것이다.\n8. 게시글 작성 # 새 게시글을 생성할 때는 아래 명령어를 사용한다.\n명령어 형식\n$ hugo new blog/\u0026lt;게시글 이름\u0026gt;.md 실제 실행한 명령어\n$ hugo new blog/new-post.md /Users/ive/github_repos/blog/content/blog/new-post.md created 게시글이 생성되는 경로\nhugo-theme-codex 테마 기준으로 게시글은 ./content/blog/ 디렉토리 아래에 생성되어야 정상적으로 표출된다.\n테마마다 게시글이 위치해야하는 경로는 다를 수 있다. 테마 페이지에서 안내해주는 가이드 문서를 정독하고 게시글을 해당 경로에 맞게 생성한다.\n. ├── archetypes ├── content │ └── blog │ └── new-post.md \u0026lt;== 새로 생성된 게시글 ├── data ├── layouts ├── public ├── resources ├── static ├── deploy.sh └── themes └── hugo-theme-codex 글 작성시 주의사항\n--- title: \u0026#34;Hugo x github 블로그 설치 방법\u0026#34; date: 2021-07-31T00:29:09+09:00 lastmod: 2021-07-31T00:33:04+09:00 slug: \u0026#34;\u0026#34; description: \u0026#34;Hugo 와 Github 를 사용해 블로그를 구축하는 방법\u0026#34; keywords: [] draft: false tags: [\u0026#34;Linux\u0026#34;, \u0026#34;OS\u0026#34;] math: false toc: true --- Front Matter 설명\ntitle : 게시글의 제목 date : 게시글의 최초 작성시간 lastmod : 게시글의 마지막 수정시간 description : 게시글의 설명글. 검색엔진 최적화(SEO, Search Engine Optimization)를 위해서 게시글 제목(title)에 담긴 내용을 풀어 간단하게 적어놓는다. draft : draft: true 일 경우, 실제 배포환경에서 해당 게시글은 보이지 않게 된다. 작성한 글을 개발 환경이 아닌 실제 환경에도 게시하고 싶다면 반드시 draft: false 값을 설정해주자. tags : 게시글의 태그 toc : table of content의 약자. Front Matter의 값이 toc: true 일 경우 마크다운 기반의 게시글 목차를 정리해서 보여준다.\n9. 배포 스크립트 작성 # 앞으로 Hugo 블로그를 관리하려면 2개의 레포지터리에 모두 Commit 해야한다. 구분된 레포지터리 경로마다 들어가서 commit과 push를 총 2번 해야하는 불편함이 있다.\n# Repository name : iiivvveee.github.io $ cd public $ git add . $ git commit -m \u0026#34;First commit\u0026#34; $ git push origin master # Repository name : blog $ cd .. $ git add . $ git commit -m $ git push origin master 2개의 레포지터리 관리는 복잡하고 귀찮으니 컨텐츠 배포를 자동화하는 스크립트를 작성한다.\n블로그 루트 디렉토리 안에 deploy.sh 스크립트 파일을 만든다.\n. ├── archetypes ├── content ├── data ├── layouts ├── public ├── resources ├── static ├── deploy.sh \u0026lt;== 우리가 작성할 스크립트 파일 └── themes └── hugo-theme-codex deploy.sh 의 내용은 아래와 같이 작성한다. 7번 라인에 hugo -t \u0026lt;자신이 사용하는 테마 이름\u0026gt; 은 자신의 환경에 맞게 변경해준다.\n#!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;여러분의 테마\u0026gt; hugo -t hugo-theme-codex # Go To Public folder, sub module commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master # Come Back up to the Project Root cd .. # blog 저장소 Commit \u0026amp; Push git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin master 10. 배포 실행 # 게시글을 작성 후 블로그에 게시하고 싶은 상황에서는 배포용 쉘스크립트인 deploy.sh 만 실행하면 된다.\n배포용 쉘스크립트 실행을 위해 블로그 프로젝트 폴더의 루트 디렉토리로 이동한다.\n$ pwd /Users/ive/github_repos/blog 배포용 쉘스크립트(deploy.sh)이 현재 디렉토리에 존재하는 지 확인한다.\n$ ls -lh total 16 drwxr-xr-x 3 ive staff 96B 7 30 20:00 archetypes -rw-r--r-- 1 ive staff 2.5K 1 2 22:41 config.toml drwxr-xr-x 6 ive staff 192B 12 7 23:45 content -rwxr-xr-x@ 1 ive staff 585B 1 2 22:05 deploy.sh drwxr-xr-x 5 ive staff 160B 12 8 00:39 layouts drwxr-xr-x 16 ive staff 512B 1 6 21:32 public drwxr-xr-x 4 ive staff 128B 8 1 14:43 resources drwxr-xr-x 5 ive staff 160B 1 2 23:22 static drwxr-xr-x 4 ive staff 128B 7 30 20:03 themes deploy.sh 파일이 현재 경로에 존재한다.\n그 다음 bash 명령어로 배포 스크립트 파일(deploy.sh)을 실행한다.\nmacOS 환경은 터미널에서 bash 명령어를 기본적으로 지원한다.\n$ bash deploy.sh bash deploy.sh Deploying updates to GitHub... Start building sites … hugo v0.86.0+extended darwin/arm64 BuildDate=unknown | EN -------------------+----- Pages | 14 Paginator pages | 0 Non-page files | 0 Static files | 12 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 33 ms [...] remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To https://github.com/iiivvveee/blog fa10865..e13e834 master -\u0026gt; master 정상적으로 게시글이 배포 완료됐다.\n참고자료 # HUGO Quick Start 공식문서\n블로그 구축기 1 (Hugo + github.io)\n블로그 구축기 (1) Hugo + Github으로 개인 블로그 만들기\n더 나아가서 # 댓글기능 추가하기\n깃허브 블로그는 댓글 기능을 기본적으로 지원하지 않는다.\nUtterances 플러그인을 이용해서 블로그 댓글 기능을 추가하고 싶다면 이 글을 참고하면 된다.\n","date":"Jul 31, 2021","permalink":"/blog/installing-hugo-github-blog/","section":"Blogs","summary":"개요 # hugo 기반의 깃허브 블로그를 구축할 수 있다.","title":"Hugo x github 블로그 설치 방법"},{"content":"","date":"Jan 1, 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"}]